"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[8835],{7709:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/week-1-introduction/4-3-vla-training-data-collection","title":"4.3: vla \u0679 raunn \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a arna owr \u06a9\u06cc taur \u06cc","description":"\u062c a \u062c","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/week-1-introduction/4-3-vla-training-data-collection.md","sourceDirName":"module-4/week-1-introduction","slug":"/module-4/week-1-introduction/4-3-vla-training-data-collection","permalink":"/ur/docs/module-4/week-1-introduction/4-3-vla-training-data-collection","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-1-introduction/4-3-vla-training-data-collection.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"44","permalink":"/ur/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning"},"next":{"title":"4.4: vla lamli \u06cc n \u0641 aa \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 vr a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632","permalink":"/ur/docs/module-4/week-1-introduction/4-4-vla-practical-implementation"}}');var t=r(4848),s=r(8453);const i={sidebar_position:3,difficulty:"advanced"},o="4.3: vla \u0679 raunn \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a arna owr \u06a9\u06cc taur \u06cc",l={},c=[{value:"\u062c a \u062c",id:"\u062c-a-\u062c",level:2},{value:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d",id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",level:2},{value:"vla \u0688\u06cc\u0679 a \u06a9\u06cc \u0636 \u0636 \u0636 rorur \u06cc at",id:"vla-\u0688\u06cc\u0679-a-\u06a9\u06cc-\u0636-\u0636-\u0636-rorur-\u06cc-at",level:2},{value:"\u0688\u06cc\u0679 a \u06a9\u06d2 \u0637 ri \u06cc\u0642 \u06a9 ar",id:"\u0688\u06cc\u0679-a-\u06a9\u06d2-\u0637-ri-\u06cc\u0642-\u06a9-ar",level:3},{value:"\u0688\u06cc\u0679 \u0627\u06cc\u06a9 \u0633\u06cc\u0633\u0633\u0631",id:"\u0688\u06cc\u0679-\u0627\u06cc\u06a9-\u0633\u06cc\u0633\u0633\u0631",level:3},{value:"\u0688\u06cc\u0679 a \u062d\u062c m \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u06a9\u06cc \u06a9\u06cc \u0636 \u0636 \u0636 \u0636 \u0636 \u0636",id:"\u0688\u06cc\u0679-a-\u062d\u062c-m-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u0636-\u0636-\u0636-\u0636-\u0636-\u0636-\u06a9\u06cc-\u06a9\u06cc-\u0636-\u0636-\u0636-\u0636-\u0636-\u0636",level:3},{value:"\u0688\u06cc\u0679 a \u0688\u06cc\u0679 a -\u06a9 a -\u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637",id:"\u0688\u06cc\u0679-a-\u0688\u06cc\u0679-a--\u06a9-a--\u06a9-\u06a9\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-\u0637-\u0637-\u0637-\u0637-\u0637",level:2},{value:"\u062c a \u0626\u0632\u06c1",id:"\u062c-a-\u0626\u0632\u06c1",level:4},{value:"pros owr cons",id:"pros-owr-cons",level:4},{value:"2",id:"2",level:3},{value:"\u062c a \u0626\u0632\u06c1",id:"\u062c-a-\u0626\u0632\u06c1-1",level:4},{value:"pros owr cons",id:"pros-owr-cons-1",level:4},{value:"3",id:"3",level:3},{value:"\u062c a \u0626\u0632\u06c1",id:"\u062c-a-\u0626\u0632\u06c1-2",level:4},{value:"pros owr cons",id:"pros-owr-cons-2",level:4},{value:"4",id:"4",level:3},{value:"\u062c a \u0626\u0632\u06c1",id:"\u062c-a-\u0626\u0632\u06c1-3",level:4},{value:"pros owr cons",id:"pros-owr-cons-3",level:4},{value:"\u062a\u0639\u0634 \u0627\u0648\u0633 \u0644\u0650\u0633\u0628\u0644\u0646\u06af \u06a9\u06cc \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u0686\u0679\u0627\u0626\u06cc \u0627\u0645\u062a",id:"\u062a\u0639\u0634-\u0627\u0648\u0633-\u0644\u0650\u0633\u0628\u0644\u0646\u06af-\u06a9\u06cc-\u062d\u06a9-\u062d\u06a9-\u062d\u06a9-\u0686\u0679\u0627\u0626\u06cc-\u0627\u0645\u062a",level:2},{value:"vud \u06a9 ar tahr \u06cc\u062d",id:"vud-\u06a9-ar-tahr-\u06cc\u062d",level:3},{value:"vum sswrs \u0688 t \u0634 r \u06cc\u062d",id:"vum-sswrs-\u0688-t-\u0634-r-\u06cc\u062d",level:3},{value:"\u0688\u06cc\u0679 a -\u06a9\u06cc vri \u06cc\u0634 n vors maiur \u06a9\u06cc \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635",id:"\u0688\u06cc\u0679-a--\u06a9\u06cc-vri-\u06cc\u0634-n-vors-maiur-\u06a9\u06cc-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635",level:2},{value:"\u0688\u06cc\u0679 a -\u06a9 \u0648\u0627\u0644 \u0679\u06cc MAUR \u06a9 SS",id:"\u0688\u06cc\u0679-a--\u06a9-\u0648\u0627\u0644-\u0679\u06cc-maur-\u06a9-ss",level:3},{value:"\u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasusna \u06af oawars babw",id:"\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasusna-\u06af-oawars-babw",level:2},{value:"\u0648\u0646 \u0688\u06cc\u0679 a \u067e a \u067e rausasussn \u06af",id:"\u0648\u0646-\u0688\u06cc\u0679-a-\u067e-a-\u067e-rausasussn-\u06af",level:3},{value:"\u0632 bain \u06a9 a \u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasassn \u06af",id:"\u0632-bain-\u06a9-a-\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasassn-\u06af",level:3},{value:"\u06cc\u06a9\u0634 in \u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasassn \u06af",id:"\u06cc\u06a9\u0634-in-\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasassn-\u06af",level:3},{value:"\u0627\u062e\u0644\u0627 at at \u062d\u0641\u0638 \u062d\u0641\u0638 atat",id:"\u0627\u062e\u0644\u0627-at-at-\u062d\u0641\u0638-\u062d\u0641\u0638-atat",level:2},{value:"taia \u0635 b \u06a9 a \u067e \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06af \u06af",id:"taia-\u0635-b-\u06a9-a-\u067e-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06af-\u06af",level:3},{value:"ra \u0632 dar \u06cc s \u06d2 t \u062d\u0641\u0638",id:"ra-\u0632-dar-\u06cc-s-\u06d2-t-\u062d\u0641\u0638",level:3},{value:"\u0688\u06cc\u0679 a \u0688\u06cc\u0679 a \u06a9\u0679\u06be a arna \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u062e \u062e \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u062c \u062c",id:"\u0688\u06cc\u0679-a-\u0688\u06cc\u0679-a-\u06a9\u0679\u06be-a-arna-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u062e-\u062e-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u062c-\u062c",level:2},{value:"re\u0650snaiuma \u062e\u0637s l \u06cc\u06d2 l \u06cc\u06d2 \u0630 mi \u06c1 idar \u0688\u06cc\u0679 a a a ai j \u06a9 sa",id:"re\u0650snaiuma-\u062e\u0637s-l-\u06cc\u06d2-l-\u06cc\u06d2-\u0630-mi-\u06c1-idar-\u0688\u06cc\u0679-a-a-a-ai-j-\u06a9-sa",level:3},{value:"\u062e LAA \u0635\u06c1",id:"\u062e-laa-\u0635\u06c1",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"43-vla-\u0679-raunn-\u06af-\u0688\u06cc\u0679-a-a-\u06a9\u0679\u06be-a-arna-owr-\u06a9\u06cc-taur-\u06cc",children:"4.3: vla \u0679 raunn \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a arna owr \u06a9\u06cc taur \u06cc"})}),"\n",(0,t.jsx)(n.h2,{id:"\u062c-a-\u062c",children:"\u062c a \u062c"}),"\n",(0,t.jsx)(n.p,{children:"\u06cc\u06c1 \u0627\u0644\u0644 l \u06cc \u06ba \u06ba \u06cc \u06cc \u06ba \u06ba \u06ba \u06a9\u06d2 \u0637 \u0637 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u060c"}),"\n",(0,t.jsx)(n.h2,{id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",children:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d"}),"\n",(0,t.jsx)(n.p,{children:"\u06a9\u06d2 \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u0645\u0627\u0645\u062a\u0644 \u0637 \u0637 rauch \u06a9 v ssmau - vla -iri \u06cc nna \u06af \u0688\u06cc\u0679 a ia \u06a9\u0679\u06be a -isrna\r\n\u06d4"}),"\n",(0,t.jsx)(n.li,{children:"\u0688\u06cc\u0679 a -\u06a9\u06cc vraun arsis \u06a9\u06d2 maeaurs \u06a9\u06cc \u0634\u062e\u06cc\u0635 \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9 \u06a9 \u06a9 jai \u06cc\u06a9 s j \u06a9 \u06a9 j \u06a9 j \u06a9 j \u06a9"}),"\n",(0,t.jsx)(n.li,{children:"\u0639\u0645\u0627\u062a\u062a \u06a9/\u06a9\u06cc \u06a9\u06cc \u06c1 \u06c1 maut \u06a9 v ssmi"}),"\n",(0,t.jsx)(n.li,{children:"\u067e r \u06cc \u067e rossassna \u06af vorsos \u06a9o buabawwava idaun \u06d2 \u06a9\u06cc \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 jula jula \u06a9\u06d2 ll \u06cc\u06d2 vla \u0688\u06cc\u0679"}),"\n",(0,t.jsx)(n.li,{children:"ai\u062e ilaa \u0642\u06cc atalat \u06a9\u06cc n\u06cc\u06c1ni\u0688 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,t.jsx)(n.li,{children:"smwlaun- \u067e r mubn \u06cc a a \u06a9\u0679\u06be a arni \u06d2 \u06d2 \u0637 \u0637 rauchoi juriuaauaauauathat \u06a9 ri \u06cc\u06ba"}),"\n",(0,t.jsx)(n.li,{children:"\u062a\u062a\u0646 \u06cc\u06a9 vaus \u06a9s \u06a9 l \u06cc\u06d2 l \u06cc\u06d2 l \u06cc\u06d2 asasaulni \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a aidrni \u06d2 \u06a9\u06cc \u06a9\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vla-\u0688\u06cc\u0679-a-\u06a9\u06cc-\u0636-\u0636-\u0636-rorur-\u06cc-at",children:"vla \u0688\u06cc\u0679 a \u06a9\u06cc \u0636 \u0636 \u0636 rorur \u06cc at"}),"\n",(0,t.jsx)(n.h3,{id:"\u0688\u06cc\u0679-a-\u06a9\u06d2-\u0637-ri-\u06cc\u0642-\u06a9-ar",children:"\u0688\u06cc\u0679 a \u06a9\u06d2 \u0637 ri \u06cc\u0642 \u06a9 ar"}),"\n",(0,t.jsx)(n.p,{children:"\u0634 \u0634 \u0634 \u0634 \u0634 \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u062c \u06a9\u06cc \u062c \u062c \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1 \u06c1 \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc"}),"\n",(0,t.jsx)(n.p,{children:"1 1.\r\n2.\r\n3."}),"\n",(0,t.jsx)(n.h3,{id:"\u0688\u06cc\u0679-\u0627\u06cc\u06a9-\u0633\u06cc\u0633\u0633\u0631",children:"\u0688\u06cc\u0679 \u0627\u06cc\u06a9 \u0633\u06cc\u0633\u0633\u0631"}),"\n",(0,t.jsx)(n.p,{children:"arr atrbaut mousasal \u0686 aa \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06cc\u06ba \u06cc\u06ba \u06cc\u06ba \u06cc\u06ba \u06cc\u06ba \u06cc\u06ba \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2 \u06c1\u06cc\u06d2: \u0621"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'\u0645\u062b\u0627\u0644 1:\r\n- Vision: [Image sequence showing kitchen scene]\r\n- Language: "Pick \u0627\u0648\u067e\u0631 \u06a9\u0627/\u06a9\u06cc red apple \u0633\u06d2 \u06a9\u0627/\u06a9\u06cc fruit bowl"\r\n- \u0627\u06cc\u06a9\u0634\u0646: [Sequence \u06a9\u0627 joint positions \u0627\u0648\u0631 gripper commands \u06a9\u0648 execute \u06a9\u0627/\u06a9\u06cc task]\r\n\r\n\u0645\u062b\u0627\u0644 2:\r\n- Vision: [Image \u06a9\u0627 \u0631\u0648\u0628\u0648\u0679 gripper holding object]\r\n- Language: "\u06a9\u06cc\u0627 am \u0645\u06cc\u06ba holding?"\r\n- \u0627\u06cc\u06a9\u0634\u0646: [Stop current \u0627\u06cc\u06a9\u0634\u0646, return "\u0622\u067e \u06c1\u06cc\u06ba holding \u0627\u06cc\u06a9 green cup"]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"\u0688\u06cc\u0679-a-\u062d\u062c-m-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u0636-\u0636-\u0636-\u0636-\u0636-\u0636-\u06a9\u06cc-\u06a9\u06cc-\u0636-\u0636-\u0636-\u0636-\u0636-\u0636",children:"\u0688\u06cc\u0679 a \u062d\u062c m \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u06a9\u06cc \u06a9\u06cc \u0636 \u0636 \u0636 \u0636 \u0636 \u0636"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"**- vaua- \u067e\u06cc\u0645\u0627\u0646\u06d2 \u06a9\u06d2 \u06a9\u0627\u0645\r\n."}),"\n",(0,t.jsx)(n.li,{children:"** \u06a9\u0631\u0627\u0633 \u0679\u0627\u0633\u06a9 \u062c\u0646\u0631\u0644\u0627\u0626\u0632\u06cc\u0634\u0646 **: \u0645\u062a\u0646\u0648\u0639 \u0679\u0627\u0633\u06a9 \u06a9\u0648\u0631\u06cc\u062c\r\n."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"\u0688\u06cc\u0679-a-\u0688\u06cc\u0679-a--\u06a9-a--\u06a9-\u06a9\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-\u0637-\u0637-\u0637-\u0637-\u0637",children:"\u0688\u06cc\u0679 a \u0688\u06cc\u0679 a -\u06a9 a -\u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637 \u0637"}),"\n",(0,t.jsx)(n.h5,{id:""}),"\n",(0,t.jsx)(n.h4,{id:"\u062c-a-\u0626\u0632\u06c1",children:"\u062c a \u0626\u0632\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"\u0628\u0631\u0631\u0627\u0646"}),"\n",(0,t.jsx)(n.h6,{id:"-1"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644 teleoperation data collection pipeline\r\n\r\nclass HumanTeleoperationCollector:\r\n    def __init__(self, robot_interface, data_buffer):\r\n        self.robot_interface = robot_interface\r\n        self.data_buffer = data_buffer\r\n        self.data_collector = DataCollectionManager()\r\n        \r\n    def collect_demonstration(self, task_description):\r\n        \"\"\"\r\n        Collect \u0627\u06cc\u06a9 single demonstration \u06a9\u0627 \u0627\u06cc\u06a9 task\r\n        \"\"\"\r\n        # Record initial state\r\n        initial_observation = self.robot_interface.get_observation()\r\n        language_instruction = self.tokenize_instruction(task_description)\r\n        \r\n        # Enable teleoperation mode\r\n        self.robot_interface.set_control_mode('teleoperation')\r\n        \r\n        # Collect trajectory data\r\n        trajectory = {\r\n            'observations': [],\r\n            '\u0627\u06cc\u06a9\u0634\u0646\u0632': [],\r\n            'language': language_instruction,\r\n            'task_description': task_description\r\n        }\r\n        \r\n        # Execute demonstration\r\n        \u062c\u0628 \u062a\u06a9 \u0646\u06c1\u06cc\u06ba self.is_episode_complete():\r\n            # Record current observation\r\n            current_obs = self.robot_interface.get_observation()\r\n            trajectory['observations'].append(current_obs)\r\n            \r\n            # Record \u0627\u06cc\u06a9\u0634\u0646\r\n            action_taken = self.robot_interface.get_last_action()\r\n            trajectory['\u0627\u06cc\u06a9\u0634\u0646\u0632'].append(action_taken)\r\n            \r\n            # Log \u06a9\u0648 buffer\r\n            self.data_buffer.store_transition(\r\n                observation=current_obs,\r\n                \u0627\u06cc\u06a9\u0634\u0646=action_taken,\r\n                language=language_instruction\r\n            )\r\n        \r\n        return trajectory\r\n    \r\n    def tokenize_instruction(self, instruction):\r\n        \"\"\"\r\n        Convert natural language \u06a9\u0648 token format\r\n        \"\"\"\r\n        # \u06cc\u06c1 \u06a9\u0631\u06d2 \u06af\u0627 interface \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0622\u067e \u06a9\u0627 tokenizer\r\n        return {\r\n            'raw_text': instruction,\r\n            'tokens': self.tokenizer.encode(instruction),\r\n            'vector': self.text_encoder.encode(instruction)\r\n        }\n"})}),"\n",(0,t.jsx)(n.h4,{id:"pros-owr-cons",children:"pros owr cons"}),"\n",(0,t.jsx)(n.p,{children:".\r\n."}),"\n",(0,t.jsx)(n.h3,{id:"2",children:"2"}),"\n",(0,t.jsx)(n.h4,{id:"\u062c-a-\u0626\u0632\u06c1-1",children:"\u062c a \u0626\u0632\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"atrb \u06cc t \u06a9 a \u0688\u06cc\u0679 a \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a \u06a9\u0679\u06be r \u06cc\u06ba m \u06cc\u06ba l \u06d2 l \u067e\u06c1 l \u06d2 \u06a9\u06cc \u06a9\u06cc munt\u0642l\u06cc \u06a9 v ia \u0635 l \u06cc rewboc"}),"\n",(0,t.jsx)(n.h6,{id:"-2"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644 \u0633\u0645\u0648\u0644\u06cc\u0634\u0646-based data collection\r\n\r\nclass SimulationDataCollector:\r\n    def __init__(self, sim_env, robot_model, num_episodes=10000):\r\n        self.sim_env = sim_env\r\n        self.robot_model = robot_model\r\n        self.num_episodes = num_episodes\r\n        self.data_storage = EpisodeReplayBuffer()\r\n        \r\n    def collect_diverse_demonstrations(self):\r\n        """\r\n        Collect diverse demonstrations across different scenarios\r\n        """\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 episode \u0645\u06cc\u06ba range(self.num_episodes):\r\n            # Randomize \u0645\u0627\u062d\u0648\u0644 conditions\r\n            self.sim_env.randomize_scene()\r\n            self.sim_env.randomize_object_poses()\r\n            self.sim_env.randomize_lighting_conditions()\r\n            \r\n            # Select random task\r\n            task_description, goal_condition = self.sample_random_task()\r\n            language_spec = self.tokenize_language(task_description)\r\n            \r\n            # Execute policy \u0645\u06cc\u06ba \u0633\u0645\u0648\u0644\u06cc\u0634\u0646\r\n            episode_data = self.generate_episode(\r\n                initial_condition=self.sim_env.get_state(),\r\n                goal_condition=goal_condition,\r\n                language_spec=language_spec\r\n            )\r\n            \r\n            # Store episode\r\n            self.data_storage.store_episode(\r\n                observations=episode_data[\'observations\'],\r\n                \u0627\u06cc\u06a9\u0634\u0646\u0632=episode_data[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'],\r\n                language=language_spec,\r\n                metadata={\r\n                    \'episode_id\': episode,\r\n                    \'task_description\': task_description,\r\n                    \'scene_config\': self.sim_env.get_scene_config()\r\n                }\r\n            )\r\n    \r\n    def domain_randomization(self):\r\n        """\r\n        Apply domain randomization \u06a9\u06d2 \u0644\u06cc\u06d2 sim-\u06a9\u0648-real transfer\r\n        """\r\n        # Randomize physical parameters\r\n        self.sim_env.set_friction_coeff(random.uniform(0.1, 2.0))\r\n        self.sim_env.set_restitution(random.uniform(0.0, 0.5))\r\n        \r\n        # Randomize visual parameters\r\n        self.sim_env.set_lighting(random_color_temperature())\r\n        self.sim_env.set_texture_randomization(True)\r\n        \r\n        # Randomize dynamic parameters\r\n        self.sim_env.set_external_force_disturbance(\r\n            random.uniform(-5, 5, size=3)\r\n        )\n'})}),"\n",(0,t.jsx)(n.h4,{id:"pros-owr-cons-1",children:"pros owr cons"}),"\n",(0,t.jsx)(n.p,{children:"."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cons"}),": \u0633\u0645\u0648\u0644\u06cc\u0634\u0646-\u06a9\u0648-reality gap, ",(0,t.jsx)(n.em,{children:"MAY"})," lack real-world complexities"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3",children:"3"}),"\n",(0,t.jsx)(n.h4,{id:"\u062c-a-\u0626\u0632\u06c1-2",children:"\u062c a \u0626\u0632\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"ir \u0646\u06c1\u0631\u0627\u0646 \u06cc \u0634 \u0634 \u06c1 \u06c1 sacurauri \u06cc\u0634 n awors baat \u0686\u06cc \u06a9\u06d2 \u06a9\u06d2 \u0630 raua \u06d2 iau iau iau ia ia ia ia \u06a9 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679"}),"\n",(0,t.jsx)(n.h6,{id:"-3"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644 self-supervised data collection\r\n\r\nclass SelfSupervisedCollector:\r\n    def __init__(self, robot_env, exploration_policy):\r\n        self.env = robot_env\r\n        self.exploration_policy = exploration_policy\r\n        self.memory_buffer = CircularBuffer(size=100000)\r\n        \r\n    def collect_exploration_data(self, max_steps=1000000):\r\n        \"\"\"\r\n        Collect data through autonomous exploration\r\n        \"\"\"\r\n        obs = self.env.reset()\r\n        total_reward = 0\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 step \u0645\u06cc\u06ba range(max_steps):\r\n            # Get exploratory \u0627\u06cc\u06a9\u0634\u0646\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.exploration_policy.get_action(obs)\r\n            \r\n            # Execute \u0627\u06cc\u06a9\u0634\u0646 \u0645\u06cc\u06ba \u0645\u0627\u062d\u0648\u0644\r\n            next_obs, reward, done, info = self.env.step\r\n            \r\n            # Store transition \u06a9\u06d2 \u0633\u0627\u062a\u06be exploration metadata\r\n            self.memory_buffer.push({\r\n                'observation': obs,\r\n                '\u0627\u06cc\u06a9\u0634\u0646': \u0627\u06cc\u06a9\u0634\u0646,\r\n                'reward': reward,\r\n                'next_observation': next_obs,\r\n                'done': done,\r\n                'exploration_strategy': self.exploration_policy.strategy_used,\r\n                'step_count': step\r\n            })\r\n            \r\n            obs = next_obs\r\n            \r\n            # Reset \u0627\u06af\u0631 episode ended\r\n            \u0627\u06af\u0631 done:\r\n                obs = self.env.reset()\r\n                \r\n    def mine_interaction_data(self):\r\n        \"\"\"\r\n        Mine meaningful interactions \u0633\u06d2 exploration data\r\n        \"\"\"\r\n        mined_interactions = []\r\n        \r\n        # Look \u06a9\u06d2 \u0644\u06cc\u06d2 significant state changes\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba \u0645\u06cc\u06ba range(1, len(self.memory_buffer)):\r\n            prev_state = self.memory_buffer[\u0645\u06cc\u06ba-1]['observation']\r\n            curr_state = self.memory_buffer[\u0645\u06cc\u06ba]['observation']\r\n            \r\n            # Measure state change significance\r\n            state_change = self.compute_state_difference(prev_state, curr_state)\r\n            \r\n            \u0627\u06af\u0631 state_change > self.STATE_CHANGE_THRESHOLD:\r\n                # \u06cc\u06c1 represents \u0627\u06cc\u06a9 meaningful interaction\r\n                mined_interactions.append({\r\n                    'pre_interaction_state': prev_state,\r\n                    'action_taken': self.memory_buffer[\u0645\u06cc\u06ba]['\u0627\u06cc\u06a9\u0634\u0646'],\r\n                    'post_interaction_state': curr_state,\r\n                    'state_change_magnitude': state_change\r\n                })\r\n        \r\n        return mined_interactions\r\n    \r\n    def compute_state_difference(self, state1, state2):\r\n        \"\"\"\r\n        Compute meaningful difference between \u0631\u0648\u0628\u0648\u0679 states\r\n        \"\"\"\r\n        # \u0645\u062b\u0627\u0644: difference \u0645\u06cc\u06ba object positions, gripper state, etc.\r\n        obj_diff = np.linalg.norm(state1['obj_pos'] - state2['obj_pos'])\r\n        gripper_diff = abs(state1['gripper_pos'] - state2['gripper_pos'])\r\n        \r\n        return obj_diff + gripper_diff\n"})}),"\n",(0,t.jsx)(n.h4,{id:"pros-owr-cons-2",children:"pros owr cons"}),"\n",(0,t.jsx)(n.p,{children:".\r\n."}),"\n",(0,t.jsx)(n.h3,{id:"4",children:"4"}),"\n",(0,t.jsx)(n.h4,{id:"\u062c-a-\u0626\u0632\u06c1-3",children:"\u062c a \u0626\u0632\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"\u0622 inlaiaun \u067e la \u06cc\u0679 \u0641 arm \u06a9 a \u06a9 a \u06a9 asatamaal \u06a9 ri \u06cc\u06ba - insan \u06cc mauaaaur \u06d2 \u062c mae \u062c sma"}),"\n",(0,t.jsx)(n.h6,{id:"-4"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644 crowdsourced data collection \u0688\u06be\u0627\u0646\u0686\u06c1\r\n\r\nclass CrowdsourcedDataCollector:\r\n    def __init__(self, api_endpoint, quality_control):\r\n        self.api_endpoint = api_endpoint\r\n        self.quality_control = quality_control\r\n        self.data_validator = DataValidator()\r\n        \r\n    def design_user_study(self, task_descriptions):\r\n        \"\"\"\r\n        Set \u0627\u0648\u067e\u0631 crowdsourcing study \u06a9\u06d2 \u0633\u0627\u062a\u06be clear instructions\r\n        \"\"\"\r\n        study_config = {\r\n            'tasks': task_descriptions,\r\n            'instructions': {\r\n                'recording_steps': [\r\n                    'Watch \u06a9\u0627/\u06a9\u06cc task video',\r\n                    'Follow \u06a9\u0627/\u06a9\u06cc text instructions',\r\n                    'Record \u0622\u067e \u06a9\u0627 \u0627\u06cc\u06a9\u0634\u0646\u0632 \u0645\u06cc\u06ba \u06a9\u0627/\u06a9\u06cc simulator'\r\n                ],\r\n                'quality_guidelines': [\r\n                    'Perform \u06a9\u0627/\u06a9\u06cc task completely',\r\n                    'Use natural language descriptions',\r\n                    'Provide clear demonstrations'\r\n                ]\r\n            },\r\n            'incentives': 'Pay-per-quality-demonstration',\r\n            'validation_methods': ['peer_review', 'expert_verification']\r\n        }\r\n        \r\n        return self.deploy_study(study_config)\r\n    \r\n    def validate_crowdsourced_data(self, collected_episodes):\r\n        \"\"\"\r\n        Quality control \u06a9\u06d2 \u0644\u06cc\u06d2 crowdsourced demonstrations\r\n        \"\"\"\r\n        validated_episodes = []\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 episode \u0645\u06cc\u06ba collected_episodes:\r\n            # Check completeness\r\n            \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba self.check_episode_completeness(episode):\r\n                continue\r\n                \r\n            # Check task success\r\n            \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba self.evaluate_task_success(episode):\r\n                continue\r\n                \r\n            # Check language quality\r\n            lang_quality = self.evaluate_language_quality(episode['language'])\r\n            \u0627\u06af\u0631 lang_quality < self.MIN_LANGUAGE_QUALITY:\r\n                continue\r\n                \r\n            # Check \u0627\u06cc\u06a9\u0634\u0646 smoothness\r\n            \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba self.check_action_smoothness:\r\n                continue\r\n                \r\n            validated_episodes.append(episode)\r\n        \r\n        return validated_episodes\r\n    \r\n    def evaluate_task_success(self, episode):\r\n        \"\"\"\r\n        Automated evaluation \u06a9\u0627 task completion\r\n        \"\"\"\r\n        final_state = episode['observations'][-1]\r\n        initial_state = episode['observations'][0]\r\n        task_goal = episode['metadata']['task_goal']\r\n        \r\n        # Use domain-specific success metrics\r\n        success_metric = self.compute_success_metric(\r\n            initial_state, final_state, task_goal\r\n        )\r\n        \r\n        return success_metric > self.SUCCESS_THRESHOLD\n"})}),"\n",(0,t.jsx)(n.h4,{id:"pros-owr-cons-3",children:"pros owr cons"}),"\n",(0,t.jsx)(n.p,{children:".\r\n."}),"\n",(0,t.jsx)(n.h2,{id:"\u062a\u0639\u0634-\u0627\u0648\u0633-\u0644\u0650\u0633\u0628\u0644\u0646\u06af-\u06a9\u06cc-\u062d\u06a9-\u062d\u06a9-\u062d\u06a9-\u0686\u0679\u0627\u0626\u06cc-\u0627\u0645\u062a",children:"\u062a\u0639\u0634 \u0627\u0648\u0633 \u0644\u0650\u0633\u0628\u0644\u0646\u06af \u06a9\u06cc \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u0686\u0679\u0627\u0626\u06cc \u0627\u0645\u062a"}),"\n",(0,t.jsx)(n.h3,{id:"vud-\u06a9-ar-tahr-\u06cc\u062d",children:"vud \u06a9 ar tahr \u06cc\u062d"}),"\n",(0,t.jsx)(n.h5,{id:"-5"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class AutomatedAnnotationPipeline:\r\n    def __init__(self):\r\n        # Load pre-trained models \u06a9\u06d2 \u0644\u06cc\u06d2 each modality\r\n        self.object_detector = self.load_pretrained_detector()\r\n        self.speech_recognizer = self.load_speech_model()\r\n        self.action_classifier = self.load_action_model()\r\n        \r\n    def annotate_batch(self, raw_data):\r\n        \"\"\"\r\n        Automatically annotate raw collected data\r\n        \"\"\"\r\n        annotated_batch = []\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba raw_data:\r\n            annotated_sample = {\r\n                'vision_annotations': self.annotate_vision(sample['images']),\r\n                'language_annotations': self.annotate_language(sample['audio']),\r\n                'action_annotations': self.annotate_actions(sample['behavior']),\r\n                'raw_data': sample\r\n            }\r\n            annotated_batch.append(annotated_sample)\r\n        \r\n        return annotated_batch\r\n    \r\n    def annotate_vision(self, images):\r\n        \"\"\"\r\n        Annotate visual content using computer vision models\r\n        \"\"\"\r\n        annotations = {\r\n            'objects': self.object_detector.predict(images),\r\n            'object_poses': self.pose_estimator.predict(images),\r\n            'affordances': self.affordance_predictor.predict(images),\r\n            'scene_graph': self.scene_graph_builder.build(images)\r\n        }\r\n        return annotations\r\n    \r\n    def annotate_language(self, audio_text):\r\n        \"\"\"\r\n        Annotate language content\r\n        \"\"\"\r\n        \u0627\u06af\u0631 isinstance(audio_text, str):\r\n            text = audio_text\r\n        else:\r\n            # Convert audio \u06a9\u0648 text\r\n            text = self.speech_recognizer.transcribe(audio_text)\r\n        \r\n        annotations = {\r\n            'intent_classification': self.intent_classifier.classify(text),\r\n            'entity_extraction': self.entity_extractor.extract(text),\r\n            'action_decomposition': self.action_parser.parse(text),\r\n            'semantic_parsing': self.semantic_parser.parse(text)\r\n        }\r\n        return annotations\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-6"}),"\n",(0,t.jsx)(n.h4,{id:"-7"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SemiAutomatedAnnotation:\r\n    def __init__(self):\r\n        self.ml_models = AutomatedAnnotationPipeline()\r\n        self.annotation_interface = AnnotationUI()\r\n        \r\n    def active_learning_annotation(self, dataset_pool):\r\n        """\r\n        Use active learning \u06a9\u0648 prioritize \u0632\u06cc\u0627\u062f\u06c1 \u062a\u0631 informative samples\r\n        """\r\n        # Get initial predictions \u0633\u06d2 ML models\r\n        predictions = self.ml_models.annotate_batch(dataset_pool)\r\n        \r\n        # Calculate uncertainty \u06a9\u06d2 \u0644\u06cc\u06d2 each sample\r\n        uncertainties = [self.calculate_uncertainty(pred) \u06a9\u06d2 \u0644\u06cc\u06d2 pred \u0645\u06cc\u06ba predictions]\r\n        \r\n        # Prioritize samples \u06a9\u06d2 \u0633\u0627\u062a\u06be highest uncertainty\r\n        sorted_indices = np.argsort(uncertainties)[::-1]\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 idx \u0645\u06cc\u06ba sorted_indices:\r\n            sample = dataset_pool[idx]\r\n            prediction = predictions[idx]\r\n            \r\n            # Show \u06a9\u0648 human annotator\r\n            corrected_annotation = self.annotation_interface.annotate(\r\n                sample, \r\n                initial_prediction=prediction\r\n            )\r\n            \r\n            \u0627\u06af\u0631 self.verify_annotation(corrected_annotation):\r\n                yield corrected_annotation\r\n            else:\r\n                # Re-submit \u06a9\u06d2 \u0644\u06cc\u06d2 verification\r\n                self.resubmit_for_verification(corrected_annotation)\r\n    \r\n    def calculate_uncertainty(self, prediction):\r\n        """\r\n        Calculate uncertainty \u06a9\u0627 ML model predictions\r\n        """\r\n        # \u0645\u062b\u0627\u0644: entropy \u06a9\u0627 confidence scores\r\n        \u0627\u06af\u0631 \'confidence_scores\' \u0645\u06cc\u06ba prediction:\r\n            conf_scores = prediction[\'confidence_scores\']\r\n            entropy = -np.sum(conf_scores * np.log(conf_scores + 1e-8))\r\n            return entropy\r\n        else:\r\n            # Default: \u0627\u0648\u0646\u0686\u0627 uncertainty \u06a9\u06d2 \u0644\u06cc\u06d2 complex samples\r\n            return self.estimate_complexity(prediction)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"vum-sswrs-\u0688-t-\u0634-r-\u06cc\u062d",children:"vum sswrs \u0688 t \u0634 r \u06cc\u062d"}),"\n",(0,t.jsx)(n.h5,{id:"-8"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class CrowdsourcedAnnotationQuality:\r\n    def __init__(self, workers):\r\n        self.workers = workers\r\n        self.gold_standard_tasks = []\r\n        \r\n    def implement_quality_control(self, annotation_task):\r\n        """\r\n        Implement quality control \u06a9\u06d2 \u0644\u06cc\u06d2 crowdsourced annotations\r\n        """\r\n        # Use multiple annotators per sample\r\n        annotations = []\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 worker \u0645\u06cc\u06ba self.select_reliable_workers(annotation_task):\r\n            annotation = worker.annotate(annotation_task)\r\n            annotations.append(annotation)\r\n        \r\n        # Aggregate multiple annotations\r\n        final_annotation = self.aggregate_annotations(annotations)\r\n        \r\n        # Assess agreement between annotators\r\n        agreement_score = self.calculate_agreement(annotations)\r\n        \r\n        \u0627\u06af\u0631 agreement_score < self.MIN_AGREEMENT_THRESHOLD:\r\n            # Collect \u0645\u0632\u06cc\u062f annotations\r\n            additional_annotations = self.collect_additional_annotations(\r\n                annotation_task, \r\n                additional_workers=self.select_expert_workers()\r\n            )\r\n            final_annotation = self.aggregate_annotations(\r\n                annotations + additional_annotations\r\n            )\r\n        \r\n        return final_annotation\r\n    \r\n    def calculate_agreement(self, annotations):\r\n        """\r\n        Calculate agreement between multiple annotators\r\n        """\r\n        \u0627\u06af\u0631 len(annotations) < 2:\r\n            return 1.0  # Perfect agreement \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 default\r\n        \r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 categorical labels: use Fleiss\' kappa\r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 continuous values: use ICC (intraclass correlation)\r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 sequence data: use sequence alignment scores\r\n        \r\n        return self.compute_categorical_agreement(annotations)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0688\u06cc\u0679-a--\u06a9\u06cc-vri-\u06cc\u0634-n-vors-maiur-\u06a9\u06cc-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635",children:"\u0688\u06cc\u0679 a -\u06a9\u06cc vri \u06cc\u0634 n vors maiur \u06a9\u06cc \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635"}),"\n",(0,t.jsx)(n.h3,{id:"\u0688\u06cc\u0679-a--\u06a9-\u0648\u0627\u0644-\u0679\u06cc-maur-\u06a9-ss",children:"\u0688\u06cc\u0679 a -\u06a9 \u0648\u0627\u0644 \u0679\u06cc MAUR \u06a9 SS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class DataQualityAssessment:\r\n    def __init__(self):\r\n        self.metrics = {\r\n            'completeness': self.measure_completeness,\r\n            'consistency': self.measure_consistency,\r\n            'accuracy': self.measure_accuracy,\r\n            'diversity': self.measure_diversity,\r\n            'balance': self.measure_balance\r\n        }\r\n    \r\n    def assess_dataset_quality(self, dataset):\r\n        \"\"\"\r\n        Comprehensive quality assessment \u06a9\u0627 VLA dataset\r\n        \"\"\"\r\n        quality_report = {}\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 metric_name, metric_func \u0645\u06cc\u06ba self.metrics.items():\r\n            quality_report[metric_name] = metric_func(dataset)\r\n        \r\n        return quality_report\r\n    \r\n    def measure_completeness(self, dataset):\r\n        \"\"\"\r\n        Measure completeness \u06a9\u0627 multimodal alignment\r\n        \"\"\"\r\n        total_samples = len(dataset)\r\n        complete_samples = 0\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset:\r\n            \u0627\u06af\u0631 (sample.get('vision_data') \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None \u0627\u0648\u0631\r\n                sample.get('language_data') \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None \u0627\u0648\u0631\r\n                sample.get('action_data') \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None \u0627\u0648\u0631\r\n                self.check_modality_alignment(sample)):\r\n                complete_samples += 1\r\n        \r\n        return complete_samples / total_samples \u0627\u06af\u0631 total_samples > 0 else 0\r\n    \r\n    def check_modality_alignment(self, sample):\r\n        \"\"\"\r\n        Check \u0627\u06af\u0631 modalities \u06c1\u06cc\u06ba temporally \u0627\u0648\u0631 logically aligned\r\n        \"\"\"\r\n        \u0627\u06af\u0631 'timestamps' \u0645\u06cc\u06ba sample:\r\n            # Check temporal alignment\r\n            max_delay = max(abs(ts - sample['timestamps'][0]) \r\n                          \u06a9\u06d2 \u0644\u06cc\u06d2 ts \u0645\u06cc\u06ba sample['timestamps'])\r\n            return max_delay < self.MAX_TEMPORAL_DELAY\r\n        else:\r\n            # Use logical consistency checks\r\n            return self.check_logical_consistency(sample)\r\n    \r\n    def measure_diversity(self, dataset):\r\n        \"\"\"\r\n        Measure diversity across different dimensions\r\n        \"\"\"\r\n        diversity_metrics = {}\r\n        \r\n        # Scene diversity\r\n        scenes = [sample.get('scene_id', 'unknown') \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset]\r\n        unique_scenes = len(set(scenes))\r\n        diversity_metrics['scene_diversity'] = unique_scenes / len(dataset)\r\n        \r\n        # Task diversity\r\n        tasks = [sample.get('task_description', '') \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset]\r\n        unique_tasks = len(set(tasks))\r\n        diversity_metrics['task_diversity'] = unique_tasks / len(dataset)\r\n        \r\n        # Language diversity\r\n        language_variations = self.compute_language_diversity(tasks)\r\n        diversity_metrics['language_diversity'] = language_variations\r\n        \r\n        # \u0627\u06cc\u06a9\u0634\u0646 diversity\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = [sample.get \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset]\r\n        action_space_coverage = self.compute_action_space_coverage\r\n        diversity_metrics['action_diversity'] = action_space_coverage\r\n        \r\n        return diversity_metrics\r\n    \r\n    def compute_language_diversity(self, texts):\r\n        \"\"\"\r\n        Compute lexical \u0627\u0648\u0631 syntactic diversity \u06a9\u0627 language data\r\n        \"\"\"\r\n        # Lexical diversity (TTR - Type-Token Ratio)\r\n        all_tokens = []\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 text \u0645\u06cc\u06ba texts:\r\n            tokens = self.tokenize(text)\r\n            all_tokens.extend(tokens)\r\n        \r\n        unique_tokens = len(set(all_tokens))\r\n        total_tokens = len(all_tokens)\r\n        \r\n        ttr = unique_tokens / total_tokens \u0627\u06af\u0631 total_tokens > 0 else 0\r\n        return ttr\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DataFilteringPipeline:\r\n    def __init__(self):\r\n        self.filters = [\r\n            self.filter_by_success_rate,\r\n            self.filter_by_data_quality,\r\n            self.filter_by_dangerous_behaviors,\r\n            self.filter_by_privacy_concerns\r\n        ]\r\n    \r\n    def filter_dataset(self, dataset):\r\n        """\r\n        Apply multiple filters \u06a9\u0648 \u0635\u0627\u0641 \u06a9\u0627/\u06a9\u06cc dataset\r\n        """\r\n        filtered_dataset = dataset\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 filter_func \u0645\u06cc\u06ba self.filters:\r\n            initial_count = len(filtered_dataset)\r\n            filtered_dataset = filter_func(filtered_dataset)\r\n            removed_count = initial_count - len(filtered_dataset)\r\n            \r\n            print(f"{filter_func.__name__}: Removed {removed_count} samples")\r\n        \r\n        return filtered_dataset\r\n    \r\n    def filter_by_success_rate(self, dataset):\r\n        """\r\n        Remove episodes \u0648\u06c1 failed \u06a9\u0648 complete tasks\r\n        """\r\n        def is_successful(episode):\r\n            # Use domain-specific success criteria\r\n            return episode.get(\'task_success\', False)\r\n        \r\n        return [ep \u06a9\u06d2 \u0644\u06cc\u06d2 ep \u0645\u06cc\u06ba dataset \u0627\u06af\u0631 is_successful(ep)]\r\n    \r\n    def filter_by_data_quality(self, dataset):\r\n        """\r\n        Remove samples \u06a9\u06d2 \u0633\u0627\u062a\u06be poor data quality\r\n        """\r\n        def has_good_quality(sample):\r\n            # Check \u06a9\u06d2 \u0644\u06cc\u06d2 common quality issues\r\n            \u0627\u06af\u0631 self.contains_corrupted_data(sample):\r\n                return False\r\n            \u0627\u06af\u0631 self.is_repetitive_data(sample):\r\n                return False\r\n            \u0627\u06af\u0631 self.has_insufficient_variation(sample):\r\n                return False\r\n            return True\r\n        \r\n        return [sample \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset \u0627\u06af\u0631 has_good_quality(sample)]\r\n    \r\n    def contains_corrupted_data(self, sample):\r\n        """\r\n        Check \u06a9\u06d2 \u0644\u06cc\u06d2 corrupted sensor data\r\n        """\r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 NaN values\r\n        \u0627\u06af\u0631 hasattr(sample[\'observations\'], \'isnan\'):\r\n            \u0627\u06af\u0631 sample[\'observations\'].isnan().any():\r\n                return True\r\n        \r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 impossible values\r\n        \u0627\u06af\u0631 self.has_impossible_physical_values(sample):\r\n            return True\r\n        \r\n        return False\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasusna-\u06af-oawars-babw",children:"\u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasusna \u06af oawars babw"}),"\n",(0,t.jsx)(n.h3,{id:"\u0648\u0646-\u0688\u06cc\u0679-a-\u067e-a-\u067e-rausasussn-\u06af",children:"\u0648\u0646 \u0688\u06cc\u0679 a \u067e a \u067e rausasussn \u06af"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\nimport torchvision.transforms \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 transforms\r\n\u0633\u06d2 PIL import Image\r\n\r\nclass VisionPreprocessor:\r\n    def __init__(self, image_size=(224, 224), normalize=True):\r\n        self.image_size = image_size\r\n        self.normalize = normalize\r\n        \r\n    def preprocess_image(self, image):\r\n        """\r\n        Preprocess \u0627\u06cc\u06a9 single image \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model\r\n        """\r\n        # Convert \u06a9\u0648 PIL image \u0627\u06af\u0631 needed\r\n        \u0627\u06af\u0631 isinstance(image, np.ndarray):\r\n            image = Image.fromarray(image.astype(\'uint8\'))\r\n        \r\n        # Apply preprocessing transforms\r\n        transform_chain = [\r\n            transforms.Resize(self.image_size),\r\n            transforms.ToTensor(),\r\n        ]\r\n        \r\n        \u0627\u06af\u0631 self.normalize:\r\n            # Use ImageNet normalization values\r\n            transform_chain.append(\r\n                transforms.Normalize(\r\n                    mean=[0.485, 0.456, 0.406],\r\n                    std=[0.229, 0.224, 0.225]\r\n                )\r\n            )\r\n        \r\n        transform = transforms.Compose(transform_chain)\r\n        return transform(image)\r\n    \r\n    def augment_image(self, image):\r\n        """\r\n        Apply data augmentation techniques\r\n        """\r\n        augmentation_chain = transforms.Compose([\r\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\r\n            transforms.RandomHorizontalFlip(p=0.5),\r\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        ])\r\n        \r\n        return augmentation_chain(image)\r\n    \r\n    def process_sequence(self, image_sequence, temporal_augmentation=True):\r\n        """\r\n        Process \u0627\u06cc\u06a9 sequence \u06a9\u0627 images\r\n        """\r\n        processed_sequence = []\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 img \u0645\u06cc\u06ba image_sequence:\r\n            processed_img = self.preprocess_image(img)\r\n            processed_sequence.append(processed_img)\r\n        \r\n        # Apply temporal augmentations\r\n        \u0627\u06af\u0631 temporal_augmentation \u0627\u0648\u0631 len(processed_sequence) > 1:\r\n            processed_sequence = self.temporal_augmentation(processed_sequence)\r\n        \r\n        return torch.stack(processed_sequence)\r\n    \r\n    def temporal_augmentation(self, sequence):\r\n        """\r\n        Apply temporal transformations \u06a9\u0648 video sequences\r\n        """\r\n        # Temporal dropout (skip frames)\r\n        \u0627\u06af\u0631 np.random.rand() < 0.1:  # 10% chance\r\n            skip_every = np.random.choice([2, 3, 4])\r\n            sequence = [sequence[\u0645\u06cc\u06ba] \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba \u0645\u06cc\u06ba range(0, len(sequence), skip_every)]\r\n        \r\n        # Reverse sequence\r\n        \u0627\u06af\u0631 np.random.rand() < 0.05:  # 5% chance\r\n            sequence = sequence[::-1]\r\n        \r\n        return sequence\n'})}),"\n",(0,t.jsx)(n.h3,{id:"\u0632-bain-\u06a9-a-\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasassn-\u06af",children:"\u0632 bain \u06a9 a \u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasassn \u06af"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport transformers\r\n\u0633\u06d2 transformers import AutoTokenizer\r\nimport re\r\n\r\nclass LanguagePreprocessor:\r\n    def __init__(self, model_name='bert-base-uncased', max_length=64):\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        self.max_length = max_length\r\n        self.model_name = model_name\r\n        \r\n    def preprocess_text(self, text):\r\n        \"\"\"\r\n        Tokenize \u0627\u0648\u0631 encode natural language text\r\n        \"\"\"\r\n        # \u0635\u0627\u0641 text\r\n        cleaned_text = self.clean_text(text)\r\n        \r\n        # Encode using tokenizer\r\n        encoded = self.tokenizer(\r\n            cleaned_text,\r\n            max_length=self.max_length,\r\n            padding='max_length',\r\n            truncation=True,\r\n            return_tensors='pt'\r\n        )\r\n        \r\n        return {\r\n            'input_ids': encoded['input_ids'].squeeze(0),\r\n            'attention_mask': encoded['attention_mask'].squeeze(0),\r\n            'text': cleaned_text\r\n        }\r\n    \r\n    def clean_text(self, text):\r\n        \"\"\"\r\n        \u0635\u0627\u0641 \u0627\u0648\u0631 normalize text\r\n        \"\"\"\r\n        # Convert \u06a9\u0648 lowercase\r\n        text = text.lower()\r\n        \r\n        # Remove extra whitespace\r\n        text = re.sub(r'\\s+', ' ', text).strip()\r\n        \r\n        # Remove special characters (keep basic punctuation)\r\n        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-]', '', text)\r\n        \r\n        return text\r\n    \r\n    def batch_preprocess(self, text_batch):\r\n        \"\"\"\r\n        Efficiently preprocess \u0627\u06cc\u06a9 batch \u06a9\u0627 texts\r\n        \"\"\"\r\n        cleaned_texts = [self.clean_text(text) \u06a9\u06d2 \u0644\u06cc\u06d2 text \u0645\u06cc\u06ba text_batch]\r\n        \r\n        encoded_batch = self.tokenizer(\r\n            cleaned_texts,\r\n            max_length=self.max_length,\r\n            padding=True,\r\n            truncation=True,\r\n            return_tensors='pt'\r\n        )\r\n        \r\n        return {\r\n            'input_ids': encoded_batch['input_ids'],\r\n            'attention_mask': encoded_batch['attention_mask'],\r\n            'texts': cleaned_texts\r\n        }\r\n    \r\n    def tokenize_with_structure(self, text, task_structure=None):\r\n        \"\"\"\r\n        Tokenize \u06a9\u06d2 \u0633\u0627\u062a\u06be awareness \u06a9\u0627 task structure\r\n        \"\"\"\r\n        # Add special tokens based \u067e\u0631 task structure\r\n        \u0627\u06af\u0631 task_structure == 'instruction_following':\r\n            text = f\"Instruction: {text} Respond:\"\r\n        elif task_structure == 'question_answering':\r\n            text = f\"Question: {text} Answer:\"\r\n        \r\n        return self.preprocess_text(text)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"\u06cc\u06a9\u0634-in-\u0688\u06cc\u0679-a-\u067e-ra-\u06cc-\u067e-rosasassn-\u06af",children:"\u06cc\u06a9\u0634 in \u0688\u06cc\u0679 a \u067e ra \u06cc \u067e rosasassn \u06af"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\n\r\nclass ActionPreprocessor:\r\n    def __init__(self, action_space_config):\r\n        self.action_space_config = action_space_config\r\n        self.normalization_params = None\r\n        \r\n    def preprocess_action(self, action_vector):\r\n        """\r\n        Normalize \u0627\u0648\u0631 validate \u0627\u06cc\u06a9\u0634\u0646 vectors\r\n        """\r\n        # Ensure \u0627\u06cc\u06a9\u0634\u0646 \u06c1\u06d2 \u0645\u06cc\u06ba expected format\r\n        \u0627\u06cc\u06a9\u0634\u0646 = self.validate_action(action_vector)\r\n        \r\n        # Normalize \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06a9\u0648 [-1, 1] range\r\n        normalized_action = self.normalize_action\r\n        \r\n        # Validate \u0628\u0639\u062f normalization\r\n        self.validate_normalized_action(normalized_action)\r\n        \r\n        return normalized_action\r\n    \r\n    def validate_action(self, action_vector):\r\n        """\r\n        Validate \u0627\u06cc\u06a9\u0634\u0646 vector format \u0627\u0648\u0631 content\r\n        """\r\n        \u0627\u06cc\u06a9\u0634\u0646 = np.asarray(action_vector)\r\n        \r\n        # Check dimensions\r\n        expected_dim = self.action_space_config.get)\r\n        \u0627\u06af\u0631 \u0627\u06cc\u06a9\u0634\u0646.shape[-1] != expected_dim:\r\n            raise ValueError\r\n        \r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 NaN \u06cc\u0627 infinite values\r\n        \u0627\u06af\u0631 np.any) \u06cc\u0627 np.any):\r\n            raise ValueError\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\r\n    \r\n    def normalize_action:\r\n        """\r\n        Normalize \u0627\u06cc\u06a9\u0634\u0646\u0632 based \u067e\u0631 \u0627\u06cc\u06a9\u0634\u0646 space limits\r\n        """\r\n        \u0627\u06af\u0631 self.normalization_params \u06c1\u06d2 None:\r\n            self.compute_normalization_params()\r\n        \r\n        # Apply normalization\r\n        normalized = / (self.normalization_params[\'std\'] + 1e-8)\r\n        \r\n        # Clamp \u06a9\u0648 [-1, 1] range \u06a9\u06d2 \u0644\u06cc\u06d2 safety\r\n        normalized = np.clip(normalized, -1.0, 1.0)\r\n        \r\n        return normalized\r\n    \r\n    def compute_normalization_params(self):\r\n        """\r\n        Compute normalization parameters \u0633\u06d2 \u0627\u06cc\u06a9\u0634\u0646 statistics\r\n        """\r\n        # \u06cc\u06c1 \u06a9\u0631\u06d2 \u06af\u0627 typically \u06c1\u0648\u0646\u0627 computed \u0633\u06d2 \u06a9\u0627/\u06a9\u06cc dataset\r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0628, use \u0627\u06cc\u06a9\u0634\u0646 space \u062a\u0634\u06a9\u06cc\u0644\r\n        action_limits = self.action_space_config.get(\'limits\')\r\n        \r\n        \u0627\u06af\u0631 action_limits:\r\n            # Compute mean \u0627\u0648\u0631 std \u0633\u06d2 limits\r\n            mins = np.array(action_limits[\'min\'])\r\n            maxs = np.array(action_limits[\'max\'])\r\n            \r\n            means = (mins + maxs) / 2.0\r\n            stds = (maxs - mins) / 2.0\r\n            \r\n            self.normalization_params = {\r\n                \'mean\': means,\r\n                \'std\': stds\r\n            }\r\n        else:\r\n            # Default normalization parameters\r\n            dummy_action = np.zeros(self.action_space_config.get(\'dimension\', 7))\r\n            self.normalization_params = {\r\n                \'mean\': np.mean(dummy_action, axis=0),\r\n                \'std\': np.std(dummy_action, axis=0) + 1e-8\r\n            }\r\n    \r\n    def discretize_actions(self, continuous_action):\r\n        """\r\n        Convert continuous \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06a9\u0648 discrete \u0627\u06af\u0631 needed\r\n        """\r\n        # \u0645\u062b\u0627\u0644: discretize based \u067e\u0631 \u0627\u06cc\u06a9\u0634\u0646 space \u062a\u0634\u06a9\u06cc\u0644\r\n        \u0627\u06af\u0631 self.action_space_config.get(\'discrete\', False):\r\n            discrete_bins = self.action_space_config.get(\'bins\', 10)\r\n            discretized = np.floor((continuous_action + 1) * (discrete_bins / 2)).astype(int)\r\n            discretized = np.clip(discretized, 0, discrete_bins - 1)\r\n            return discretized\r\n        \r\n        return continuous_action\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0627\u062e\u0644\u0627-at-at-\u062d\u0641\u0638-\u062d\u0641\u0638-atat",children:"\u0627\u062e\u0644\u0627 at at \u062d\u0641\u0638 \u062d\u0641\u0638 atat"}),"\n",(0,t.jsx)(n.h3,{id:"taia-\u0635-b-\u06a9-a-\u067e-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06c1-\u06af-\u06af",children:"taia \u0635 b \u06a9 a \u067e \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06af \u06af"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class BiasDetectionFramework:\r\n    def __init__(self):\r\n        self.bias_detectors = {\r\n            'demographic_bias': self.detect_demographic_bias,\r\n            'action_bias': self.detect_action_bias,\r\n            'language_bias': self.detect_language_bias,\r\n            'environment_bias': self.detect_environment_bias\r\n        }\r\n    \r\n    def audit_dataset_for_bias(self, dataset):\r\n        \"\"\"\r\n        Comprehensive bias audit \u06a9\u0627 \u06a9\u0627/\u06a9\u06cc dataset\r\n        \"\"\"\r\n        bias_report = {}\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 bias_type, detector \u0645\u06cc\u06ba self.bias_detectors.items():\r\n            bias_report[bias_type] = detector(dataset)\r\n        \r\n        return bias_report\r\n    \r\n    def detect_demographic_bias(self, dataset):\r\n        \"\"\"\r\n        Detect bias related \u06a9\u0648 demographic groups\r\n        \"\"\"\r\n        # \u06cc\u06c1 \u06a9\u0631\u06d2 \u06af\u0627 involve analyzing \u06a9\u0627/\u06a9\u06cc demographic characteristics\r\n        # \u06a9\u0627 human demonstrators \u0627\u0648\u0631 identifying disparities\r\n        \r\n        demographics_analysis = {\r\n            'gender_representation': self.analyze_gender_representation(dataset),\r\n            'age_distribution': self.analyze_age_distribution(dataset),\r\n            'cultural_bias_indicators': self.identify_cultural_biases(dataset)\r\n        }\r\n        \r\n        return demographics_analysis\r\n    \r\n    def detect_action_bias(self, dataset):\r\n        \"\"\"\r\n        Detect bias \u0645\u06cc\u06ba \u0627\u06cc\u06a9\u0634\u0646 demonstrations\r\n        \"\"\"\r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 stereotypical patterns\r\n        action_patterns = self.extract_action_patterns(dataset)\r\n        \r\n        bias_indicators = {\r\n            'stereotypical_actions': self.identify_stereotypes(action_patterns),\r\n            'dominance_patterns': self.analyze_social_dominance_patterns(dataset),\r\n            'safety_bias': self.check_for_safety_disparities(dataset)\r\n        }\r\n        \r\n        return bias_indicators\n"})}),"\n",(0,t.jsx)(n.h3,{id:"ra-\u0632-dar-\u06cc-s-\u06d2-t-\u062d\u0641\u0638",children:"ra \u0632 dar \u06cc s \u06d2 t \u062d\u0641\u0638"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PrivacyProtectionFramework:\r\n    def __init__(self):\r\n        self.privacy_tools = [\r\n            self.blur_faces_in_images,\r\n            self.remove_identifiable_info,\r\n            self.apply_differential_privacy\r\n        ]\r\n    \r\n    def protect_privacy(self, dataset):\r\n        """\r\n        Apply privacy protection measures\r\n        """\r\n        protected_dataset = dataset.copy()\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 tool \u0645\u06cc\u06ba self.privacy_tools:\r\n            protected_dataset = tool(protected_dataset)\r\n        \r\n        return protected_dataset\r\n    \r\n    def blur_faces_in_images(self, dataset):\r\n        """\r\n        Blur faces \u0645\u06cc\u06ba collected images \u06a9\u0648 protect identity\r\n        """\r\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\')\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 sample \u0645\u06cc\u06ba dataset:\r\n            \u0627\u06af\u0631 \'images\' \u0645\u06cc\u06ba sample:\r\n                \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba, image \u0645\u06cc\u06ba enumerate(sample[\'images\']):\r\n                    # Convert \u06a9\u0648 opencv format \u0627\u06af\u0631 needed\r\n                    \u0627\u06af\u0631 isinstance(image, PIL.Image.Image):\r\n                        image_cv = np.array(image)\r\n                        image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\r\n                    else:\r\n                        image_cv = image\r\n                    \r\n                    # Detect faces\r\n                    gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\r\n                    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\r\n                    \r\n                    # Blur faces\r\n                    \u06a9\u06d2 \u0644\u06cc\u06d2 (x, y, w, h) \u0645\u06cc\u06ba faces:\r\n                        face_region = image_cv[y:y+h, x:x+w]\r\n                        blurred_face = cv2.GaussianBlur(face_region, (99, 99), 30)\r\n                        image_cv[y:y+h, x:x+w] = blurred_face\r\n                    \r\n                    # Convert \u067e\u06cc\u0686\u06be\u06d2 \u06a9\u0648 original format\r\n                    \u0627\u06af\u0631 isinstance:\r\n                        image_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\r\n                        sample[\'images\'][\u0645\u06cc\u06ba] = PIL.Image.fromarray(image_rgb)\r\n                    else:\r\n                        sample[\'images\'][\u0645\u06cc\u06ba] = image_cv\r\n        \r\n        return dataset\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0688\u06cc\u0679-a-\u0688\u06cc\u0679-a-\u06a9\u0679\u06be-a-arna-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u062e-\u062e-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688\u06cc\u0679-\u0688-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u0622\u0641-\u062c-\u062c",children:"\u0688\u06cc\u0679 a \u0688\u06cc\u0679 a \u06a9\u0679\u06be a arna \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u062e \u062e \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688\u06cc\u0679 \u0688 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u0622\u0641 \u062c \u062c"}),"\n",(0,t.jsx)(n.h3,{id:"re\u0650snaiuma-\u062e\u0637s-l-\u06cc\u06d2-l-\u06cc\u06d2-\u0630-mi-\u06c1-idar-\u0688\u06cc\u0679-a-a-a-ai-j-\u06a9-sa",children:"re\u0650snaiuma \u062e\u0637s l \u06cc\u06d2 l \u06cc\u06d2 \u0630 mi \u06c1 idar \u0688\u06cc\u0679 a a a ai j \u06a9 sa"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'DATA_COLLECTION_ETHICS_GUIDELINES = {\r\n    \'informed_consent\': {\r\n        \'requirement\': "\u062a\u0645\u0627\u0645 participants \u0636\u0631\u0648\u0631 provide informed consent",\r\n        \'\u0646\u0641\u0627\u0630\': [\r\n            "Clear explanation \u06a9\u0627 data use",\r\n            "Voluntary participation",\r\n            "\u0635\u062d\u06cc\u062d \u06a9\u0648 withdraw"\r\n        ]\r\n    },\r\n    \'privacy_protection\': {\r\n        \'requirement\': "Protect participant privacy \u0627\u0648\u0631 confidentiality",\r\n        \'\u0646\u0641\u0627\u0630\': [\r\n            "Data anonymization",\r\n            "Secure storage protocols",\r\n            "Access control mechanisms"\r\n        ]\r\n    },\r\n    \'fair_compensation\': {\r\n        \'requirement\': "Fair compensation \u06a9\u06d2 \u0644\u06cc\u06d2 participant contributions",\r\n        \'\u0646\u0641\u0627\u0630\': [\r\n            "Equitable pay rates",\r\n            "Recognition \u06a9\u0627 contributions",\r\n            "Community benefit sharing"\r\n        ]\r\n    },\r\n    \'inclusive_design\': {\r\n        \'requirement\': "Ensure diverse \u0627\u0648\u0631 inclusive dataset collection",\r\n        \'\u0646\u0641\u0627\u0630\': [\r\n            "Diverse participant recruitment",\r\n            "Multiple interaction styles",\r\n            "Accessibility considerations"\r\n        ]\r\n    },\r\n    \'transparency\': {\r\n        \'requirement\': "Transparency \u0645\u06cc\u06ba data collection \u0627\u0648\u0631 use",\r\n        \'\u0646\u0641\u0627\u0630\': [\r\n            "Public dataset documentation",\r\n            "Clear usage terms",\r\n            "Regular reporting"\r\n        ]\r\n    }\r\n}\r\n\r\ndef establish_ethics_review_process():\r\n    """\r\n    Establish \u0627\u06cc\u06a9 ethics review process \u06a9\u06d2 \u0644\u06cc\u06d2 VLA data collection\r\n    """\r\n    ethics_board = {\r\n        \'\u062a\u0631\u06a9\u06cc\u0628\': [\r\n            "\u0630\u06c1\u0627\u0646\u062a ethics researchers",\r\n            "Legal experts", \r\n            "Community representatives",\r\n            "Technical experts"\r\n        ],\r\n        \'review_criteria\': [\r\n            "Privacy protection measures",\r\n            "Bias mitigation strategies",\r\n            "Participant rights safeguards",\r\n            "Social impact assessment"\r\n        ],\r\n        \'review_process\': "Mandatory review \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0645\u0627\u0645 \u0646\u06cc\u0627 data collection initiatives"\r\n    }\r\n    \r\n    return ethics_board\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u062e-laa-\u0635\u06c1",children:"\u062e LAA \u0635\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"\u06cc\u06c1 \u0627\u0644\u0644 l \u06cc \u06ba"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{}),"\n",(0,t.jsx)(n.li,{}),"\n",(0,t.jsx)(n.li,{}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"4\r\n5."}),"\n",(0,t.jsx)(n.p,{children:"\u0627\u0648\u0627\u0624\u0646\u0686\u0627- \u0627\u0650\u0633\u0648\u0633\u0648\u0627\u0644\u0679\u06cc \u060c \u0645\u062a\u0646\u0648 \u060c \u0627\u0648\u0627\u0648\u0631 \u0627\u062d\u0644 \u0627\u0644\u0627 \u0637 \u0637 vr \u062c \u067e\u0627\u06af\u0644 \u062c umadi \u06c1\u06d2 bun \u06cc adaid \u06cc \u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc jaula \u06af \u06af \u06cc \u06cc \u06cc \u06cc \u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06cc \u06a9 \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 a \u06a9 \u06a9 a \u06a9 a \u06cc a \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 a \u06a9 al asal ia \u06d2 maa \u06d2 maa \u0688 li \u0632 \u06a9\u06d2 stai stai \u06be sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc staul\u06cc sat\u06cc satis stati \u06be sati \u06be sati \u06be sati \u06be sati \u06be sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sati\u06cc sati\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sat\u06cc sati \u06be stati \u06be sati \u06be stati \u06be stati \u06be stati \u06be stati \u06be stati \u06be stati sati \u06be stati sati \u06be stati sati \u06be stati sati \u06be sati \u06be sati \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be sat\u06cc \u06be st\u06cc \u06be st\u06cc \u06be sat\u06cc \u06be sat\u06cc \u06be sat\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc st\u06cc \u06be st\u06cc \u06be st\u06cc \u06be \u06be st\u06cc \u06be \u06be st\u06cc \u06be st\u06cc \u06be st\u06cc sat\u06cc sat\u06cc"})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var a=r(6540);const t={},s=a.createContext(t);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);