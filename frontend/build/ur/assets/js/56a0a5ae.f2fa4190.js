"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[1565],{7592:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/week-4-advanced-vla-applications/4-1-advanced-vla-applications","title":"\u062f\u0679 4: \u0627\u0639\u0644\u06cc \u062f\u0631\u06cc\u06cc\u062c\u06d2 \u06a9\u06cc vla a \u06cc\u067e la \u06cc \u06a9\u06cc\u0634 n \u0632","description":"\u062c a \u062c","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/week-4-advanced-vla-applications/4-1-advanced-vla-applications.md","sourceDirName":"module-4/week-4-advanced-vla-applications","slug":"/module-4/week-4-advanced-vla-applications/4-1-advanced-vla-applications","permalink":"/ur/docs/module-4/week-4-advanced-vla-applications/4-1-advanced-vla-applications","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-4-advanced-vla-applications/4-1-advanced-vla-applications.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"itt \u06c1 3: vla an \u0636 \u0645\u0645\u0627\u0645","permalink":"/ur/docs/module-4/week-3-vla-integration/3-1-vla-integration-with-robotics"}}');var s=r(4848),t=r(8453);const i={sidebar_position:3,difficulty:"advanced"},l="\u062f\u0679 4: \u0627\u0639\u0644\u06cc \u062f\u0631\u06cc\u06cc\u062c\u06d2 \u06a9\u06cc vla a \u06cc\u067e la \u06cc \u06a9\u06cc\u0634 n \u0632",o={},c=[{value:"\u062c a \u062c",id:"\u062c-a-\u062c",level:2},{value:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d",id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",level:2},{value:"\u0627\u0639\u0644 \u06cc \u06a9\u06cc vla \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641",id:"\u0627\u0639\u0644-\u06cc-\u06a9\u06cc-vla-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641",level:2},{value:"\u0641 aiuni \u0688\u06cc\u0634 ni \u06a9\u06d2 maa \u0688 l \u06a9\u06d2 l \u06cc\u06d2 rewbwaus",id:"\u0641-aiuni-\u0688\u06cc\u0634-ni-\u06a9\u06d2-maa-\u0688-l-\u06a9\u06d2-l-\u06cc\u06d2-rewbwaus",level:3},{value:"ml \u0679\u06cc maul ss\u06ccsn\u06cc \u06a9\u06d2 nour",id:"ml-\u0679\u06cc-maul-ss\u06ccsn\u06cc-\u06a9\u06d2-nour",level:2},{value:"ss \u06cc l \u0641 sswaurwa \u0626\u0632\u0688 lsnn \u06af \u06a9\u06d2 l \u06cc\u06d2 vla",id:"ss-\u06cc-l-\u0641-sswaurwa-\u0626\u0632\u0688-lsnn-\u06af-\u06a9\u06d2-l-\u06cc\u06d2-vla",level:3},{value:"maUab \u06c1 t ssiun \u06d2 \u06a9\u06d2 saat \u06be vla",id:"mauab-\u06c1-t-ssiun-\u06d2-\u06a9\u06d2-saat-\u06be-vla",level:3},{value:"\u0627\u0650\u0633\u0646\u0633\u0627\u0646 \u06cc- \u0631\u0648\u0628\u0648 \u0628\u0648\u0628\u0648 \u0628\u0648\u0628\u0648 \u0628\u0627\u0626\u0648\u0645\u06cc t \u0633\u0627\u0633 \u06be \u200b\u200b\u06be \u06be vla",id:"\u0627\u0650\u0633\u0646\u0633\u0627\u0646-\u06cc--\u0631\u0648\u0628\u0648-\u0628\u0648\u0628\u0648-\u0628\u0648\u0628\u0648-\u0628\u0627\u0626\u0648\u0645\u06cc-t-\u0633\u0627\u0633-\u06be-\u06be-\u06be-vla",level:2},{value:"\u0642 \u0642 \u0632 \u0632 a \u06a9 ahahass",id:"\u0642-\u0642-\u0632-\u0632-a-\u06a9-ahahass",level:3},{value:"baaaumi\u06cc taia\u0624n \u06a9\u06d2 saaata \u06be \u06a9 am \u067e \u067e \u067e am \u067e \u067e am am il drimad",id:"baaaumi\u06cc-taia\u0624n-\u06a9\u06d2-saaata-\u06be-\u06a9-am-\u067e-\u067e-\u067e-am-\u067e-\u067e-am-am-il-drimad",level:3},{value:"\u0627\u0639\u0644 \u06cc \u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 n \u0632",id:"\u0627\u0639\u0644-\u06cc-\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc\u0634-\u06a9\u06cc\u0634-\u06a9\u06cc\u0634-n-\u0632",level:2},{value:"dexterous \u06c1\u06cc ra \u067e\u06be\u06cc ri \u06cc",id:"dexterous-\u06c1\u06cc-ra-\u067e\u06be\u06cc-ri-\u06cc",level:3},{value:"mlai-rwbo vausr \u0688\u06cc n \u06cc\u0634 n \u06cc\u0634 in",id:"mlai-rwbo-vausr-\u0688\u06cc-n-\u06cc\u0634-n-\u06cc\u0634-in",level:3},{value:"\u062a\u062a\u0634\u062e\u06cc\u0635 \u0627\u0648\u0627\u0648\u0631 \u0628\u06cc\u0628\u0646\u0686 \u0645\u0627\u0631\u06cc\u06cc\u06a9\u0646\u06af",id:"\u062a\u062a\u0634\u062e\u06cc\u0635-\u0627\u0648\u0627\u0648\u0631-\u0628\u06cc\u0628\u0646\u0686-\u0645\u0627\u0631\u06cc\u06cc\u06a9\u0646\u06af",level:2},{value:"\u0639\u0645\u0627\u0644\u06c1 \u0648\u0648\u0631\u06a9 \u0627\u0650\u0633",id:"\u0639\u0645\u0627\u0644\u06c1-\u0648\u0648\u0631\u06a9-\u0627\u0650\u0633",level:2},{value:"\u062e LAA \u0635\u06c1",id:"\u062e-laa-\u0635\u06c1",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"\u062f\u0679-4-\u0627\u0639\u0644\u06cc-\u062f\u0631\u06cc\u06cc\u062c\u06d2-\u06a9\u06cc-vla-a-\u06cc\u067e-la-\u06cc-\u06a9\u06cc\u0634-n-\u0632",children:"\u062f\u0679 4: \u0627\u0639\u0644\u06cc \u062f\u0631\u06cc\u06cc\u062c\u06d2 \u06a9\u06cc vla a \u06cc\u067e la \u06cc \u06a9\u06cc\u0634 n \u0632"})}),"\n",(0,s.jsx)(e.h2,{id:"\u062c-a-\u062c",children:"\u062c a \u062c"}),"\n",(0,s.jsx)(e.p,{children:"\u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 N \u06c1\u0641 Maussm \u06a9 O Maussmiu bnaa \u06cc a\u06d4"}),"\n",(0,s.jsx)(e.h2,{id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",children:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d"}),"\n",(0,s.jsx)(e.p,{children:"\u06a9\u06d2 \u0630 \u06a9 \u06a9 \u06a9 \u06a9 a/\u06a9\u06cc \u06a9\u06cc \u062e \u062e \u062e \u062e atatam \u06a9 a \u06cc\u06c1 \u06cc\u06c1 \u06c1\u0641 \u06c1\u0641 \u0622\u067e \u0622\u067e \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06d2 \u06d2 \u06d2 \u06d2 \u06af \u06af \u06af \u06af \u06af"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Explore state-\u06a9\u0627-\u06a9\u0627/\u06a9\u06cc-art VLA applications"}),"\n",(0,s.jsx)(e.li,{children:"Understand human-\u0631\u0648\u0628\u0648\u0679 collaboration using \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632"}),"\n",(0,s.jsx)(e.li,{children:"\u0645\u0644\u0679\u06cc \u0645\u0648\u0688\u0644 \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06cc \u062a\u06a9\u0646\u06cc\u06a9 \u06a9\u06cc \u062a\u062d\u0642\u06cc\u0642\u0627\u062a \u06a9\u0631\u06cc\u06ba"}),"\n",(0,s.jsx)(e.li,{children:"\u0627\u0639\u0644\u06cc \u062f\u0631\u06cc\u06cc\u062c\u06d2 \u06a9\u06cc vla ssaumau \u06a9\u06d2 ll \u06cc\u06d2 \u067e\u06cc\u0686\u06cc \u067e\u06cc\u0686\u06cc \u06a9 amwau \u06a9 o juna \u0641\u0630 ina \u0641\u0630 \u06a9 ra"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"\u0627\u0639\u0644-\u06cc-\u06a9\u06cc-vla-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641",children:"\u0627\u0639\u0644 \u06cc \u06a9\u06cc vla \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641"}),"\n",(0,s.jsx)(e.h3,{id:"\u0641-aiuni-\u0688\u06cc\u0634-ni-\u06a9\u06d2-maa-\u0688-l-\u06a9\u06d2-l-\u06cc\u06d2-rewbwaus",children:"\u0641 aiuni \u0688\u06cc\u0634 ni \u06a9\u06d2 maa \u0688 l \u06a9\u06d2 l \u06cc\u06d2 rewbwaus"}),"\n",(0,s.jsx)(e.p,{children:"\u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\u0633\u06d2 transformers import CLIPVisionModel, CLIPTextModel\r\n\r\nclass FoundationVLA:\r\n    def __init__(self):\r\n        super(FoundationVLA, self).__init__()\r\n        \r\n        # Use pre-trained CLIP \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 foundation\r\n        self.vision_encoder = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")\r\n        self.text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")\r\n        \r\n        # Task-specific \u0627\u06cc\u06a9\u0634\u0646 head\r\n        self.action_head = nn.Sequential(\r\n            nn.Linear(512, 256),  # Based \u067e\u0631 CLIP embedding size\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 7)  # 7-DOF \u0627\u06cc\u06a9\u0634\u0646 space\r\n        )\r\n        \r\n        # Learnable fusion layer\r\n        self.fusion = nn.MultiheadAttention(embed_dim=512, num_heads=8)\r\n    \r\n    def forward(self, pixel_values, input_ids, attention_mask):\r\n        # Encode vision \u0627\u0648\u0631 text using foundation models\r\n        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\r\n        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\r\n        \r\n        # Get embeddings\r\n        vision_embeds = vision_outputs.last_hidden_state\r\n        text_embeds = text_outputs.last_hidden_state\r\n        \r\n        # Fuse multimodal representations\r\n        fused_embeds, attention_weights = self.fusion(\r\n            query=vision_embeds,\r\n            key=text_embeds,\r\n            value=text_embeds\r\n        )\r\n        \r\n        # Average \u06a9\u06d2 \u0627\u0648\u067e\u0631 sequence dimension\r\n        fused_features = fused_embeds.mean(dim=1)\r\n        \r\n        # Generate \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(fused_features)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n'})}),"\n",(0,s.jsx)(e.h3,{id:""}),"\n",(0,s.jsx)(e.p,{children:"\u062f \u06a9 \u06a9 \u06a9 \u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9 \u06a9 \u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class HierarchicalVLA:\r\n    def __init__(self):\r\n        super(HierarchicalVLA, self).__init__()\r\n        \r\n        # \u0627\u0648\u0646\u0686\u0627-level planner (task decomposition)\r\n        self.task_planner = TaskPlanner()\r\n        \r\n        # Mid-level skill selector\r\n        self.skill_selector = SkillSelector()\r\n        \r\n        # \u06a9\u0645-level \u0627\u06cc\u06a9\u0634\u0646 generator\r\n        self.action_generator = LowLevelActionGenerator()\r\n        \r\n    def forward(self, image, instruction):\r\n        # Step 1: Task planning\r\n        subtasks = self.task_planner(image, instruction)\r\n        \r\n        # Step 2: Skill selection \u06a9\u06d2 \u0644\u06cc\u06d2 each subtask\r\n        skill_sequence = []\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 subtask \u0645\u06cc\u06ba subtasks:\r\n            skill = self.skill_selector(image, subtask)\r\n            skill_sequence.append(skill)\r\n        \r\n        # Step 3: Generate \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06a9\u06d2 \u0644\u06cc\u06d2 each skill\r\n        all_actions = []\r\n        current_image = image\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 skill \u0645\u06cc\u06ba skill_sequence:\r\n            \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_generator(current_image, skill)\r\n            all_actions.extend\r\n            \r\n            # Update image \u0628\u0639\u062f \u0627\u06cc\u06a9\u0634\u0646 execution (simulated)\r\n            current_image = self.update_image\r\n        \r\n        return all_actions\r\n\r\nclass TaskPlanner:\r\n    def __init__(self):\r\n        super(TaskPlanner, self).__init__()\r\n        # \u0628\u0691\u0627 language model \u06a9\u06d2 \u0644\u06cc\u06d2 task decomposition\r\n        \u0633\u06d2 transformers import GPT2LMHeadModel, GPT2Tokenizer\r\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\n        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\r\n        \r\n        # Add padding token \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba present\r\n        \u0627\u06af\u0631 self.tokenizer.pad_token \u06c1\u06d2 None:\r\n            self.tokenizer.pad_token = self.tokenizer.eos_token\r\n    \r\n    def forward(self, image, instruction):\r\n        # Task decomposition using language model\r\n        prompt = f\"Decompose \u06cc\u06c1 task: {instruction}\\nSubtasks:\"\r\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\r\n        \r\n        \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n            outputs = self.model.generate(\r\n                **inputs,\r\n                max_length=100,\r\n                num_return_sequences=1,\r\n                pad_token_id=self.tokenizer.eos_token_id\r\n            )\r\n        \r\n        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\r\n        subtasks = self.parse_subtasks(generated_text)\r\n        \r\n        return subtasks\r\n    \r\n    def parse_subtasks(self, text):\r\n        # Parse generated text into structured subtasks\r\n        # \u0646\u0641\u0627\u0630 \u06a9\u0631\u06d2 \u06af\u0627 extract subtasks \u0633\u06d2 generated text\r\n        pass\n"})}),"\n",(0,s.jsx)(e.h2,{id:"ml-\u0679\u06cc-maul-ss\u06ccsn\u06cc-\u06a9\u06d2-nour",children:"ml \u0679\u06cc maul ss\u06ccsn\u06cc \u06a9\u06d2 nour"}),"\n",(0,s.jsx)(e.h3,{id:"ss-\u06cc-l-\u0641-sswaurwa-\u0626\u0632\u0688-lsnn-\u06af-\u06a9\u06d2-l-\u06cc\u06d2-vla",children:"ss \u06cc l \u0641 sswaurwa \u0626\u0632\u0688 lsnn \u06af \u06a9\u06d2 l \u06cc\u06d2 vla"}),"\n",(0,s.jsx)(e.p,{children:"atrb \u06cc t waul a \u0650 s maaus maausli \u062e vad ssautah snaur asataamal asrat \u06d2 \u06c1 swa:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SelfSupervisedVLA:\r\n    def __init__(self):\r\n        super(SelfSupervisedVLA, self).__init__()\r\n        \r\n        # Encoder networks\r\n        self.vision_encoder = self.build_vision_encoder()\r\n        self.text_encoder = self.build_text_encoder()\r\n        \r\n        # Projection heads \u06a9\u06d2 \u0644\u06cc\u06d2 contrastive learning\r\n        self.vision_projection = nn.Linear(512, 128)\r\n        self.text_projection = nn.Linear(512, 128)\r\n        \r\n        # Temperature \u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631 \u06a9\u06d2 \u0644\u06cc\u06d2 contrastive loss\r\n        self.temperature = nn.\u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631(torch.ones([]) * np.log(1 / 0.07))\r\n    \r\n    def build_vision_encoder(self):\r\n        return nn.Sequential(\r\n            # Vision transformer \u06cc\u0627 ResNet\r\n        )\r\n    \r\n    def build_text_encoder(self):\r\n        return nn.Sequential(\r\n            # BERT, GPT, \u06cc\u0627 similar\r\n        )\r\n    \r\n    def forward(self, images, texts):\r\n        # Encode images \u0627\u0648\u0631 texts\r\n        image_features = self.vision_encoder(images)\r\n        text_features = self.text_encoder(texts)\r\n        \r\n        # Project \u06a9\u0648 common space\r\n        image_projections = self.vision_projection(image_features)\r\n        text_projections = self.text_projection(text_features)\r\n        \r\n        # Normalize\r\n        image_projections = F.normalize(image_projections, dim=-1)\r\n        text_projections = F.normalize(text_projections, dim=-1)\r\n        \r\n        return image_projections, text_projections\r\n    \r\n    def contrastive_loss(self, image_projections, text_projections):\r\n        # Calculate contrastive loss\r\n        logits = torch.matmul(image_projections, text_projections.T) * self.temperature.exp()\r\n        \r\n        labels = torch.arange(logits.shape[0], device=logits.device)\r\n        \r\n        # Cross entropy loss\r\n        loss_i = F.cross_entropy(logits, labels)\r\n        loss_t = F.cross_entropy(logits.T, labels)\r\n        \r\n        return (loss_i + loss_t) / 2\n"})}),"\n",(0,s.jsx)(e.h3,{id:"mauab-\u06c1-t-ssiun-\u06d2-\u06a9\u06d2-saat-\u06be-vla",children:"maUab \u06c1 t ssiun \u06d2 \u06a9\u06d2 saat \u06be vla"}),"\n",(0,s.jsx)(e.p,{children:"mauaaur \u06d2 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u06a9 \u0634 \u06a9 \u06a9 \u06a9 \u0634 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0634 \u0634 \u06a9 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u0634 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0634 i \u0634 \u0634 \u0634 \u0634 \u0634 \u0634"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ImitationLearningVLA:\r\n    def __init__(self):\r\n        super(ImitationLearningVLA, self).__init__()\r\n        \r\n        # Policy network\r\n        self.vla_policy = VLAModel()\r\n        \r\n        # Behavioral cloning loss\r\n        self.mse_loss = nn.MSELoss()\r\n    \r\n    def forward(self, images, instructions):\r\n        return self.vla_policy(images, instructions)\r\n    \r\n    def imitation_learning_loss(self, expert_states, expert_actions, images, instructions):\r\n        # Get policy \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        policy_actions = self.vla_policy(images, instructions)\r\n        \r\n        # Calculate behavioral cloning loss\r\n        loss = self.mse_loss(policy_actions, expert_actions)\r\n        \r\n        return loss\r\n\r\n# Training loop \u06a9\u06d2 \u0633\u0627\u062a\u06be demonstration data\r\ndef train_with_demonstrations(model, dataset, num_epochs=100):\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    \r\n    \u06a9\u06d2 \u0644\u06cc\u06d2 epoch \u0645\u06cc\u06ba range(num_epochs):\r\n        total_loss = 0\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 batch \u0645\u06cc\u06ba torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True):\r\n            optimizer.zero_grad()\r\n            \r\n            # Extract batch data\r\n            images = batch['images']\r\n            instructions = batch['instructions']\r\n            expert_actions = batch['expert_actions']\r\n            \r\n            # Calculate imitation learning loss\r\n            loss = model.imitation_learning_loss(\r\n                batch['states'], expert_actions, images, instructions\r\n            )\r\n            \r\n            # Backpropagate\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            total_loss += loss.item()\r\n        \r\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataset):.4f}\")\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u0627\u0650\u0633\u0646\u0633\u0627\u0646-\u06cc--\u0631\u0648\u0628\u0648-\u0628\u0648\u0628\u0648-\u0628\u0648\u0628\u0648-\u0628\u0627\u0626\u0648\u0645\u06cc-t-\u0633\u0627\u0633-\u06be-\u06be-\u06be-vla",children:"\u0627\u0650\u0633\u0646\u0633\u0627\u0646 \u06cc- \u0631\u0648\u0628\u0648 \u0628\u0648\u0628\u0648 \u0628\u0648\u0628\u0648 \u0628\u0627\u0626\u0648\u0645\u06cc t \u0633\u0627\u0633 \u06be \u200b\u200b\u06be \u06be vla"}),"\n",(0,s.jsx)(e.h3,{id:"\u0642-\u0642-\u0632-\u0632-a-\u06a9-ahahass",children:"\u0642 \u0642 \u0632 \u0632 a \u06a9 ahahass"}),"\n",(0,s.jsx)(e.p,{children:"baud\u06cc\u06c1\u06cc \u06cc\u06c1\u06ccaarsi\u06ccs \u06a9\u06cc taal \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0650 \u0650 \u0650"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class NaturalLanguageVLA:\r\n    def __init__(self, vla_model):\r\n        self.vla_model = vla_model\r\n        \r\n        # Language understanding \u0645\u0627\u0688\u06cc\u0648\u0644\r\n        \u0633\u06d2 transformers import pipeline\r\n        self.question_answering = pipeline("question-answering")\r\n        self.text_classifier = pipeline("text-classification")\r\n        \r\n        # Context tracking\r\n        self.context_memory = []\r\n    \r\n    def process_command(self, user_command, current_image):\r\n        # Understand user \u06a9\u0645\u0627\u0646\u0688\r\n        intent = self.classify_intent(user_command)\r\n        \r\n        \u0627\u06af\u0631 intent == "navigation":\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.handle_navigation_command(user_command, current_image)\r\n        elif intent == "manipulation":\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.handle_manipulation_command(user_command, current_image)\r\n        elif intent == "information":\r\n            response = self.handle_information_request(user_command, current_image)\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.generate_action_for_response(response)\r\n        else:\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.handle_default_command(user_command, current_image)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\r\n    \r\n    def classify_intent:\r\n        # Classify \u06a9\u0627/\u06a9\u06cc intent \u06a9\u0627 \u06a9\u0627/\u06a9\u06cc user \u06a9\u0645\u0627\u0646\u0688\r\n        result = self.text_classifier\r\n        return result[0][\'label\'].lower()\r\n    \r\n    def handle_navigation_command:\r\n        # Extract destination \u0633\u06d2 \u06a9\u0645\u0627\u0646\u0688\r\n        destination = self.extract_destination\r\n        \r\n        # Generate navigation \u0627\u06cc\u06a9\u0634\u0646\r\n        \u0627\u06cc\u06a9\u0634\u0646 = self.vla_model\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\r\n    \r\n    def extract_destination:\r\n        # Extract destination \u0633\u06d2 natural language\r\n        # \u06cc\u06c1 \u06a9\u0631\u06d2 \u06af\u0627 use \u0645\u0632\u06cc\u062f sophisticated NLP \u0645\u06cc\u06ba practice\r\n        keywords = ["go \u06a9\u0648", "navigate \u06a9\u0648", "move \u06a9\u0648", "walk \u06a9\u0648"]\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 keyword \u0645\u06cc\u06ba keywords:\r\n            \u0627\u06af\u0631 keyword \u0645\u06cc\u06ba \u06a9\u0645\u0627\u0646\u0688.lower():\r\n                return \u06a9\u0645\u0627\u0646\u0688.lower().split(keyword)[-1].strip()\r\n        \r\n        return "unknown location"\r\n    \r\n    def update_context:\r\n        # Update interaction history\r\n        self.context_memory.append({\r\n            \'\u06a9\u0645\u0627\u0646\u0688\': \u06a9\u0645\u0627\u0646\u0688,\r\n            \'\u0627\u06cc\u06a9\u0634\u0646\': \u0627\u06cc\u06a9\u0634\u0646,\r\n            \'result\': result\r\n        })\r\n        \r\n        # Keep \u0635\u0631\u0641 recent context\r\n        \u0627\u06af\u0631 len(self.context_memory) > 10:\r\n            self.context_memory = self.context_memory[-10:]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"baaaumi\u06cc-taia\u0624n-\u06a9\u06d2-saaata-\u06be-\u06a9-am-\u067e-\u067e-\u067e-am-\u067e-\u067e-am-am-il-drimad",children:"baaaumi\u06cc taia\u0624n \u06a9\u06d2 saaata \u06be \u06a9 am \u067e \u067e \u067e am \u067e \u067e am am il drimad"}),"\n",(0,s.jsx)(e.p,{children:"\u0627\u0648\u0633\u0646\u0648 \u06ba \u06a9 j \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u0686 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 \u06cc\u06a9 \u06be \u06be \u06be \u06be \u06be \u06cc\u06a9 is"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class CollaborativeVLA:\r\n    def __init__(self, robot_vla, human_model):\r\n        self.robot_vla = robot_vla\r\n        self.human_model = human_model\r\n        \r\n        # Task allocation \u0645\u0627\u0688\u06cc\u0648\u0644\r\n        self.task_allocator = TaskAllocationModule()\r\n        \r\n        # \u0645\u0648\u0627\u0635\u0644\u0627\u062a interface\r\n        self.comms_interface = CommunicationInterface()\r\n    \r\n    def execute_collaborative_task(self, task_description, human_feedback=None):\r\n        # Analyze task \u0627\u0648\u0631 allocate components\r\n        robot_tasks, human_tasks = self.task_allocator.allocate(\r\n            task_description, human_feedback\r\n        )\r\n        \r\n        # Execute \u0631\u0648\u0628\u0648\u0679 portion\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 robot_task \u0645\u06cc\u06ba robot_tasks:\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.robot_vla(robot_task['image'], robot_task['instruction'])\r\n            # Execute \u0627\u06cc\u06a9\u0634\u0646 \u0627\u0648\u0631 monitor results\r\n            \r\n            # Communicate progress \u06a9\u0648 human\r\n            self.comms_interface.send_status(robot_task['status'])\r\n        \r\n        # Wait \u06a9\u06d2 \u0644\u06cc\u06d2 human tasks completion\r\n        human_completion = self.wait_for_human_completion(human_tasks)\r\n        \r\n        # Continue \u06a9\u06d2 \u0633\u0627\u062a\u06be next tasks \u0627\u06af\u0631 needed\r\n        \u0627\u06af\u0631 human_completion:\r\n            return self.continue_task(task_description)\r\n        \r\n        return \"completed\"\r\n    \r\n    def wait_for_human_completion(self, human_tasks):\r\n        # Wait \u06a9\u06d2 \u0644\u06cc\u06d2 human \u06a9\u0648 complete assigned tasks\r\n        # \u06cc\u06c1 involves \u0645\u0648\u0627\u0635\u0644\u0627\u062a \u06a9\u06d2 \u0633\u0627\u062a\u06be human operator\r\n        return self.comms_interface.wait_for_confirmation()\r\n    \r\n    def continue_task(self, task_description):\r\n        # Continue \u06a9\u06d2 \u0633\u0627\u062a\u06be remaining tasks\r\n        pass\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u0627\u0639\u0644-\u06cc-\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc-\u06a9\u06cc\u0634-\u06a9\u06cc\u0634-\u06a9\u06cc\u0634-n-\u0632",children:"\u0627\u0639\u0644 \u06cc \u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 n \u0632"}),"\n",(0,s.jsx)(e.h3,{id:"dexterous-\u06c1\u06cc-ra-\u067e\u06be\u06cc-ri-\u06cc",children:"dexterous \u06c1\u06cc ra \u067e\u06be\u06cc ri \u06cc"}),"\n",(0,s.jsx)(e.p,{children:"vla \u06a9\u06d2 l \u06cc\u06d2 l \u06cc\u06d2 \u067e\u06cc\u0686\u06cc \u067e\u06cc\u0686\u06cc \u06c1\u06cc \u06c1\u06cc \u067e\u06be\u06cc \u067e\u06be\u06cc \u067e\u06be\u06cc \u067e\u06be\u06cc \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9 amwau \u06a9 Aasasamal:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class DexterousManipulationVLA:\r\n    def __init__(self):\r\n        # \u0627\u0648\u0646\u0686\u0627-precision manipulation model\r\n        self.manipulation_model = self.build_manipulation_model()\r\n        \r\n        # Hand pose estimation\r\n        self.hand_pose_estimator = HandPoseEstimator()\r\n        \r\n        # Tactile feedback integration\r\n        self.tactile_processor = TactileProcessor()\r\n    \r\n    def build_manipulation_model(self):\r\n        return nn.Sequential(\r\n            # Multi-modal transformer \u06a9\u06d2 \u0633\u0627\u062a\u06be vision \u0627\u0648\u0631 tactile inputs\r\n            nn.TransformerEncoder(\r\n                nn.TransformerEncoderLayer(d_model=768, nhead=12),\r\n                num_layers=12\r\n            ),\r\n            nn.Linear(768, 14)  # 7 joint positions + 7 joint velocities\r\n        )\r\n    \r\n    def manipulate_object(self, image, instruction, tactile_data=None):\r\n        # Process visual input\r\n        visual_features = self.extract_visual_features(image)\r\n        \r\n        # Process language instruction\r\n        lang_features = self.encode_language(instruction)\r\n        \r\n        # Process tactile input \u0627\u06af\u0631 available\r\n        \u0627\u06af\u0631 tactile_data:\r\n            tactile_features = self.tactile_processor(tactile_data)\r\n        else:\r\n            tactile_features = torch.zeros(1, 64)  # Placeholder\r\n        \r\n        # Combine \u062a\u0645\u0627\u0645 modalities\r\n        combined_features = torch.cat([\r\n            visual_features, \r\n            lang_features, \r\n            tactile_features\r\n        ], dim=1)\r\n        \r\n        # Generate manipulation \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.manipulation_model(combined_features)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n    \r\n    def extract_visual_features(self, image):\r\n        # Extract features relevant \u06a9\u06d2 \u0644\u06cc\u06d2 manipulation\r\n        # e.g., object pose, grasp points, etc.\r\n        pass\r\n    \r\n    def encode_language(self, text):\r\n        # Encode language instruction\r\n        pass\n"})}),"\n",(0,s.jsx)(e.h3,{id:"mlai-rwbo-vausr-\u0688\u06cc-n-\u06cc\u0634-n-\u06cc\u0634-in",children:"mlai-rwbo vausr \u0688\u06cc n \u06cc\u0634 n \u06cc\u0634 in"}),"\n",(0,s.jsx)(e.p,{children:"vla \u06a9\u06d2 l \u06cc\u06d2 \u06a9 \u06a9 a \u06a9 amaal ss \u06d2 \u0632\u06cc ad \u06c1 Jrobouch \u06a9 v mrboc"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class MultiRobotVLA:\r\n    def __init__(self, num_robots):\r\n        self.num_robots = num_robots\r\n        self.robot_models = nn.ModuleList([\r\n            VLAModel() \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(num_robots)\r\n        ])\r\n        \r\n        # \u0645\u0648\u0627\u0635\u0644\u0627\u062a \u0645\u0627\u0688\u06cc\u0648\u0644\r\n        self.comms_module = CommunicationModule()\r\n        \r\n        # Coordination controller\r\n        self.coordinator = CoordinationController()\r\n    \r\n    def coordinate_robots(self, global_task, robot_states):\r\n        # Decompose global task\r\n        subtasks = self.coordinator.decompose_task(global_task, robot_states)\r\n        \r\n        # Assign tasks \u06a9\u0648 robots\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = []\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba, (robot_state, subtask) \u0645\u06cc\u06ba enumerate(zip(robot_states, subtasks)):\r\n            # Get \u0631\u0648\u0628\u0648\u0679-specific instructions\r\n            robot_instruction = self.generate_robot_instruction\r\n            \r\n            # Generate \u0627\u06cc\u06a9\u0634\u0646 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0631\u0648\u0628\u0648\u0679\r\n            \u0627\u06cc\u06a9\u0634\u0646 = self.robot_models[\u0645\u06cc\u06ba](\r\n                robot_state['image'], \r\n                robot_instruction\r\n            )\r\n            \r\n            \u0627\u06cc\u06a9\u0634\u0646\u0632.append(\u0627\u06cc\u06a9\u0634\u0646)\r\n        \r\n        # Coordinate \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        coordinated_actions = self.comms_module.sync_actions\r\n        \r\n        return coordinated_actions\r\n    \r\n    def generate_robot_instruction(self, subtask, robot_id, all_states):\r\n        # Generate instructions specific \u06a9\u0648 each \u0631\u0648\u0628\u0648\u0679\r\n        # considering \u0627\u0646 \u06a9\u0627 capabilities \u0627\u0648\u0631 positions\r\n        pass\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u062a\u062a\u0634\u062e\u06cc\u0635-\u0627\u0648\u0627\u0648\u0631-\u0628\u06cc\u0628\u0646\u0686-\u0645\u0627\u0631\u06cc\u06cc\u06a9\u0646\u06af",children:"\u062a\u062a\u0634\u062e\u06cc\u0635 \u0627\u0648\u0627\u0648\u0631 \u0628\u06cc\u0628\u0646\u0686 \u0645\u0627\u0631\u06cc\u06cc\u06a9\u0646\u06af"}),"\n",(0,s.jsx)(e.h3,{id:"-1"}),"\n",(0,s.jsx)(e.p,{children:"vla maa \u0688 l \u06a9\u06cc \u06a9 arard \u06af\u06cc \u06a9 a andai:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class VLAEvaluator:\r\n    def __init__(self, vla_model):\r\n        self.model = vla_model\r\n        \r\n    def evaluate_model(self, test_dataset):\r\n        metrics = {\r\n            'success_rate': 0,\r\n            'action_accuracy': 0,\r\n            'language_alignment': 0,\r\n            'computation_time': 0,\r\n            'safety_violations': 0\r\n        }\r\n        \r\n        total_tasks = len(test_dataset)\r\n        successful_tasks = 0\r\n        total_time = 0\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 task \u0645\u06cc\u06ba test_dataset:\r\n            start_time = time.time()\r\n            \r\n            # Execute task\r\n            success = self.execute_task(task)\r\n            \r\n            \u0627\u06af\u0631 success:\r\n                successful_tasks += 1\r\n            \r\n            total_time += time.time() - start_time\r\n        \r\n        metrics['success_rate'] = successful_tasks / total_tasks\r\n        metrics['computation_time'] = total_time / total_tasks\r\n        \r\n        return metrics\r\n    \r\n    def execute_task(self, task):\r\n        # Execute \u0627\u06cc\u06a9 single task \u0627\u0648\u0631 determine \u0627\u06af\u0631 \u06cc\u06c1 \u062a\u06be\u0627 successful\r\n        # \u0646\u0641\u0627\u0630 \u06a9\u0631\u06d2 \u06af\u0627 run \u06a9\u0627/\u06a9\u06cc task \u0627\u0648\u0631 check success criteria\r\n        pass\r\n    \r\n    def benchmark_against_baseline(self, baseline_model, test_dataset):\r\n        # Compare VLA model against baseline approaches\r\n        vla_metrics = self.evaluate_model(test_dataset)\r\n        baseline_metrics = baseline_model.evaluate_model(test_dataset)\r\n        \r\n        comparison = {\r\n            'vla': vla_metrics,\r\n            'baseline': baseline_metrics,\r\n            'improvement': {}\r\n        }\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 metric \u0645\u06cc\u06ba vla_metrics:\r\n            \u0627\u06af\u0631 isinstance(vla_metrics[metric], (int, float)):\r\n                comparison['improvement'][metric] = (\r\n                    vla_metrics[metric] - baseline_metrics[metric]\r\n                )\r\n        \r\n        return comparison\n"})}),"\n",(0,s.jsx)(e.h2,{id:"\u0639\u0645\u0627\u0644\u06c1-\u0648\u0648\u0631\u06a9-\u0627\u0650\u0633",children:"\u0639\u0645\u0627\u0644\u06c1 \u0648\u0648\u0631\u06a9 \u0627\u0650\u0633"}),"\n",(0,s.jsx)(e.p,{children:"\u06c1\u0641 \u06c1\u0641 t \u06c1 's vr \u06a9 \u06a9 ss ss maus \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 wansi \u0688 vla a \u06cc\u067e li \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06a9\u06cc\u0634 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"\u062a\u0639\u0645\u0639\u0631\u0627 ac \u0627 tn\u0638\u06ccm"}),"\n",(0,s.jsx)(e.li,{children:"\u0645\u0648\u0644 \u0679\u06cc maul swsna\u06d2 \u06a9\u06cc taun\u06cc\u06a9o vunas"}),"\n",(0,s.jsx)(e.li,{children:"\u0642 \u0642 \u0642 \u0642 \u0642 \u06cc \u06cc \u0627\u0628\u0627\u0646 \u06a9 a anahr \u0641\u06cc s bna \u0626\u06cc\u06ba"}),"\n",(0,s.jsx)(e.li,{}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"\u062e-laa-\u0635\u06c1",children:"\u062e LAA \u0635\u06c1"}),"\n",(0,s.jsx)(e.p,{children:"\u06cc\u06c1 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u062c \u062c d \u06cc d tri \u06cc n a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632 \u06a9\u06cc \u06a9\u06be \u06a9\u06be \u06a9\u06be \u06a9\u06be \u06a9\u06cc\u06d4 \u06a9\u06cc\u06d4 \u0622\u067e '\u062c \u062c \u062c d \u06cc d t \u06a9 nau \u06ba \u06a9\u06d2 \u0628\u0627\u0626\u0631 \u06d2 \u0645\u0627 \u06cc\u06ba \u0633\u0648\u0633\u0627 \u06a9\u06d2 l \u06cc\u06d2 l \u06cc\u06d2 n \u0641\u06cc s vla sssausm onai j \u06a9 srna\u06d4 maa mawaval 4 \u06a9\u06d2 \u06a9\u06d2 \u0650atatam aaaur waulnluliun\u06af-"})]})}function u(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>i,x:()=>l});var a=r(6540);const s={},t=a.createContext(s);function i(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);