"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[7675],{2253:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>s,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-4/week-1-introduction/4-4-vla-practical-implementation","title":"4.4: vla lamli \u06cc n \u0641 aa \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 vr a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632","description":"\u062c a \u062c","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/week-1-introduction/4-4-vla-practical-implementation.md","sourceDirName":"module-4/week-1-introduction","slug":"/module-4/week-1-introduction/4-4-vla-practical-implementation","permalink":"/ur/docs/module-4/week-1-introduction/4-4-vla-practical-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-1-introduction/4-4-vla-practical-implementation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"4.3: vla \u0679 raunn \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a arna owr \u06a9\u06cc taur \u06cc","permalink":"/ur/docs/module-4/week-1-introduction/4-3-vla-training-data-collection"},"next":{"title":"\u0686\u0679 2: VLA \u0628\u0646 \u06cc Adatatatat","permalink":"/ur/docs/module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models"}}');var t=r(4848),i=r(8453);const s={sidebar_position:4,difficulty:"advanced"},o="4.4: vla lamli \u06cc n \u0641 aa \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 vr a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632",l={},m=[{value:"\u062c a \u062c",id:"\u062c-a-\u062c",level:2},{value:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d",id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",level:2},{value:"vla maa \u0688 l n \u0641 aa \u0641 s \u06d2 sa \u06a9 sr \u06cc\u0686",id:"vla-maa-\u0688-l-n-\u0641-aa-\u0641-s-\u06d2-sa-\u06a9-sr-\u06cc\u0686",level:2},{value:"bin \u06cc aad \u06cc vla \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r",id:"bin-\u06cc-aad-\u06cc-vla-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r",level:3},{value:"\u0679 r \u06cc nnn \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af",id:"\u0679-r-\u06cc-nnn-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af",level:2},{value:"\u0688\u06cc\u0679 a \u06a9\u06cc \u06cc araur \u06cc \u06a9\u06d2 l \u06cc\u06d2 trbaut",id:"\u0688\u06cc\u0679-a-\u06a9\u06cc-\u06cc-araur-\u06cc-\u06a9\u06d2-l-\u06cc\u06d2-trbaut",level:3},{value:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a \u06a9\u06cc \u062c \u062c \u062c \u062c \u062c \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9",id:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a-\u06a9\u06cc-\u062c-\u062c-\u062c-\u062c-\u062c-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",level:3},{value:"\u0627\u0648\u0646\u0645\u0627\u0645 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641",id:"\u0627\u0648\u0646\u0645\u0627\u0645-\u06a9\u06d2-\u06a9\u06d2-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641",level:2},{value:"ros 2 anaumamam",id:"ros-2-anaumamam",level:3},{value:"\u0622\u0626\u0632\u06a9 ssaum an \u0636 \u0645\u0645\u0627\u0645",id:"\u0622\u0626\u0632\u06a9-ssaum-an-\u0636-\u0645\u0645\u0627\u0645",level:3},{value:"vla maa \u0688 l \u06a9\u06cc \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635",id:"vla-maa-\u0688-l-\u06a9\u06cc-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635",level:2},{value:"\u0679\u0627 \u0634\u062e\u06cc\u0635 \u0645\u06cc\u0645\u0631 \u06a9 s",id:"\u0679\u0627-\u0634\u062e\u06cc\u0635-\u0645\u06cc\u0645\u0631-\u06a9-s",level:3},{value:"\u0622\u067e\u0679\u06cc maaun awrse \u06a9\u06cc \u06a9\u06cc ttttt tt tt tt tt \u06a9\u06cc",id:"\u0622\u067e\u0679\u06cc-maaun-awrse-\u06a9\u06cc-\u06a9\u06cc-ttttt-tt-tt-tt-tt-\u06a9\u06cc",level:2},{value:"maa \u0688 l \u06a9\u06cc a \u0635 laa \u062d \u06a9\u06cc taun \u06cc\u06a9",id:"maa-\u0688-l-\u06a9\u06cc-a-\u0635-laa-\u062d-\u06a9\u06cc-taun-\u06cc\u06a9",level:3},{value:"\u062d\u0642\u06cc\u0642\u06cc \u0634\u06c1 \u0634\u06c1 \u0634\u06c1 \u06a9\u06cc \u06a9\u06cc \u062a \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638",id:"\u062d\u0642\u06cc\u0642\u06cc-\u0634\u06c1-\u0634\u06c1-\u0634\u06c1-\u06a9\u06cc-\u06a9\u06cc-\u062a-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638",level:2},{value:"\u0627\u0648\u0633\u0631 \u06a9\u0648 \u0633\u0646\u0628\u0627\u0644\u0646\u06d2 \u0645\u0650\u06a9 \u0627\u0644\u06a9",id:"\u0627\u0648\u0633\u0631-\u06a9\u0648-\u0633\u0646\u0628\u0627\u0644\u0646\u06d2-\u0645\u0650\u06a9-\u0627\u0644\u06a9",level:3},{value:"\u062e LAA \u0635\u06c1",id:"\u062e-laa-\u0635\u06c1",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"44-vla-lamli-\u06cc-n-\u0641-aa-\u0630-\u0630-\u0630-\u0630-\u0630-\u0630-vr-a-\u06cc\u067e-li-\u06a9\u06cc\u0634-n-\u0632",children:"4.4: vla lamli \u06cc n \u0641 aa \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 vr a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632"})}),"\n",(0,t.jsx)(n.h2,{id:"\u062c-a-\u062c",children:"\u062c a \u062c"}),"\n",(0,t.jsx)(n.p,{children:"\u06cc\u06c1 \u0627\u06cc\u0644\u0644 \u0644\u0650\u0644\u06cc \u0644\u0650\u0644 \u0646\u0650\u0646\u0688\u0646\u0633-\u0646\u06cc\u06c1\u0627 \u06a9 \u0627\u0650\u0633\u0648\u0633 \u0648\u0650\u0644\u0646 \u0644\u0650\u0644\u0633\u0646 \u0644\u0650\u0644\u0633\u0646 \u0627\u0644\u06cc \u0644\u0627\u0644\u0633\u0646 (\u0648\u0650\u0633 \u0633\u06cc\u0633\u0644 \u0627\u0650\u0633\u06cc\u0633) \u0688 oiquliumni \u0679 \u06a9 a vla sssium\u0632 \u060c maaul \u0679 jaul \u0679 jaumamamaumaumaumaumaumaumaumamamamamaumaumaumaumamaumaumamaumaumaumaumaumaumaumaumaumaumaumaumaul \u06a9\u06d2 \u06cc \u06cc \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06af\u06d2\u06d4 \u0630 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06c1 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 \u0641 \u0641 \u0641 \u0641 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u0641 \u0641 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0630 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u0641 \u0641 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0630 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0630 \u0630 \u0630 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0630 \u0630 \u0630 \u0630 \u0630 \u06a9\u06d2 \u0641 \u0641 \u0641 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06c1 \u06c1 \u06c1 \u06a9\u06cc \u06a9\u06cc \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1"}),"\n",(0,t.jsx)(n.h2,{id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",children:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d"}),"\n",(0,t.jsx)(n.p,{children:"\u06a9\u06d2 \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u0644\u0627\u0644 \u0622\u0648 -"}),"\n",(0,t.jsx)(n.li,{children:"\u0679 r \u06cc nn \u0648 \u0628\u0644 \u06a9\u06c1"}),"\n",(0,t.jsx)(n.li,{children:"\u0639\u0646\u0627\u0636\u0645\u0627\u0645 \u0639\u06a9"}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u0637a\u0637a.nn\r\n-."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"\u0627\u0645\u062a \u06a9\u0648 \u0633\u0645\u062c\u06be\u0648"}),"\n",(0,t.jsx)(n.li,{children:"\u0627\u0650\u0633 \u0627\u0644\u0627 \u062d \u06a9\u06cc taun \u06cc\u06a9 si \u06cc\u06a9\u06be\u06cc\u06ba l \u06cc\u06d2 vla tt \u062e\u0641\u06cc\u0641"}),"\n",(0,t.jsx)(n.li,{children:"\u0688\u06cc b \u06af n \u06af \u06a9\u06cc \u062d\u06a9 \u062d\u06a9 jut amli \u06cc tahar \u06a9 ri \u06a9\u06d2 l \u06cc\u06d2 vla sssaum \u0632"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vla-maa-\u0688-l-n-\u0641-aa-\u0641-s-\u06d2-sa-\u06a9-sr-\u06cc\u0686",children:"vla maa \u0688 l n \u0641 aa \u0641 s \u06d2 sa \u06a9 sr \u06cc\u0686"}),"\n",(0,t.jsx)(n.h3,{id:"bin-\u06cc-aad-\u06cc-vla-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r",children:"bin \u06cc aad \u06cc vla \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r"}),"\n",(0,t.jsx)(n.p,{children:"\u0622\u0626\u06cc\u06d2 \u0628\u0646 \u06cc \u0627\u0634\u062a\u06c1\u0627\u0631 \u06cc vla maa \u0688 l - \u067e \u067e"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\nimport torchvision.models \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 models\r\nimport torch.nn.functional \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 F\r\n\u0633\u06d2 transformers import AutoTokenizer, AutoModel\r\nimport numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\n\r\nclass VisionEncoder:\r\n    """Vision encoder using ResNet backbone"""\r\n    def __init__(self, pretrained=True):\r\n        super().__init__()\r\n        # Use \u0627\u06cc\u06a9 pre-trained ResNet \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 vision backbone\r\n        resnet = models.resnet50(pretrained=pretrained)\r\n        \r\n        # Remove \u06a9\u0627/\u06a9\u06cc final classification layer\r\n        self.features = nn.Sequential(*list(resnet.children())[:-2])\r\n        \r\n        # Add adaptive pooling \u06a9\u0648 get fixed-size features\r\n        self.global_pool = nn.AdaptiveAvgPool2d((7, 7))\r\n        \r\n        # Projection layer \u06a9\u0648 match language encoder dimensions\r\n        self.projection = nn.Linear(2048, 768)  # ResNet outputs 2048-dim, match BERT 768-dim\r\n        \r\n    def forward(self, x):\r\n        # x shape: (batch, channels, height, width)\r\n        features = self.features(x)  # (batch, 2048, h, w)\r\n        features = self.global_pool(features)  # (batch, 2048, 7, 7)\r\n        \r\n        # Reshape \u06a9\u0648 (batch, num_patches, feature_dim)\r\n        batch_size, channels, h, w = features.shape\r\n        features = features.view(batch_size, channels, h * w).permute(0, 2, 1)  # (batch, 49, 2048)\r\n        \r\n        # Project \u06a9\u0648 language embedding dimension\r\n        projected = self.projection(features)  # (batch, 49, 768)\r\n        \r\n        return projected\r\n\r\nclass LanguageEncoder:\r\n    """Language encoder using pre-trained transformer"""\r\n    def __init__(self, model_name=\'bert-base-uncased\'):\r\n        super().__init__()\r\n        self.model_name = model_name\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        self.transformer = AutoModel.from_pretrained(model_name)\r\n        \r\n        # Freeze pre-trained weights initially\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 param \u0645\u06cc\u06ba self.transformer.parameters():\r\n            param.requires_grad = False\r\n    \r\n    def forward(self, input_ids, attention_mask):\r\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\r\n        # Use \u06a9\u0627/\u06a9\u06cc CLS token representation \u06cc\u0627 mean pooling\r\n        last_hidden_states = outputs.last_hidden_state\r\n        # Option 1: CLS token\r\n        # pooled_output = last_hidden_states[:, 0, :]  # (batch, 768)\r\n        \r\n        # Option 2: Mean pooling\r\n        pooled_output = (last_hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)  # (batch, 768)\r\n        \r\n        return pooled_output, last_hidden_states  # Return both pooled \u0627\u0648\u0631 sequence outputs\r\n\r\nclass CrossAttentionFusion:\r\n    """Cross-attention mechanism \u06a9\u0648 fuse vision \u0627\u0648\u0631 language features"""\r\n    def __init__(self, embed_dim=768, num_heads=8):\r\n        super().__init__()\r\n        self.multihead_attn = nn.MultiheadAttention(\r\n            embed_dim=embed_dim,\r\n            num_heads=num_heads,\r\n            batch_first=True\r\n        )\r\n        self.layer_norm = nn.LayerNorm(embed_dim)\r\n        self.dropout = nn.Dropout(0.1)\r\n        \r\n    def forward(self, vision_features, language_features):\r\n        # vision_features: (batch, num_patches, embed_dim)\r\n        # language_features: (batch, seq_len, embed_dim)\r\n        \r\n        # Cross-attention: vision attends \u06a9\u0648 language\r\n        attended_features, attn_weights = self.multihead_attn(\r\n            query=vision_features,  # Vision \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 query\r\n            key=language_features,  # Language \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 key\r\n            value=language_features  # Language \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 value\r\n        )\r\n        \r\n        # Residual connection \u0627\u0648\u0631 layer norm\r\n        fused_features = self.layer_norm(vision_features + self.dropout(attended_features))\r\n        \r\n        return fused_features, attn_weights\r\n\r\nclass ActionDecoder:\r\n    """\u0627\u06cc\u06a9\u0634\u0646 decoder \u06a9\u0648 generate \u0631\u0648\u0628\u0648\u0679 commands \u0633\u06d2 fused representations"""\r\n    def __init__(self, input_dim=768, action_dim=7, hidden_dim=512):\r\n        super().__init__()\r\n        self.action_dim = action_dim\r\n        \r\n        self.network = nn.Sequential(\r\n            nn.Linear(input_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_dim, action_dim),\r\n            nn.Tanh()  # \u0627\u06cc\u06a9\u0634\u0646\u0632 \u0645\u06cc\u06ba [-1, 1] range\r\n        )\r\n    \r\n    def forward(self, fused_features):\r\n        # fused_features: (batch, seq_len, embed_dim)\r\n        # Take \u06a9\u0627/\u06a9\u06cc mean across \u06a9\u0627/\u06a9\u06cc sequence dimension\r\n        global_features = fused_features.mean(dim=1)  # (batch, embed_dim)\r\n        \r\n        # Generate \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.network(global_features)  # (batch, action_dim)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n\r\nclass VLAModel:\r\n    """Complete Vision-Language-\u0627\u06cc\u06a9\u0634\u0646 Model"""\r\n    def __init__(self, language_model_name=\'bert-base-uncased\'):\r\n        super().__init__()\r\n        \r\n        # Initialize components\r\n        self.vision_encoder = VisionEncoder()\r\n        self.language_encoder = LanguageEncoder(language_model_name)\r\n        self.cross_attention_fusion = CrossAttentionFusion()\r\n        self.action_decoder = ActionDecoder()\r\n        \r\n        # Learnable query \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06cc\u06a9\u0634\u0646 generation\r\n        self.action_query = nn.\u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631(torch.randn(1, 1, 768))\r\n        \r\n    def forward(self, images, input_ids, attention_mask):\r\n        # Encode vision\r\n        vision_features = self.vision_encoder(images)  # (batch, num_patches, 768)\r\n        \r\n        # Encode language\r\n        lang_pooled, lang_sequence = self.language_encoder(input_ids, attention_mask)  # pooled: (batch, 768), sequence: (batch, seq_len, 768)\r\n        \r\n        # Expand language features \u06a9\u0648 match vision spatial dimensions\r\n        batch_size = vision_features.size(0)\r\n        expanded_lang = lang_sequence.mean(dim=1, keepdim=True).expand(-1, vision_features.size(1), -1)\r\n        \r\n        # Fuse vision \u0627\u0648\u0631 language\r\n        fused_features, attention_weights = self.cross_attention_fusion(\r\n            vision_features, expanded_lang\r\n        )  # (batch, num_patches, 768)\r\n        \r\n        # Generate \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_decoder(fused_features)  # (batch, action_dim)\r\n        \r\n        return {\r\n            \'\u0627\u06cc\u06a9\u0634\u0646\u0632\': \u0627\u06cc\u06a9\u0634\u0646\u0632,\r\n            \'fused_features\': fused_features,\r\n            \'attention_weights\': attention_weights,\r\n            \'vision_features\': vision_features,\r\n            \'language_features\': lang_pooled\r\n        }\r\n    \r\n    def freeze_language_encoder(self):\r\n        """Freeze language encoder weights"""\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 param \u0645\u06cc\u06ba self.language_encoder.parameters():\r\n            param.requires_grad = False\r\n    \r\n    def unfreeze_language_encoder(self, fine_tune_layers=None):\r\n        """Unfreeze language encoder weights \u06a9\u06d2 \u0644\u06cc\u06d2 fine-tuning"""\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 param \u0645\u06cc\u06ba self.language_encoder.parameters():\r\n            param.requires_grad = True\r\n\r\n# \u0645\u062b\u0627\u0644 training loop\r\ndef train_vla_model(model, dataloader, optimizer, criterion, device=\'cuda\'):\r\n    """Train \u06a9\u0627/\u06a9\u06cc VLA model"""\r\n    model.train()\r\n    \r\n    total_loss = 0\r\n    num_batches = 0\r\n    \r\n    \u06a9\u06d2 \u0644\u06cc\u06d2 batch_idx, batch \u0645\u06cc\u06ba enumerate(dataloader):\r\n        # Move data \u06a9\u0648 device\r\n        images = batch[\'images\'].\u06a9\u0648(device)\r\n        input_ids = batch[\'input_ids\'].\u06a9\u0648(device)\r\n        attention_mask = batch[\'attention_mask\'].\u06a9\u0648(device)\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = batch[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'].\u06a9\u0648(device)\r\n        \r\n        # Forward pass\r\n        outputs = model(images, input_ids, attention_mask)\r\n        predicted_actions = outputs[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\']\r\n        \r\n        # Compute loss\r\n        loss = criterion\r\n        \r\n        # Backward pass\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        \r\n        # Gradient clipping\r\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n        \r\n        # Update parameters\r\n        optimizer.step()\r\n        \r\n        # Accumulate statistics\r\n        total_loss += loss.item()\r\n        num_batches += 1\r\n        \r\n        \u0627\u06af\u0631 batch_idx % 100 == 0:\r\n            print(f\'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\')\r\n    \r\n    avg_loss = total_loss / num_batches\r\n    return avg_loss\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0679-r-\u06cc-nnn-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af-\u06af",children:"\u0679 r \u06cc nnn \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af \u06af"}),"\n",(0,t.jsx)(n.h3,{id:"\u0688\u06cc\u0679-a-\u06a9\u06cc-\u06cc-araur-\u06cc-\u06a9\u06d2-l-\u06cc\u06d2-trbaut",children:"\u0688\u06cc\u0679 a \u06a9\u06cc \u06cc araur \u06cc \u06a9\u06d2 l \u06cc\u06d2 trbaut"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\n\u0633\u06d2 torch.utils.data import Dataset, DataLoader\r\n\u0633\u06d2 transformers import AutoTokenizer\r\nimport torchvision.transforms \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 transforms\r\n\u0633\u06d2 PIL import Image\r\nimport json\r\n\r\nclass VLADataset(Dataset):\r\n    \"\"\"Dataset class \u06a9\u06d2 \u0644\u06cc\u06d2 VLA training data\"\"\"\r\n    def __init__(self, data_path, tokenizer_name='bert-base-uncased', \r\n                 max_length=64, image_size=224):\r\n        self.data_path = data_path\r\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\r\n        self.max_length = max_length\r\n        self.image_transform = transforms.Compose([\r\n            transforms.Resize((image_size, image_size)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n        \r\n        # Load dataset\r\n        \u06a9\u06d2 \u0633\u0627\u062a\u06be \u06a9\u06be\u0644\u0627(data_path, 'r') \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 f:\r\n            self.data = json.load(f)\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n    \r\n    def __getitem__(self, idx):\r\n        sample = self.data[idx]\r\n        \r\n        # Process image\r\n        image_path = sample['image_path']\r\n        image = Image.\u06a9\u06be\u0644\u0627(image_path).convert('RGB')\r\n        image_tensor = self.image_transform(image)\r\n        \r\n        # Process language\r\n        language_text = sample['language_instruction']\r\n        encoded_text = self.tokenizer(\r\n            language_text,\r\n            max_length=self.max_length,\r\n            padding='max_length',\r\n            truncation=True,\r\n            return_tensors='pt'\r\n        )\r\n        \r\n        # Process \u0627\u06cc\u06a9\u0634\u0646\r\n        \u0627\u06cc\u06a9\u0634\u0646 = torch.tensor\r\n        \r\n        return {\r\n            'images': image_tensor,\r\n            'input_ids': encoded_text['input_ids'].squeeze(0),\r\n            'attention_mask': encoded_text['attention_mask'].squeeze(0),\r\n            '\u0627\u06cc\u06a9\u0634\u0646\u0632': \u0627\u06cc\u06a9\u0634\u0646\r\n        }\r\n\r\ndef create_vla_trainer(model, dataset, config):\r\n    \"\"\"Create trainer \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model\"\"\"\r\n    \r\n    # Create data loader\r\n    dataloader = DataLoader(\r\n        dataset,\r\n        batch_size=config.get('batch_size', 16),\r\n        shuffle=True,\r\n        num_workers=config.get('num_workers', 4),\r\n        pin_memory=True\r\n    )\r\n    \r\n    # \u062a\u0631\u062a\u06cc\u0628 optimizer\r\n    learning_rate = config.get('learning_rate', 1e-4)\r\n    weight_decay = config.get('weight_decay', 0.01)\r\n    \r\n    optimizer = torch.optim.AdamW(\r\n        model.parameters(),\r\n        lr=learning_rate,\r\n        weight_decay=weight_decay\r\n    )\r\n    \r\n    # \u062a\u0631\u062a\u06cc\u0628 scheduler\r\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\r\n        optimizer,\r\n        T_0=config.get('scheduler_T_0', 1000),\r\n        T_mult=2\r\n    )\r\n    \r\n    # \u062a\u0631\u062a\u06cc\u0628 loss function\r\n    criterion = nn.MSELoss()  # \u06a9\u06d2 \u0644\u06cc\u06d2 continuous \u0627\u06cc\u06a9\u0634\u0646 spaces\r\n    \r\n    return {\r\n        'dataloader': dataloader,\r\n        'optimizer': optimizer,\r\n        'scheduler': scheduler,\r\n        'criterion': criterion\r\n    }\r\n\r\n# \u0645\u062b\u0627\u0644 training \u062a\u0634\u06a9\u06cc\u0644\r\nTRAINING_CONFIG = {\r\n    'batch_size': 16,\r\n    'learning_rate': 1e-4,\r\n    'weight_decay': 0.01,\r\n    'num_epochs': 50,\r\n    'device': 'cuda' \u0627\u06af\u0631 torch.cuda.is_available() else 'cpu',\r\n    'gradient_clip_value': 1.0,\r\n    'save_checkpoint_every': 5,\r\n    'validate_every': 1000\r\n}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a-\u06a9\u06cc-\u062c-\u062c-\u062c-\u062c-\u062c-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",children:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a \u06a9\u06cc \u062c \u062c \u062c \u062c \u062c \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,t.jsx)(n.h5,{id:""}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DomainRandomizationAugmenter:\r\n    """Apply domain randomization techniques \u06a9\u06d2 \u0644\u06cc\u06d2 robust VLA training"""\r\n    def __init__(self):\r\n        self.color_jitter = transforms.ColorJitter(\r\n            brightness=0.3, \r\n            contrast=0.3, \r\n            saturation=0.3, \r\n            hue=0.1\r\n        )\r\n        self.random_grayscale = transforms.RandomGrayscale(p=0.1)\r\n        self.random_rotation = transforms.RandomRotation(degrees=10)\r\n        \r\n    def randomize_domain(self, image, domain_params=None):\r\n        """\r\n        Apply domain randomization \u06a9\u0648 input image\r\n        """\r\n        # Randomize color properties\r\n        image = self.color_jitter(image)\r\n        \r\n        # Randomly apply grayscale\r\n        image = self.random_grayscale(image)\r\n        \r\n        # Add random lighting effects\r\n        image = self.add_random_lighting_effects(image)\r\n        \r\n        # Add random shadows\r\n        image = self.add_random_shadows(image)\r\n        \r\n        return image\r\n    \r\n    def add_random_lighting_effects(self, image):\r\n        """Add random lighting variations"""\r\n        # Random gamma correction\r\n        gamma = np.random.uniform(0.8, 1.2)\r\n        image = transforms.functional.adjust_gamma(image, gamma)\r\n        \r\n        # Random brightness\r\n        brightness_factor = np.random.uniform(0.8, 1.2)\r\n        image = transforms.functional.adjust_brightness(image, brightness_factor)\r\n        \r\n        return image\r\n    \r\n    def add_random_shadows(self, image):\r\n        """Add random shadows \u06a9\u0648 image"""\r\n        # \u06cc\u06c1 \u06c1\u06d2 \u0627\u06cc\u06a9 simplified version - \u0645\u06cc\u06ba practice \u0622\u067e\'d implement \u0645\u0632\u06cc\u062f sophisticated shadow generation\r\n        \u0627\u06af\u0631 np.random.rand() < 0.2:  # 20% chance \u06a9\u0648 add shadows\r\n            # Create random shadow mask\r\n            shadow_intensity = np.random.uniform(0.7, 0.9)\r\n            shadow_mask = torch.rand_like(image) * (1 - shadow_intensity) + shadow_intensity\r\n            image = image * shadow_mask\r\n        \r\n        return image\r\n\r\nclass VLADomainRandomizationTrainer:\r\n    """VLA trainer \u06a9\u06d2 \u0633\u0627\u062a\u06be domain randomization"""\r\n    def __init__(self, model, domain_augmenter):\r\n        self.model = model\r\n        self.domain_augmenter = domain_augmenter\r\n        self.real_ratio = 0.5  # 50% real data, 50% randomized data\r\n    \r\n    def train_epoch_with_domain_rand(self, train_loader, optimizer, criterion, device):\r\n        """Train \u0627\u06cc\u06a9 epoch \u06a9\u06d2 \u0633\u0627\u062a\u06be domain randomization"""\r\n        self.model.train()\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 batch_idx, batch \u0645\u06cc\u06ba enumerate(train_loader):\r\n            # Split batch between real \u0627\u0648\u0631 augmented data\r\n            batch_size = len(batch[\'images\'])\r\n            split_idx = int(batch_size * self.real_ratio)\r\n            \r\n            # Process real images\r\n            real_images = batch[\'images\'][:split_idx]\r\n            real_input_ids = batch[\'input_ids\'][:split_idx]\r\n            real_attention_mask = batch[\'attention_mask\'][:split_idx]\r\n            real_actions = batch[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'][:split_idx]\r\n            \r\n            # Process augmented images\r\n            aug_images = batch[\'images\'][split_idx:].clone()\r\n            \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba \u0645\u06cc\u06ba range(aug_images.shape[0]):\r\n                # Convert tensor \u06a9\u0648 PIL Image \u06a9\u06d2 \u0644\u06cc\u06d2 augmentation\r\n                img_tensor = aug_images[\u0645\u06cc\u06ba]\r\n                # Denormalize\r\n                denorm_img = img_tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\r\n                denorm_img = denorm_img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\r\n                denorm_img = torch.clamp(denorm_img, 0, 1)\r\n                \r\n                pil_img = transforms.ToPILImage()(denorm_img)\r\n                \r\n                # Apply domain randomization\r\n                aug_pil_img = self.domain_augmenter.randomize_domain(pil_img)\r\n                \r\n                # Convert \u067e\u06cc\u0686\u06be\u06d2 \u06a9\u0648 normalized tensor\r\n                aug_tensor = transforms.ToTensor()(aug_pil_img)\r\n                aug_tensor = (aug_tensor - torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\r\n                \r\n                aug_images[\u0645\u06cc\u06ba] = aug_tensor\r\n            \r\n            # Combine real \u0627\u0648\u0631 augmented data\r\n            all_images = torch.cat([real_images, aug_images], dim=0)\r\n            all_input_ids = torch.cat([real_input_ids, batch[\'input_ids\'][split_idx:]], dim=0)\r\n            all_attention_mask = torch.cat([real_attention_mask, batch[\'attention_mask\'][split_idx:]], dim=0)\r\n            all_actions = torch.cat\r\n            \r\n            # Forward pass\r\n            outputs = self.model(all_images, all_input_ids, all_attention_mask)\r\n            predicted_actions = outputs[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\']\r\n            \r\n            # Compute loss\r\n            loss = criterion(predicted_actions, all_actions)\r\n            \r\n            # Backward pass\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\r\n            optimizer.step()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0627\u0648\u0646\u0645\u0627\u0645-\u06a9\u06d2-\u06a9\u06d2-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641-\u0641",children:"\u0627\u0648\u0646\u0645\u0627\u0645 \u06a9\u06d2 \u06a9\u06d2 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641"}),"\n",(0,t.jsx)(n.h3,{id:"ros-2-anaumamam",children:"ros 2 anaumamam"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\n\u0633\u06d2 rclpy.\u0646\u0648\u062f import \u0646\u0648\u062f\r\n\u0633\u06d2 sensor_msgs.msg import Image, CameraInfo\r\n\u0633\u06d2 geometry_msgs.msg import Twist\r\n\u0633\u06d2 std_msgs.msg import String\r\n\u0633\u06d2 cv_bridge import CvBridge\r\nimport numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\n\u0633\u06d2 PIL import Image \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 PILImage\r\nimport torch\r\n\r\nclass VLAROS2Node:\r\n    """ROS 2 \u0646\u0648\u062f \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model integration"""\r\n    def __init__(self):\r\n        super().__init__(\'vla_ros2_node\')\r\n        \r\n        # Initialize VLA model\r\n        self.device = torch.device else \'cpu\')\r\n        self.vla_model = self.load_vla_model()\r\n        self.vla_model.\u06a9\u0648(self.device)\r\n        self.vla_model.eval()\r\n        \r\n        # Initialize CV bridge\r\n        self.bridge = CvBridge()\r\n        \r\n        # ROS 2 publishers \u0627\u0648\u0631 subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \r\n            \'/camera/color/image_raw\', \r\n            self.image_callback, \r\n            10\r\n        )\r\n        \r\n        self.language_sub = self.create_subscription(\r\n            String,\r\n            \'/\u06a9\u0645\u0627\u0646\u0688\',\r\n            self.language_callback,\r\n            10\r\n        )\r\n        \r\n        self.action_pub = self.create_publisher(\r\n            Twist,  # \u06cc\u0627 custom \u0627\u06cc\u06a9\u0634\u0646 message type\r\n            \'/cmd_vel\',\r\n            10\r\n        )\r\n        \r\n        # Internal state\r\n        self.current_image = None\r\n        self.pending_command = None\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        \r\n        # Timer \u06a9\u06d2 \u0644\u06cc\u06d2 processing loop\r\n        self.process_timer = self.create_timer(0.1, self.process_callbacks)  # 10 Hz\r\n        \r\n        self.get_logger().info\r\n    \r\n    def load_vla_model(self):\r\n        """Load pre-trained VLA model"""\r\n        model = VLAModel()\r\n        \r\n        # Load saved weights\r\n        checkpoint_path = "path/\u06a9\u0648/vla_model.pth"\r\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\r\n        model.load_state_dict(checkpoint[\'model_state_dict\'])\r\n        \r\n        return model\r\n    \r\n    def image_callback(self, msg):\r\n        """Process incoming camera images"""\r\n        try:\r\n            # Convert ROS image \u06a9\u0648 OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\r\n            \r\n            # Convert \u06a9\u0648 PIL Image\r\n            pil_image = PILImage.fromarray(cv_image)\r\n            \r\n            # Preprocess image\r\n            transform = transforms.Compose([\r\n                transforms.Resize((224, 224)),\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], \r\n                                   std=[0.229, 0.224, 0.225])\r\n            ])\r\n            \r\n            self.current_image = transform(pil_image).unsqueeze(0).\u06a9\u0648(self.device)\r\n            \r\n        except Exception \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 e:\r\n            self.get_logger().error(f\'Error processing image: {e}\')\r\n    \r\n    def language_callback(self, msg):\r\n        """Process incoming language commands"""\r\n        self.pending_command = msg.data\r\n        self.get_logger().info\r\n    \r\n    def process_callbacks(self):\r\n        """Process image \u0627\u0648\u0631 language inputs \u06a9\u0648 generate \u0627\u06cc\u06a9\u0634\u0646\u0632"""\r\n        \u0627\u06af\u0631 self.current_image \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None \u0627\u0648\u0631 self.pending_command \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None:\r\n            try:\r\n                # Tokenize language \u06a9\u0645\u0627\u0646\u0688\r\n                encoded_lang = self.tokenizer(\r\n                    self.pending_command,\r\n                    max_length=64,\r\n                    padding=\'max_length\',\r\n                    truncation=True,\r\n                    return_tensors=\'pt\'\r\n                )\r\n                \r\n                input_ids = encoded_lang[\'input_ids\'].\u06a9\u0648(self.device)\r\n                attention_mask = encoded_lang[\'attention_mask\'].\u06a9\u0648(self.device)\r\n                \r\n                # Generate \u0627\u06cc\u06a9\u0634\u0646 \u06a9\u06d2 \u0633\u0627\u062a\u06be VLA model\r\n                \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n                    model_output = self.vla_model(\r\n                        self.current_image, \r\n                        input_ids, \r\n                        attention_mask\r\n                    )\r\n                    predicted_actions = model_output[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'].cpu().numpy()[0]\r\n                \r\n                # Convert \u06a9\u0648 ROS message\r\n                cmd_msg = self.convert_action_to_cmdvel(predicted_actions)\r\n                \r\n                # Publish \u0627\u06cc\u06a9\u0634\u0646\r\n                self.action_pub.publish(cmd_msg)\r\n                \r\n                # Clear pending \u06a9\u0645\u0627\u0646\u0688\r\n                self.pending_command = None\r\n                \r\n                self.get_logger().info\r\n                \r\n            except Exception \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 e:\r\n                self.get_logger().error\r\n    \r\n    def convert_action_to_cmdvel(self, action_vector):\r\n        """Convert model output \u06a9\u0648 ROS Twist message"""\r\n        cmd_vel = Twist()\r\n        \r\n        # Map \u0627\u06cc\u06a9\u0634\u0646 vector \u06a9\u0648 Twist\r\n        # Adjust based \u067e\u0631 \u0622\u067e \u06a9\u0627 \u0631\u0648\u0628\u0648\u0679\'s \u0627\u06cc\u06a9\u0634\u0646 space\r\n        cmd_vel.linear.x = float(action_vector[0])  # Forward/backward\r\n        cmd_vel.linear.y = float(action_vector[1])  # Sideways\r\n        cmd_vel.linear.z = float(action_vector[2])  # \u0627\u0648\u067e\u0631/\u0646\u06cc\u0686\u06d2\r\n        \r\n        cmd_vel.angular.x = float(action_vector[3])  # Roll\r\n        cmd_vel.angular.y = float(action_vector[4])  # Pitch\r\n        cmd_vel.angular.z = float(action_vector[5])  # Yaw\r\n        \r\n        return cmd_vel\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vla_node = VLAROS2Node()\r\n    \r\n    try:\r\n        rclpy.spin(vla_node)\r\n    except KeyboardInterrupt:\r\n        vla_node.get_logger().info\r\n    finally:\r\n        vla_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\u0627\u06af\u0631 __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"\u0622\u0626\u0632\u06a9-ssaum-an-\u0636-\u0645\u0645\u0627\u0645",children:"\u0622\u0626\u0632\u06a9 ssaum an \u0636 \u0645\u0645\u0627\u0645"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# \u0645\u062b\u0627\u0644 \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 integration \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632\r\n\r\n\u0633\u06d2 omni.isaac.core import World\r\n\u0633\u06d2 omni.isaac.core.robots import \u0631\u0648\u0628\u0648\u0679\r\n\u0633\u06d2 omni.isaac.core.utils.nucleus import get_assets_root_path\r\n\u0633\u06d2 omni.isaac.core.utils.stage import add_reference_to_stage\r\n\u0633\u06d2 omni.isaac.core.utils.prims import get_prim_at_path\r\n\u0633\u06d2 omni.isaac.core.sensors import Camera\r\n\u0633\u06d2 omni.isaac.core import SimulationApp\r\nimport numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\nimport torch\r\nimport torchvision.transforms \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 transforms\r\n\u0633\u06d2 PIL import Image\r\n\r\nclass VLAIssacSimInterface:\r\n    """Interface between \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 \u0627\u0648\u0631 VLA model"""\r\n    def __init__(self, vla_model_path):\r\n        # Initialize \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 \u0627\u06cc\u067e\u0644\u06cc \u06a9\u06cc\u0634\u0646\r\n        self.sim_app = SimulationApp({"headless": False})\r\n        \r\n        # Initialize world\r\n        self.world = World(stage_units_in_meters=1.0)\r\n        self.world.scene.add_default_ground_plane()\r\n        \r\n        # Load VLA model\r\n        self.device = torch.device else \'cpu\')\r\n        self.vla_model = self.load_vla_model(vla_model_path)\r\n        self.vla_model.\u06a9\u0648(self.device)\r\n        self.vla_model.eval()\r\n        \r\n        # Initialize \u0631\u0648\u0628\u0648\u0679\r\n        self.\u0631\u0648\u0628\u0648\u0679 = self.setup_robot()\r\n        \r\n        # Initialize sensors\r\n        self.camera = self.setup_camera()\r\n        \r\n        # Initialize tokenizer\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        \r\n        # \u0627\u06cc\u06a9\u0634\u0646 space parameters\r\n        self.max_lin_vel = 1.0  # m/s\r\n        self.max_ang_vel = 1.0  # rad/s\r\n        \r\n    def load_vla_model(self, model_path):\r\n        """Load VLA model \u06a9\u06d2 \u0644\u06cc\u06d2 \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 integration"""\r\n        model = VLAModel()\r\n        checkpoint = torch.load(model_path, map_location=self.device)\r\n        model.load_state_dict(checkpoint[\'model_state_dict\'])\r\n        return model\r\n    \r\n    def setup_robot(self):\r\n        """\u062a\u0631\u062a\u06cc\u0628 \u0631\u0648\u0628\u0648\u0679 \u0645\u06cc\u06ba \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645"""\r\n        assets_root_path = get_assets_root_path()\r\n        \u0627\u06af\u0631 assets_root_path:\r\n            \u0631\u0648\u0628\u0648\u0679 = self.world.scene.add(\r\n                \u0631\u0648\u0628\u0648\u0679(\r\n                    prim_path="/World/\u0631\u0648\u0628\u0648\u0679",\r\n                    name="vla_robot",\r\n                    usd_path=assets_root_path + "/Isaac/Robots/TurtleBot3Burger/turtlebot3_burger.usd",\r\n                    position=[0, 0, 0.1],\r\n                    orientation=[0, 0, 0, 1]\r\n                )\r\n            )\r\n            return \u0631\u0648\u0628\u0648\u0679\r\n        else:\r\n            raise Exception\r\n    \r\n    def setup_camera(self):\r\n        """\u062a\u0631\u062a\u06cc\u0628 camera sensor \u067e\u0631 \u0631\u0648\u0628\u0648\u0679"""\r\n        camera = Camera(\r\n            prim_path="/World/\u0631\u0648\u0628\u0648\u0679/base_camera",\r\n            position=np.array([0.2, 0, 0.1]),\r\n            frequency=30,\r\n            resolution=(640, 480)\r\n        )\r\n        camera.initialize()\r\n        return camera\r\n    \r\n    def capture_observation(self):\r\n        """Capture current observation \u0633\u06d2 \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645"""\r\n        # Get RGB image \u0633\u06d2 camera\r\n        rgb_image = self.camera.get_rgb()\r\n        \r\n        # Process image \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model\r\n        transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((224, 224)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n        \r\n        # Convert numpy array \u06a9\u0648 tensor\r\n        image_tensor = transform(rgb_image)\r\n        image_tensor = image_tensor.unsqueeze(0).\u06a9\u0648(self.device)  # Add batch dimension\r\n        \r\n        # Get \u0631\u0648\u0628\u0648\u0679 state (position, orientation)\r\n        robot_pos, robot_quat = self.\u0631\u0648\u0628\u0648\u0679.get_world_pose()\r\n        robot_lin_vel, robot_ang_vel = self.\u0631\u0648\u0628\u0648\u0679.get_linear_velocity(), self.\u0631\u0648\u0628\u0648\u0679.get_angular_velocity()\r\n        \r\n        return {\r\n            \'image\': image_tensor,\r\n            \'position\': robot_pos,\r\n            \'orientation\': robot_quat,\r\n            \'linear_velocity\': robot_lin_vel,\r\n            \'angular_velocity\': robot_ang_vel\r\n        }\r\n    \r\n    def execute_command(self, command_text):\r\n        """Execute \u0627\u06cc\u06a9 natural language \u06a9\u0645\u0627\u0646\u0688 using VLA model"""\r\n        # Capture current observation\r\n        obs = self.capture_observation()\r\n        image_tensor = obs[\'image\']\r\n        \r\n        # Tokenize \u06a9\u0645\u0627\u0646\u0688\r\n        encoded_text = self.tokenizer(\r\n            command_text,\r\n            max_length=64,\r\n            padding=\'max_length\',\r\n            truncation=True,\r\n            return_tensors=\'pt\'\r\n        )\r\n        \r\n        input_ids = encoded_text[\'input_ids\'].\u06a9\u0648(self.device)\r\n        attention_mask = encoded_text[\'attention_mask\'].\u06a9\u0648(self.device)\r\n        \r\n        # Generate \u0627\u06cc\u06a9\u0634\u0646 \u06a9\u06d2 \u0633\u0627\u062a\u06be VLA model\r\n        \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n            model_output = self.vla_model(image_tensor, input_ids, attention_mask)\r\n            predicted_action = model_output[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'].cpu().numpy()[0]\r\n        \r\n        # Execute \u0627\u06cc\u06a9\u0634\u0646 \u0645\u06cc\u06ba \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645\r\n        self.execute_robot_action(predicted_action)\r\n        \r\n        return predicted_action\r\n    \r\n    def execute_robot_action(self, action_vector):\r\n        """Execute \u0627\u06cc\u06a9\u0634\u0646 vector \u067e\u0631 \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 \u0631\u0648\u0628\u0648\u0679"""\r\n        # Map \u0646\u06cc\u0648\u0631\u0644 \u0646\u06cc\u0679 \u0648\u0631\u06a9 output \u06a9\u0648 \u0631\u0648\u0628\u0648\u0679 commands\r\n        lin_vel = np.clip(action_vector[0] * self.max_lin_vel, -self.max_lin_vel, self.max_lin_vel)\r\n        ang_vel = np.clip(action_vector[5] * self.max_ang_vel, -self.max_ang_vel, self.max_ang_vel)\r\n        \r\n        # Apply \u06a9\u0645\u0627\u0646\u0688 \u06a9\u0648 \u0631\u0648\u0628\u0648\u0679\r\n        # \u06cc\u06c1 assumes \u0627\u06cc\u06a9 differential drive model\r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 TurtleBot3, \u0622\u067e \u06a9\u0631\u06d2 \u06af\u0627 publish \u06a9\u0648 /cmd_vel \u0679\u0627\u067e\u06a9 \u06cc\u0627 directly control motors\r\n        self.\u0631\u0648\u0628\u0648\u0679.apply_wheel_actions(\r\n            wheel_velocities=[lin_vel - ang_vel * 0.5, lin_vel + ang_vel * 0.5],  # \u0628\u0627\u0626\u06cc\u06ba, \u0635\u062d\u06cc\u062d wheel velocities\r\n            wheel_names=["left_wheel", "right_wheel"]\r\n        )\r\n    \r\n    def run_command_sequence(self, commands, steps_per_command=100):\r\n        """Run \u0627\u06cc\u06a9 sequence \u06a9\u0627 commands \u0645\u06cc\u06ba \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645"""\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba, \u06a9\u0645\u0627\u0646\u0688 \u0645\u06cc\u06ba enumerate(commands):\r\n            self.get_logger().info}: {\u06a9\u0645\u0627\u0646\u0688}")\r\n            \r\n            \u06a9\u06d2 \u0644\u06cc\u06d2 step \u0645\u06cc\u06ba range(steps_per_command):\r\n                # Execute \u06a9\u0645\u0627\u0646\u0688\r\n                \u0627\u06cc\u06a9\u0634\u0646 = self.execute_command\r\n                \r\n                # Step \u0633\u0645\u0648\u0644\u06cc\u0634\u0646\r\n                self.world.step(render=True)\r\n                \r\n                \u0627\u06af\u0631 step % 50 == 0:  # Log every 50 steps\r\n                    obs = self.capture_observation()\r\n                    self.get_logger().info\r\n    \r\n    def run_simulation(self):\r\n        """Run \u06a9\u0627/\u06a9\u06cc main \u0633\u0645\u0648\u0644\u06cc\u0634\u0646 loop"""\r\n        self.world.reset()\r\n        \r\n        # \u0645\u062b\u0627\u0644 \u06a9\u0645\u0627\u0646\u0688 sequence\r\n        commands = [\r\n            "Move forward",\r\n            "Turn \u0628\u0627\u0626\u06cc\u06ba",\r\n            "Stop",\r\n            "Go \u06a9\u0648 \u06a9\u0627/\u06a9\u06cc red box"\r\n        ]\r\n        \r\n        self.run_command_sequence(commands)\r\n        \r\n        # \u0628\u0646\u062f \u0633\u0645\u0648\u0644\u06cc\u0634\u0646\r\n        self.world.clear()\r\n        self.sim_app.\u0628\u0646\u062f()\r\n\r\n# \u0645\u062b\u0627\u0644 usage\r\ndef run_vla_isaac_sim_demo():\r\n    """Run VLA model \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0622\u0626\u0632\u06a9 \u0633\u06cc\u0645 demo"""\r\n    vla_interface = VLAIssacSimInterface\r\n    \r\n    try:\r\n        vla_interface.run_simulation()\r\n    except Exception \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 e:\r\n    finally:\r\n        vla_interface.sim_app.\u0628\u0646\u062f()\r\n\r\n\u0627\u06af\u0631 __name__ == "__main__":\r\n    run_vla_isaac_sim_demo()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vla-maa-\u0688-l-\u06a9\u06cc-\u0634\u062e\u06cc\u0635-\u0634\u062e\u06cc\u0635",children:"vla maa \u0688 l \u06a9\u06cc \u0634\u062e\u06cc\u0635 \u0634\u062e\u06cc\u0635"}),"\n",(0,t.jsx)(n.h3,{id:"\u0679\u0627-\u0634\u062e\u06cc\u0635-\u0645\u06cc\u0645\u0631-\u06a9-s",children:"\u0679\u0627 \u0634\u062e\u06cc\u0635 \u0645\u06cc\u0645\u0631 \u06a9 s"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 np\r\n\u0633\u06d2 sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\nimport torch\r\n\r\nclass VLAEvaluator:\r\n    """Evaluator \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model performance"""\r\n    def __init__(self, model, test_dataloader):\r\n        self.model = model\r\n        self.test_dataloader = test_dataloader\r\n        self.device = next(model.parameters()).device\r\n        \r\n    def evaluate_model(self):\r\n        """Comprehensive evaluation \u06a9\u0627 VLA model"""\r\n        self.model.eval()\r\n        \r\n        all_predictions = []\r\n        all_targets = []\r\n        all_attention_weights = []\r\n        \r\n        \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n            \u06a9\u06d2 \u0644\u06cc\u06d2 batch \u0645\u06cc\u06ba self.test_dataloader:\r\n                images = batch[\'images\'].\u06a9\u0648(self.device)\r\n                input_ids = batch[\'input_ids\'].\u06a9\u0648(self.device)\r\n                attention_mask = batch[\'attention_mask\'].\u06a9\u0648(self.device)\r\n                \u0627\u06cc\u06a9\u0634\u0646\u0632 = batch[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\'].\u06a9\u0648(self.device)\r\n                \r\n                outputs = self.model(images, input_ids, attention_mask)\r\n                predictions = outputs[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\']\r\n                \r\n                all_predictions.append(predictions.cpu().numpy())\r\n                all_targets.append.numpy())\r\n                \r\n                \u0627\u06af\u0631 \'attention_weights\' \u0645\u06cc\u06ba outputs:\r\n                    all_attention_weights.append(outputs[\'attention_weights\'].cpu().numpy())\r\n        \r\n        all_predictions = np.vstack(all_predictions)\r\n        all_targets = np.vstack(all_targets)\r\n        \r\n        # Compute metrics\r\n        metrics = {\r\n            \'mse\': self.mean_squared_error(all_predictions, all_targets),\r\n            \'mae\': self.mean_absolute_error(all_predictions, all_targets),\r\n            \'cosine_similarity\': self.cosine_similarity(all_predictions, all_targets),\r\n            \'success_rate\': self.task_success_rate(all_predictions, all_targets),\r\n            \'action_space_coverage\': self.action_space_coverage(all_predictions)\r\n        }\r\n        \r\n        return metrics, {\r\n            \'predictions\': all_predictions,\r\n            \'targets\': all_targets,\r\n            \'attention_weights\': all_attention_weights\r\n        }\r\n    \r\n    def mean_squared_error(self, predictions, targets):\r\n        """Compute MSE \u06a9\u06d2 \u0644\u06cc\u06d2 continuous \u0627\u06cc\u06a9\u0634\u0646\u0632"""\r\n        return np.mean((predictions - targets) ** 2)\r\n    \r\n    def mean_absolute_error(self, predictions, targets):\r\n        """Compute MAE \u06a9\u06d2 \u0644\u06cc\u06d2 continuous \u0627\u06cc\u06a9\u0634\u0646\u0632"""\r\n        return np.mean(np.abs(predictions - targets))\r\n    \r\n    def cosine_similarity(self, predictions, targets):\r\n        """Compute cosine similarity between prediction \u0627\u0648\u0631 target vectors"""\r\n        # Normalize vectors\r\n        pred_norm = predictions / (np.linalg.norm(predictions, axis=1, keepdims=True) + 1e-8)\r\n        targ_norm = targets / (np.linalg.norm(targets, axis=1, keepdims=True) + 1e-8)\r\n        \r\n        # Compute cosine similarity\r\n        similarities = np.sum(pred_norm * targ_norm, axis=1)\r\n        return np.mean(similarities)\r\n    \r\n    def task_success_rate(self, predictions, targets, threshold=0.1):\r\n        """Compute success rate based \u067e\u0631 task completion"""\r\n        # \u06cc\u06c1 \u06c1\u06d2 \u0627\u06cc\u06a9 simplified metric - \u0645\u06cc\u06ba practice, \u0622\u067e\'d \u0631\u06a9\u06be\u062a\u06d2 \u06c1\u06cc\u06ba \u0645\u0632\u06cc\u062f complex success criteria\r\n        distances = np.linalg.norm(predictions - targets, axis=1)\r\n        success_rate = np.mean(distances < threshold)\r\n        return success_rate\r\n    \r\n    def action_space_coverage(self, predictions):\r\n        """Measure \u06a9\u06cc\u0633\u06d2 much \u06a9\u0627 \u06a9\u0627/\u06a9\u06cc \u0627\u06cc\u06a9\u0634\u0646 space \u06c1\u06d2 utilized"""\r\n        # Compute range \u06a9\u0627 predicted \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        min_pred = np.min(predictions, axis=0)\r\n        max_pred = np.max(predictions, axis=0)\r\n        \r\n        # Assuming \u0627\u06cc\u06a9\u0634\u0646 space \u06c1\u06d2 [-1, 1] \u06a9\u06d2 \u0644\u06cc\u06d2 each dimension\r\n        action_range = 2.0  # \u0633\u06d2 -1 \u06a9\u0648 1\r\n        coverage = (max_pred - min_pred) / action_range\r\n        \r\n        return np.mean(coverage)\r\n    \r\n    def evaluate_language_understanding(self, test_prompts_and_targets):\r\n        """Evaluate \u06a9\u06cc\u0633\u06d2 well model understands language \u0645\u06cc\u06ba context \u06a9\u0627 vision"""\r\n        correct_understanding = 0\r\n        total_evaluations = 0\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 prompt, target_behavior \u0645\u06cc\u06ba test_prompts_and_targets:\r\n            # \u06a9\u06d2 \u0644\u06cc\u06d2 each prompt, test \u0627\u06af\u0631 model behaves differently based \u067e\u0631 visual context\r\n            # \u06cc\u06c1 requires defining specific behavioral tests\r\n            \r\n            # \u0645\u062b\u0627\u0644: test \u0627\u06af\u0631 "lift \u06a9\u0627/\u06a9\u06cc red cup" vs "lift \u06a9\u0627/\u06a9\u06cc blue cup" \r\n            # produces different behaviors \u06a9\u0628 both objects \u06c1\u06cc\u06ba visible\r\n            pass\r\n        \r\n        return correct_understanding / total_evaluations \u0627\u06af\u0631 total_evaluations > 0 else 0\r\n\r\n# \u0645\u062b\u0627\u0644 evaluation usage\r\ndef run_vla_evaluation(model, test_loader, checkpoint_path):\r\n    """Run evaluation \u06a9\u0627 trained VLA model"""\r\n    # Load model checkpoint\r\n    checkpoint = torch.load(checkpoint_path)\r\n    model.load_state_dict(checkpoint[\'model_state_dict\'])\r\n    \r\n    # Create evaluator\r\n    evaluator = VLAEvaluator(model, test_loader)\r\n    \r\n    # Run evaluation\r\n    metrics, detailed_results = evaluator.evaluate_model()\r\n    \r\n    # Print results\r\n    print("VLA Model Evaluation Results:")\r\n    print(f"MSE: {metrics[\'mse\']:.4f}")\r\n    print(f"MAE: {metrics[\'mae\']:.4f}")\r\n    print(f"Cosine Similarity: {metrics[\'cosine_similarity\']:.4f}")\r\n    print(f"Success Rate: {metrics[\'success_rate\']:.4f}")\r\n    \r\n    return metrics, detailed_results\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u0622\u067e\u0679\u06cc-maaun-awrse-\u06a9\u06cc-\u06a9\u06cc-ttttt-tt-tt-tt-tt-\u06a9\u06cc",children:"\u0622\u067e\u0679\u06cc maaun awrse \u06a9\u06cc \u06a9\u06cc ttttt tt tt tt tt \u06a9\u06cc"}),"\n",(0,t.jsx)(n.h3,{id:"maa-\u0688-l-\u06a9\u06cc-a-\u0635-laa-\u062d-\u06a9\u06cc-taun-\u06cc\u06a9",children:"maa \u0688 l \u06a9\u06cc a \u0635 laa \u062d \u06a9\u06cc taun \u06cc\u06a9"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\u0633\u06d2 torch.quantization import quantize_dynamic, quantize_per_tensor\r\nimport torch_tensorrt\r\n\r\nclass VLAOptimizer:\r\n    """Model optimizer \u06a9\u06d2 \u0644\u06cc\u06d2 efficient VLA inference"""\r\n    def __init__(self, model):\r\n        self.model = model\r\n        self.original_model = model\r\n    \r\n    def quantize_model(self):\r\n        """Apply quantization \u06a9\u0648 reduce model size \u0627\u0648\u0631 improve inference speed"""\r\n        # Dynamic quantization\r\n        quantized_model = quantize_dynamic(\r\n            self.model,\r\n            {nn.Linear, nn.LSTM, nn.GRU},\r\n            dtype=torch.qint8\r\n        )\r\n        \r\n        return quantized_model\r\n    \r\n    def prune_model(self, pruning_ratio=0.2):\r\n        """Apply structured pruning \u06a9\u0648 reduce model parameters"""\r\n        import torch.nn.utils.prune \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 prune\r\n        \r\n        # Create \u0627\u06cc\u06a9 copy \u06a9\u0627 \u06a9\u0627/\u06a9\u06cc model \u06a9\u0648 avoid modifying \u06a9\u0627/\u06a9\u06cc original\r\n        pruned_model = self._copy_model(self.model)\r\n        \r\n        # Apply pruning \u06a9\u0648 linear layers\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 name, \u0645\u0627\u0688\u06cc\u0648\u0644 \u0645\u06cc\u06ba pruned_model.named_modules():\r\n            \u0627\u06af\u0631 isinstance:\r\n                prune.l1_unstructured\r\n                # Make pruning permanent\r\n                prune.remove\r\n        \r\n        return pruned_model\r\n    \r\n    def jit_compile(self):\r\n        """Compile model \u06a9\u06d2 \u0633\u0627\u062a\u06be Torch JIT \u06a9\u06d2 \u0644\u06cc\u06d2 faster inference"""\r\n        # Trace \u06a9\u0627/\u06a9\u06cc model \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0645\u062b\u0627\u0644 inputs\r\n        dummy_image = torch.randn(1, 3, 224, 224)\r\n        dummy_input_ids = torch.randint(0, 1000, (1, 64))\r\n        dummy_attention_mask = torch.ones(1, 64)\r\n        \r\n        example_inputs = (dummy_image, dummy_input_ids, dummy_attention_mask)\r\n        \r\n        # Trace \u06a9\u0627/\u06a9\u06cc model\r\n        traced_model = torch.jit.trace(self.model, example_inputs)\r\n        \r\n        return traced_model\r\n    \r\n    def tensor_rt_compile(self):\r\n        """Compile model \u06a9\u06d2 \u0633\u0627\u062a\u06be TensorRT \u06a9\u06d2 \u0644\u06cc\u06d2 NVIDIA GPUs"""\r\n        # TensorRT compilation\r\n        compiled_model = torch_tensorrt.compile(\r\n            self.model,\r\n            inputs=[\r\n                torch_tensorrt.Input((1, 3, 224, 224)),\r\n                torch_tensorrt.Input((1, 64), dtype=torch.int32),\r\n                torch_tensorrt.Input((1, 64), dtype=torch.bool)\r\n            ],\r\n            enabled_precisions={torch.float, torch.half},  # Use FP32 \u0627\u0648\u0631 FP16\r\n            workspace_size=1 << 22  # 4MB workspace\r\n        )\r\n        \r\n        return compiled_model\r\n    \r\n    def optimize_for_mobile(self):\r\n        """Optimize model \u06a9\u06d2 \u0644\u06cc\u06d2 mobile/edge deployment"""\r\n        # Trace \u0627\u0648\u0631 optimize \u06a9\u06d2 \u0644\u06cc\u06d2 mobile\r\n        dummy_image = torch.randn(1, 3, 224, 224)\r\n        dummy_input_ids = torch.randint(0, 1000, (1, 64))\r\n        dummy_attention_mask = torch.ones(1, 64)\r\n        \r\n        traced_script_module = torch.jit.trace(\r\n            self.model, \r\n            (dummy_image, dummy_input_ids, dummy_attention_mask)\r\n        )\r\n        \r\n        # Optimize \u06a9\u06d2 \u0644\u06cc\u06d2 mobile\r\n        optimized_model = torch.jit.optimize_for_mobile(traced_script_module)\r\n        \r\n        return optimized_model\r\n    \r\n    def benchmark_models(self, sample_batch, num_runs=100):\r\n        """Benchmark different optimized versions \u06a9\u0627 \u06a9\u0627/\u06a9\u06cc model"""\r\n        import time\r\n        \r\n        models = {\r\n            \'original\': self.model,\r\n            \'quantized\': self.quantize_model(),\r\n            \'jit_compiled\': self.jit_compile()\r\n        }\r\n        \r\n        \u0627\u06af\u0631 torch.cuda.is_available():\r\n            models[\'tensor_rt\'] = self.tensor_rt_compile()\r\n        \r\n        results = {}\r\n        \r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 name, model \u0645\u06cc\u06ba models.items():\r\n            model.eval()\r\n            \r\n            # Warmup\r\n            \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n                \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(10):\r\n                    _ = model(\r\n                        sample_batch[\'images\'][:1],\r\n                        sample_batch[\'input_ids\'][:1],\r\n                        sample_batch[\'attention_mask\'][:1]\r\n                    )\r\n            \r\n            # Benchmark inference time\r\n            start_time = time.time()\r\n            \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n                \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(num_runs):\r\n                    _ = model(\r\n                        sample_batch[\'images\'][:1],\r\n                        sample_batch[\'input_ids\'][:1],\r\n                        sample_batch[\'attention_mask\'][:1]\r\n                    )\r\n            \r\n            end_time = time.time()\r\n            avg_time = (end_time - start_time) / num_runs\r\n            \r\n            # Calculate memory usage\r\n            \u0627\u06af\u0631 torch.cuda.is_available():\r\n                max_memory = torch.cuda.max_memory_allocated()\r\n            else:\r\n                max_memory = "N/\u0627\u06cc\u06a9"\r\n            \r\n            results[name] = {\r\n                \'avg_inference_time\': avg_time * 1000,  # Convert \u06a9\u0648 ms\r\n                \'memory_usage\': max_memory,\r\n                \'throughput\': 1 / avg_time  # samples per \u062f\u0648\u0633\u0631\u0627\r\n            }\r\n        \r\n        return results\r\n\r\n# \u0645\u062b\u0627\u0644 optimization usage\r\ndef optimize_and_deploy_vla_model(model, sample_batch):\r\n    """Complete optimization \u0627\u0648\u0631 deployment pipeline"""\r\n    optimizer = VLAOptimizer(model)\r\n    \r\n    # Run benchmarks\r\n    print("Benchmarking different model optimizations...")\r\n    benchmark_results = optimizer.benchmark_models(sample_batch)\r\n    \r\n    # Display results\r\n    \u06a9\u06d2 \u0644\u06cc\u06d2 name, metrics \u0645\u06cc\u06ba benchmark_results.items():\r\n        print(f"{name}:")\r\n        print(f"  Avg Inference Time: {metrics[\'avg_inference_time\']:.2f} ms")\r\n        print(f"  Throughput: {metrics[\'throughput\']:.2f} FPS")\r\n        print(f"  Memory Usage: {metrics[\'memory_usage\']}")\r\n        print()\r\n    \r\n    # Choose \u06a9\u0627/\u06a9\u06cc best optimization based \u067e\u0631 requirements\r\n    # \u06a9\u06d2 \u0644\u06cc\u06d2 real-time \u0631\u0648\u0628\u0648\u0679\u06a9\u0633, prioritize inference speed\r\n    optimized_model = optimizer.jit_compile()  # \u0627\u0686\u06be\u0627 balance \u06a9\u0627 speed \u0627\u0648\u0631 compatibility\r\n    \r\n    return optimized_model\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u062d\u0642\u06cc\u0642\u06cc-\u0634\u06c1-\u0634\u06c1-\u0634\u06c1-\u06a9\u06cc-\u06a9\u06cc-\u062a-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638-\u062d\u0641\u0638",children:"\u062d\u0642\u06cc\u0642\u06cc \u0634\u06c1 \u0634\u06c1 \u0634\u06c1 \u06a9\u06cc \u06a9\u06cc \u062a \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638 \u062d\u0641\u0638"}),"\n",(0,t.jsx)(n.h3,{id:"\u0627\u0648\u0633\u0631-\u06a9\u0648-\u0633\u0646\u0628\u0627\u0644\u0646\u06d2-\u0645\u0650\u06a9-\u0627\u0644\u06a9",children:"\u0627\u0648\u0633\u0631 \u06a9\u0648 \u0633\u0646\u0628\u0627\u0644\u0646\u06d2 \u0645\u0650\u06a9 \u0627\u0644\u06a9"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VLAErrorHandler:\r\n    """Error handling \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model deployment"""\r\n    def __init__(self, model, fallback_policy=None):\r\n        self.model = model\r\n        self.fallback_policy = fallback_policy \u06cc\u0627 self.default_fallback\r\n        self.error_count = 0\r\n        self.consecutive_errors = 0\r\n        self.max_consecutive_errors = 5\r\n        self.safe_action = torch.zeros(7)  # Default safe \u0627\u06cc\u06a9\u0634\u0646\r\n    \r\n    def safe_predict(self, images, input_ids, attention_mask):\r\n        """Safe prediction \u06a9\u06d2 \u0633\u0627\u062a\u06be error handling"""\r\n        try:\r\n            # Check inputs\r\n            \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba self.validate_inputs(images, input_ids, attention_mask):\r\n                return self.fallback_policy()\r\n            \r\n            # Make prediction\r\n            \u06a9\u06d2 \u0633\u0627\u062a\u06be torch.no_grad():\r\n                outputs = self.model(images, input_ids, attention_mask)\r\n                \u0627\u06cc\u06a9\u0634\u0646\u0632 = outputs[\'\u0627\u06cc\u06a9\u0634\u0646\u0632\']\r\n            \r\n            # Validate outputs\r\n            \u0627\u06af\u0631 \u0646\u06c1\u06cc\u06ba self.validate_outputs:\r\n                self.log_warning("Invalid model outputs detected")\r\n                return self.fallback_policy()\r\n            \r\n            # Reset error counters\r\n            self.consecutive_errors = 0\r\n            \r\n            return \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n            \r\n        except Exception \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 e:\r\n            self.log_error(f"VLA prediction error: {e}")\r\n            self.error_count += 1\r\n            self.consecutive_errors += 1\r\n            \r\n            # Trigger fallback \u0627\u06af\u0631 too many consecutive errors\r\n            \u0627\u06af\u0631 self.consecutive_errors >= self.max_consecutive_errors:\r\n                self.log_error("Too many consecutive errors, triggering safety protocol")\r\n                return self.emergency_stop_action()\r\n            \r\n            return self.fallback_policy()\r\n    \r\n    def validate_inputs(self, images, input_ids, attention_mask):\r\n        """Validate input data"""\r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 NaN \u06cc\u0627 inf values\r\n        \u0627\u06af\u0631 torch.isnan(images).any() \u06cc\u0627 torch.isinf(images).any():\r\n            return False\r\n        \u0627\u06af\u0631 torch.isnan(input_ids).any() \u06cc\u0627 torch.isinf(input_ids).any():\r\n            return False\r\n        \u0627\u06af\u0631 torch.isnan(attention_mask).any() \u06cc\u0627 torch.isinf(attention_mask).any():\r\n            return False\r\n        \r\n        # Check dimensions\r\n        \u0627\u06af\u0631 images.dim() != 4 \u06cc\u0627 images.shape[1:] != (3, 224, 224):\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def validate_outputs:\r\n        """Validate model outputs"""\r\n        \u0627\u06af\u0631 torch.isnan.any() \u06cc\u0627 torch.isinf.any():\r\n            return False\r\n        \r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 extremely \u0628\u0691\u0627 values \u0648\u06c1 _MAYBE_ indicate problems\r\n        \u0627\u06af\u0631 torch.abs.max() > 10.0:\r\n            return False\r\n        \r\n        return True\r\n    \r\n    def default_fallback(self):\r\n        """Default fallback \u0627\u06cc\u06a9\u0634\u0646"""\r\n        return self.safe_action.clone().unsqueeze(0)\r\n    \r\n    def emergency_stop_action(self):\r\n        """Emergency stop \u0627\u06cc\u06a9\u0634\u0646"""\r\n        stop_action = torch.zeros_like(self.safe_action)\r\n        # Add specific stop commands \u0627\u06af\u0631 needed\r\n        return stop_action.unsqueeze(0)\r\n    \r\n    def log_error(self, message):\r\n        """Log error message"""\r\n        print(f"ERROR: {message}")\r\n    \r\n    def log_warning(self, message):\r\n        """Log warning message"""\r\n        print(f"WARNING: {message}")\r\n\r\nclass VLAMonitoring:\r\n    """Runtime monitoring \u06a9\u06d2 \u0644\u06cc\u06d2 VLA model"""\r\n    def __init__(self):\r\n        self.inference_times = []\r\n        self.action_history = []\r\n        self.language_command_history = []\r\n        self.performance_threshold = 0.5  # Threshold \u06a9\u06d2 \u0644\u06cc\u06d2 performance alerts\r\n        self.anomaly_threshold = 3.0     # Standard deviations \u06a9\u06d2 \u0644\u06cc\u06d2 anomaly detection\r\n    \r\n    def monitor_inference(self, input_data, model_output, inference_time):\r\n        """Monitor model inference"""\r\n        # Record inference time\r\n        self.inference_times.append(inference_time)\r\n        \r\n        # Record outputs\r\n        self.action_history.append.numpy())\r\n        \r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 anomalies\r\n        \u0627\u06af\u0631 len(self.inference_times) > 10:\r\n            self.check_for_anomalies()\r\n    \r\n    def check_for_anomalies(self):\r\n        """Check \u06a9\u06d2 \u0644\u06cc\u06d2 performance \u06cc\u0627 behavioral anomalies"""\r\n        # Check inference time spikes\r\n        \u0627\u06af\u0631 len(self.inference_times) > 20:\r\n            recent_avg = np.mean(self.inference_times[-10:])\r\n            historical_avg = np.mean(self.inference_times[:-10])\r\n            \r\n            \u0627\u06af\u0631 recent_avg > historical_avg * 2:  # Performance degradation\r\n        \r\n        # Check \u06a9\u06d2 \u0644\u06cc\u06d2 anomalous \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06af\u0631 len(self.action_history) > 20:\r\n            recent_actions = np.array(self.action_history[-10:])\r\n            historical_actions = np.array(self.action_history[:-10])\r\n            \r\n            recent_mean = np.mean(recent_actions, axis=0)\r\n            historical_mean = np.mean(historical_actions, axis=0)\r\n            historical_std = np.std(historical_actions, axis=0)\r\n            \r\n            # Detect \u0627\u06af\u0631 recent \u0627\u06cc\u06a9\u0634\u0646\u0632 \u06c1\u06cc\u06ba far \u0633\u06d2 historical norms\r\n            z_scores = np.abs((recent_mean - historical_mean) / (historical_std + 1e-8))\r\n            \u0627\u06af\u0631 np.any(z_scores > self.anomaly_threshold):\r\n    \r\n    def get_health_report(self):\r\n        """Generate health report"""\r\n        \u0627\u06af\u0631 len(self.inference_times) == 0:\r\n            return "\u0646\u06c1\u06cc\u06ba data collected yet"\r\n        \r\n        report = {\r\n            \'avg_inference_time\': np.mean(self.inference_times),\r\n            \'std_inference_time\': np.std(self.inference_times),\r\n            \'total_inferences\': len(self.inference_times),\r\n            \'action_variance\': np.var(self.action_history) \u0627\u06af\u0631 self.action_history else 0\r\n        }\r\n        \r\n        return report\n'})}),"\n",(0,t.jsx)(n.h2,{id:"\u062e-laa-\u0635\u06c1",children:"\u062e LAA \u0635\u06c1"}),"\n",(0,t.jsx)(n.p,{children:"AML MATLAL\u06cc\u06cc \u0646\u0650\u0646\u0646 \u0627\u0644\u06cc\u06a9 \u0645\u0627\u0645\u0627\u0639\u0644 \u0646\u0650\u0646 \u062d AA \u0637\u06c1 \u06a9\u06cc A:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"2\r\n3.\r\n4.\r\n5\r\n6.\r\n7."}),"\n",(0,t.jsx)(n.p,{children:"\u06cc\u06c1 \u0627\u0639\u062f\u0645\u0644\u06cc \u06a9\u06d2 \u0646 (\u06a9/\u06a9\u06cc \u0641 \u0641 aiauni \u0688\u06cc\u0634 ni \u06a9\u06d2 lacl \u06a9 sol \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 Jaulililaus jaulililaus jamalaulaus jauli \u0622\u0626\u06cc \u0622\u0626\u06cc \u0622\u0626\u06cc \u062c\u06cc \u062c\u06cc \u062c\u06cc \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c\u06cc \u062c \u062c\u06cc \u062c \u062c \u062c \u062c \u062c\u06cc \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c \u062c\u06cc \u062c\u06cc \u062c\u06cc \u062c\u06cc \u062c \u062c\u06cc \u0627\u0648\u0627\u0648\u0631 \u06a9\u06cc \u062a \u062a \u062a \u06cc \u06cc \u06cc \u06cc \u06cc \u06c1\u06d2 \u0635 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u06a9 \u06cc\u06d2 \u06cc\u06d2 \u06a9 \u06a9 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06a9 \u06a9 \u06a9 \u06a9 \u06cc\u06d2 \u06a9 \u06a9 \u06a9 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06a9 \u06a9 \u06cc\u06d2 \u06cc\u06d2 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06cc\u06d2 \u06a9 \u06c1\u06d2 \u0636 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 ll \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06c1\u06d2 \u0636 \u06c1\u06d2 \u06c1\u06d2."}),"\n",(0,t.jsx)(n.p,{children:"\u06cc\u06c1 \u06a9/\u06a9\u06cc submodules \u06a9\u06d2 lli maausl 4 (Wauss ia \u06cc l aas maa a \u06cc) \u06a9 a iaatataam uswa\u06d4 \u06a9 a/\u06a9\u06cc wauss ala \u06d2 maa \u0624 Lai \u06a9\u06cc jumaiundi \u06a9 \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u0646 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0646 \u0646 \u0646 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06c1\u06cc\u06ba\u06d4 \u06c1\u06cc\u06ba\u06d4 \u06c1\u06cc\u06ba\u06d4 \u06c1\u06cc\u06ba\u06d4 \u06c1\u06cc\u06ba\u06d4"})]})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var a=r(6540);const t={},i=a.createContext(t);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);