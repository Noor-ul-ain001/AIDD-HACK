"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[1651],{3769:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>d,default:()=>c,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"module-4/week-1-introduction/4-2-vla-architecture-deep-learning","title":"44","description":"\u062c a \u062c","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/week-1-introduction/4-2-vla-architecture-deep-learning.md","sourceDirName":"module-4/week-1-introduction","slug":"/module-4/week-1-introduction/4-2-vla-architecture-deep-learning","permalink":"/ur/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"4.1: VLA \u0645\u0627\u0688\u0644\u0632 \u06a9\u0627 \u062c\u0627\u0626\u0632\u06c1","permalink":"/ur/docs/module-4/week-1-introduction/4-1-overview-of-vla-models"},"next":{"title":"4.3: vla \u0679 raunn \u06af \u0688\u06cc\u0679 a a \u06a9\u0679\u06be a arna owr \u06a9\u06cc taur \u06cc","permalink":"/ur/docs/module-4/week-1-introduction/4-3-vla-training-data-collection"}}');var i=r(4848),a=r(8453);const t={sidebar_position:2,difficulty:"advanced"},d="44",l={},o=[{value:"\u062c a \u062c",id:"\u062c-a-\u062c",level:2},{value:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d",id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",level:2},{value:"\u0641 aiuni \u0688\u06cc\u0634 Na: \u0646\u0639\u0648\u0631\u0644 \u0646 \u06cc\u0679 \u0648\u0648\u0631 \u06a9\u06d2 \u0627\u062c\u0632 \u06a9\u06d2",id:"\u0641-aiuni-\u0688\u06cc\u0634-na-\u0646\u0639\u0648\u0631\u0644-\u0646-\u06cc\u0679-\u0648\u0648\u0631-\u06a9\u06d2-\u0627\u062c\u0632-\u06a9\u06d2",level:2},{value:"\u0648\u0646 \u0622nauswaur\u0632",id:"\u0648\u0646-\u0622nauswaur\u0632",level:3},{value:"\u0632 \u0628\u0627\u0646 anauchr \u0632",id:"\u0632-\u0628\u0627\u0646-anauchr-\u0632",level:3},{value:"Aisn \u0688\u06cc\u06a9 vaur \u0632",id:"aisn-\u0688\u06cc\u06a9-vaur-\u0632",level:3},{value:"msslsl \u062c\u06af\u06c1\u06cc\u06ba",id:"msslsl-\u062c\u06af\u06c1\u06cc\u06ba",level:4},{value:"\u062a\u0648\u0648 \u06d2 \u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 r \u06cc\u0642\u06c1 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9",id:"\u062a\u0648\u0648-\u06d2-\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-r-\u06cc\u0642\u06c1-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",level:2},{value:"-",id:"-",level:3},{value:"mwli mwuxl thsos",id:"mwli-mwuxl-thsos",level:3},{value:"vla \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r \u067e\u06cc\u0679 rn",id:"vla-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r-\u067e\u06cc\u0679-rn",level:2},{value:"\u0645\u0648\u0648\u0644 \u06a9 \u0645\u0648\u0633\u0644 \u0641\u06cc \u0627\u0648\u0648\u0646\u0627\u0679\u0646 \u06cc\u06a9",id:"\u0645\u0648\u0648\u0644-\u06a9-\u0645\u0648\u0633\u0644-\u0641\u06cc-\u0627\u0648\u0648\u0646\u0627\u0679\u0646-\u06cc\u06a9",level:2},{value:"\u0630At\u06c1 \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc",id:"\u0630at\u06c1-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc",level:3},{value:"\u0679 RAUNN \u06af \u067e\u06cc RAA \u0688\u06cc MI \u0632",id:"\u0679-raunn-\u06af-\u067e\u06cc-raa-\u0688\u06cc-mi-\u0632",level:2},{value:"\u06a9 MA \u06a9 ssiun \u06d2 Sausna Ansan \u06cc taauraat (rlhf)",id:"\u06a9-ma-\u06a9-ssiun-\u06d2-sausna-ansan-\u06cc-taauraat-rlhf",level:3},{value:"\u0645\u0633\u062a\u0627\u0639\u062f \u0633\u06cc\u0633\u0646\u0627",id:"\u0645\u0633\u062a\u0627\u0639\u062f-\u0633\u06cc\u0633\u0646\u0627",level:3},{value:"\u06a9 Mauswaunl talat",id:"\u06a9-mauswaunl-talat",level:2},{value:"maumur \u06cc \u200b\u200b\u06a9\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9",id:"maumur-\u06cc-\u06a9\u06cc-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",level:3},{value:"JARTB \u06a9\u06cc Llimbai\u0626 \u06a9\u06cc \u062d \u062d \u062d \u062d \u062d \u062d \u062d",id:"jartb-\u06a9\u06cc-llimbai\u0626-\u06a9\u06cc-\u062d-\u062d-\u062d-\u062d-\u062d-\u062d-\u062d",level:3},{value:"maa \u0688 l \u06ccsaaln \u06af \u0627\u0648\u0633\u0646",id:"maa-\u0688-l-\u06ccsaaln-\u06af-\u0627\u0648\u0633\u0646",level:2},{value:"\u062e LAA \u0635\u06c1",id:"\u062e-laa-\u0635\u06c1",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"44",children:"44"})}),"\n",(0,i.jsx)(e.h2,{id:"\u062c-a-\u062c",children:"\u062c a \u062c"}),"\n",(0,i.jsx)(e.p,{children:"\u06cc\u06c1 \u0627\u0644\u0644 \u0644\u0650\u0644 \u0644\u0650\u0644 \u0627\u0644\u062d\u0645\u0648\u0644 \u0646\u0650\u0646 \u06a9 \u06c1 \u0627\u06cc\u0645 \u06a9 \u0627\u06cc\u06a9/\u06a9\u06cc \u0641 \u0641 aiaunaiul \u0646\u0650\u0646\u0648\u0631\u0644 \u0646\u0650\u0646\u0648\u0631\u0644 \u0646\u0650\u0646\u0646\u0646 \u0648\u0648\u0631 \u06a9\u06d2 \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u06a9\u06d2 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,i.jsx)(e.h2,{id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",children:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d"}),"\n",(0,i.jsx)(e.p,{children:"\u06a9\u06d2 \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc \u0630\u06cc"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"smauso \u06a9\u06c1/\u06a9\u06cc bin \u06cc aad \u06cc \u0688\u06cc\u067e llnnn \u06af \u06af \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 a a \u06af\u06cc a \u06c1\u06d2 a \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2"}),"\n",(0,i.jsx)(e.li,{children:"aranssaurmr \u067e \u067e \u06cc \u06cc \u06cc \u06cc \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u0641 \u06a9\u06d2 \u06a9\u06d2"}),"\n",(0,i.jsx)(e.li,{children:"\u062a\u0648\u0646\u0648 \u06a9\u06d2 \u06a9\u06d2 \u0637 ri \u06a9 ar \u06a9 v dri \u06cc aa \u0641 t icr \u06cc\u06ba\u06d4"}),"\n",(0,i.jsx)(e.li,{children:"\u06a9 a/\u06a9\u06cc taun \u06cc\u06a9\u06cc ai \u062c\u0632 a \u0621 \u06a9 v ssm \u062c\u06be\u06cc\u06ba\u06d4 \u0645\u0648\u0648\u0644\u0633 \u0645\u0648\u0648\u0633\u0644 \u0627\u0644\u0633\u0648\u0646"}),"\n",(0,i.jsx)(e.li,{children:"\u062a\u0627\u0644\u0633\u0644 \u0645\u0627\u0645\u0627 \u0627\u0644\u0646\u06af \u06a9\u06d2 \u0644\u0650\u0644 \u06cc \u0650 \u0650snasn Aa\u0650nri\u0634n \u06a9\u06d2 \u0628\u0648\u0631\u06d2"}),"\n",(0,i.jsx)(e.li,{children:"\u06a9 a/\u06a9\u06cc \u06a9 \u06a9 mausnillatahau \u06ba \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,i.jsx)(e.li,{children:"\u0639\u0646\u0627 \u0645\u06cc\u06ba \u0646\u0650\u0646 \u060c \u0627\u0648\u0633\u0648\u0648\u0631 \u060c \u0627\u0648\u0633\u0648\u0648\u0633\u0648\u0633\u0633\u0631 \u0627\u0648\u0633\u0648\u0633\u0648\u0631 \u0627\u0639\u062f\u0639\u0645\u062d\u0631 \u06a9\u06d2 \u062f\u0641\u0631\u0645\u06a9\u0648\u0646 \u0627\u062d\u0631 \u0642 \u06cc\u0633\u0648 \u0633\u0648\u0633\u0648"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"\u0641-aiuni-\u0688\u06cc\u0634-na-\u0646\u0639\u0648\u0631\u0644-\u0646-\u06cc\u0679-\u0648\u0648\u0631-\u06a9\u06d2-\u0627\u062c\u0632-\u06a9\u06d2",children:"\u0641 aiuni \u0688\u06cc\u0634 Na: \u0646\u0639\u0648\u0631\u0644 \u0646 \u06cc\u0679 \u0648\u0648\u0631 \u06a9\u06d2 \u0627\u062c\u0632 \u06a9\u06d2"}),"\n",(0,i.jsx)(e.h3,{id:"\u0648\u0646-\u0622nauswaur\u0632",children:"\u0648\u0646 \u0622nauswaur\u0632"}),"\n",(0,i.jsx)(e.p,{children:"\u0648\u0698\u0646 \u0627\u0646\u06a9\u0648\u0688\u0631\u0632 maus waus \u06cc \u06cc \u06cc \u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06cc\u06c1 \u06cc\u06c1 \u0641\u0646 \u062a\u0639\u0645\u06cc\u0631\u0627\u062a \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,i.jsx)(e.h5,{id:""}),"\n",(0,i.jsx)(e.p,{children:"\u06d4"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"** \u0627\u0648\u0633\u062f **\r\n."}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Applications"}),": Early \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632, embedded systems"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\r\nclass VisionEncoder:\r\n    def __init__(self, input_channels=3, feature_dim=512):\r\n        super().__init__()\r\n        self.conv_layers = nn.Sequential(\r\n            nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\r\n        )\r\n        self.projection = nn.Linear(256, feature_dim)\r\n    \r\n    def forward(self, x):\r\n        features = self.conv_layers(x)  # Shape: (batch, 256, 1, 1)\r\n        features = features.view(features.size(0), -1)  # Flatten\r\n        projected_features = self.projection(features)  # Shape: (batch, feature_dim)\r\n        return projected_features\n"})}),"\n",(0,i.jsx)(e.p,{children:"\u06a9\u06cca"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"** \u0622r\u06a9\u06cc\u0679\u06cc\u06a9\u0686r **: \u067e\u06cc\u0686 \u0627\u06cc\u0645\u0628\u06cc\u0688\u0646\u06af + \u0679\u0631\u0627\u0646\u0633\u0641\u0627\u0631\u0645\u0631 \u0628\u0644\u0627\u06a9\u0633\r\n.\r\n."}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Applications"}),": Modern \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632, state-\u06a9\u0627-\u06a9\u0627/\u06a9\u06cc-art performance"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\nimport torch.nn.functional \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 F\r\n\r\nclass VisionTransformer:\r\n    def __init__(self, patch_size=16, num_channels=3, embed_dim=768, depth=12, num_heads=12):\r\n        super().__init__()\r\n        self.patch_size = patch_size\r\n        self.embed_dim = embed_dim\r\n        self.num_patches = (224 // patch_size) ** 2\r\n        \r\n        # Patch embedding layer\r\n        self.patch_embed = nn.Conv2d(num_channels, embed_dim, \r\n                                     kernel_size=patch_size, stride=patch_size)\r\n        \r\n        # Positional embeddings\r\n        self.pos_embed = nn.\u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631(torch.randn(1, self.num_patches + 1, embed_dim))\r\n        self.cls_token = nn.\u067e\u06cc\u0631\u0627\u0645\u06cc\u0679\u0631(torch.randn(1, 1, embed_dim))\r\n        \r\n        # Transformer blocks\r\n        self.blocks = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(depth)\r\n        ])\r\n        \r\n        self.norm = nn.LayerNorm(embed_dim)\r\n    \r\n    def forward(self, x):\r\n        B, C, H, W = x.shape\r\n        \r\n        # Convert image \u06a9\u0648 patches\r\n        x = self.patch_embed(x)  # (B, embed_dim, num_patches_h, num_patches_w)\r\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\r\n        \r\n        # Add class token\r\n        cls_tokens = self.cls_token.expand(B, -1, -1)\r\n        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)\r\n        \r\n        # Add positional embeddings\r\n        x = x + self.pos_embed[:, :x.size(1)]\r\n        \r\n        # Apply transformer blocks\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 block \u0645\u06cc\u06ba self.blocks:\r\n            x = block(x)\r\n        \r\n        x = self.norm(x)\r\n        return x[:, 0]  # Return class token embedding\r\n\r\nclass TransformerBlock:\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\r\n        self.norm1 = nn.LayerNorm(embed_dim)\r\n        self.norm2 = nn.LayerNorm(embed_dim)\r\n        self.mlp = nn.Sequential(\r\n            nn.Linear(embed_dim, embed_dim * 4),\r\n            nn.GELU(),\r\n            nn.Linear(embed_dim * 4, embed_dim)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        # Self-attention\r\n        attn_out, _ = self.attention(x, x, x)\r\n        x = x + attn_out\r\n        x = self.norm1(x)\r\n        \r\n        # Feed-forward\r\n        mlp_out = self.mlp(x)\r\n        x = x + mlp_out\r\n        x = self.norm2(x)\r\n        \r\n        return x\n"})}),"\n",(0,i.jsx)(e.h5,{id:"-1"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"ConvNeXt"}),": Convolutional layers \u06a9\u06d2 \u0633\u0627\u062a\u06be Transformer-style normalization"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Swin Transformer"}),": Shifted windows \u06a9\u06d2 \u0644\u06cc\u06d2 local-global attention\r\n."]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"\u0632-\u0628\u0627\u0646-anauchr-\u0632",children:"\u0632 \u0628\u0627\u0646 anauchr \u0632"}),"\n",(0,i.jsx)(e.p,{children:"\u0632 \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u0646\u06cc\u0686\u06d2 \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e"}),"\n",(0,i.jsx)(e.h5,{id:"-2"}),"\n",(0,i.jsx)(e.p,{children:"."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GPT style"}),": Causal generation \u06a9\u0627 text \u0627\u0648\u0631 \u0627\u06cc\u06a9\u0634\u0646\u0632"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"T5 style"}),": Encoder-decoder \u06a9\u06d2 \u0644\u06cc\u06d2 text-\u06a9\u0648-text tasks"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\r\nclass LanguageEncoder:\r\n    def __init__(self, vocab_size=50257, embed_dim=768, max_seq_len=512, num_layers=12, num_heads=12):\r\n        super().__init__()\r\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\r\n        self.pos_embedding = nn.Embedding(max_seq_len, embed_dim)\r\n        \r\n        self.blocks = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(num_layers)\r\n        ])\r\n        \r\n        self.ln_f = nn.LayerNorm(embed_dim)  # Final layer norm\r\n    \r\n    def forward(self, input_ids, attention_mask=None):\r\n        # Embed tokens\r\n        token_emb = self.token_embedding(input_ids)  # (B, seq_len, embed_dim)\r\n        \r\n        # Add positional embeddings\r\n        seq_len = input_ids.size(1)\r\n        pos_ids = torch.arange\r\n        pos_emb = self.pos_embedding(pos_ids)  # (seq_len, embed_dim)\r\n        pos_emb = pos_emb.unsqueeze(0)  # (1, seq_len, embed_dim)\r\n        \r\n        x = token_emb + pos_emb\r\n        \r\n        # Apply transformer blocks\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 block \u0645\u06cc\u06ba self.blocks:\r\n            x = block(x)\r\n        \r\n        x = self.ln_f(x)\r\n        return x  # (B, seq_len, embed_dim)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"aisn-\u0688\u06cc\u06a9-vaur-\u0632",children:"Aisn \u0688\u06cc\u06a9 vaur \u0632"}),"\n",(0,i.jsx)(e.p,{children:"A\u0650n J \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc j \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc j \u06a9\u06cc"}),"\n",(0,i.jsx)(e.h4,{id:"msslsl-\u062c\u06af\u06c1\u06cc\u06ba",children:"msslsl \u062c\u06af\u06c1\u06cc\u06ba"}),"\n",(0,i.jsx)(e.p,{children:"."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Operational Space"}),": \u0627\u062e\u062a\u062a\u0627\u0645-effector positions, rotations"]}),"\n"]}),"\n",(0,i.jsx)(e.h5,{id:"-3"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Primitive \u0627\u06cc\u06a9\u0634\u0646\u0632"}),": Pick, place, \u06a9\u06be\u0644\u0627, \u0628\u0646\u062f"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Symbolic \u0627\u06cc\u06a9\u0634\u0646\u0632"}),": \u0627\u0648\u0646\u0686\u0627-level commands"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\r\nclass ActionDecoder:\r\n    def __init__(self, latent_dim=512, action_dim=7, hidden_dim=256):\r\n        super().__init__()\r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(latent_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim),\r\n            nn.Tanh()  # Normalize \u06a9\u0648 [-1, 1] \u06a9\u06d2 \u0644\u06cc\u06d2 continuous control\r\n        )\r\n        \r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 different \u0627\u06cc\u06a9\u0634\u0646 types, we _MAYBE_ also \u0631\u06a9\u06be\u062a\u06d2 \u06c1\u06cc\u06ba:\r\n        # - Separate heads \u06a9\u06d2 \u0644\u06cc\u06d2 position \u0627\u0648\u0631 rotation\r\n        # - Variational layers \u06a9\u06d2 \u0644\u06cc\u06d2 uncertainty estimation\r\n        # - Temporal prediction \u06a9\u06d2 \u0644\u06cc\u06d2 trajectory planning\r\n    \r\n    def forward(self, latent_state):\r\n        # Latent state \u0633\u06d2 multimodal fusion\r\n        \u0627\u06cc\u06a9\u0634\u0646 = self.decoder(latent_state)\r\n        return \u0627\u06cc\u06a9\u0634\u0646\n"})}),"\n",(0,i.jsx)(e.h2,{id:"\u062a\u0648\u0648-\u06d2-\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-r-\u06cc\u0642\u06c1-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",children:"\u062a\u0648\u0648 \u06d2 \u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 r \u06cc\u0642\u06c1 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,i.jsx)(e.h3,{id:"-4"}),"\n",(0,i.jsx)(e.p,{children:"\u062e ud ssaaut \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06a9 j \u06c1\u06d2 \u0642 \u0642 \u0642 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\r\nclass MultiHeadSelfAttention:\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        assert embed_dim % num_heads == 0\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = embed_dim // num_heads\r\n        \r\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\r\n        self.proj = nn.Linear(embed_dim, embed_dim)\r\n        \r\n    def forward(self, x):\r\n        B, N, C = x.shape\r\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\r\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\r\n        q, k, v = qkv[0], qkv[1], qkv[2]\r\n        \r\n        # Compute attention weights\r\n        attn_weights = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\r\n        attn_weights = F.softmax(attn_weights, dim=-1)\r\n        \r\n        # Apply attention \u06a9\u0648 values\r\n        output = attn_weights @ v\r\n        output = output.transpose(1, 2).reshape(B, N, C)\r\n        \r\n        return self.proj(output)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"-",children:"-"}),"\n",(0,i.jsx)(e.p,{children:"iSras \u06d2 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06cc\u06a9 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06be \u06be \u06be \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06be \u06be \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class CrossAttention:\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = embed_dim // num_heads\r\n        \r\n        # Separate projections \u06a9\u06d2 \u0644\u06cc\u06d2 query (e.g., language) \u0627\u0648\u0631 key/value (e.g., vision)\r\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\r\n        self.kv_proj = nn.Linear(embed_dim, embed_dim * 2)\r\n        self.output_proj = nn.Linear(embed_dim, embed_dim)\r\n    \r\n    def forward(self, query, key_value):\r\n        B, N, C = query.shape\r\n        _, M, _ = key_value.shape\r\n        \r\n        # Project query, key, \u0627\u0648\u0631 value\r\n        q = self.query_proj(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\r\n        kv = self.kv_proj(key_value).reshape(B, M, 2, self.num_heads, self.head_dim).transpose(1, 2)\r\n        k, v = kv[:, :, 0], kv[:, :, 1]\r\n        \r\n        # Compute cross-attention\r\n        attn_weights = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\r\n        attn_weights = F.softmax(attn_weights, dim=-1)\r\n        \r\n        output = attn_weights @ v\r\n        output = output.transpose(1, 2).reshape(B, N, C)\r\n        \r\n        return self.output_proj(output)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"mwli-mwuxl-thsos",children:"mwli mwuxl thsos"}),"\n",(0,i.jsx)(e.p,{children:"\u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u060c \u060c \u060c \u060c \u060c \u0637 \u0637 \u0637 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 ari \u06a9 ari \u0637 ari \u0637 rauchos ssiumaulumaut \u0637 rauchus ssiuchus ssiulumaut \u0637 ruuchuchuchus \u0637 \u0637 \u0637 \u0637 \u06a9 \u0631\u0627\u0624\u0686\u0633 \u062c\u0648\u0644\u0648\u0645\u0627\u0624\u0679 \u06a9 \u0631\u0627\u0624\u0686 \u0637 \u0627\u06cc\u0631\u06cc \u0637 \u0631\u0627\u0624\u0686\u0648\u0633 \u0637 \u0627\u06d2 \u0622\u0631 \u0637"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class MultimodalAttention:\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.vision_to_lang = CrossAttention(embed_dim, num_heads)\r\n        self.lang_to_vision = CrossAttention(embed_dim, num_heads)\r\n        self.action_to_multimodal = CrossAttention(embed_dim, num_heads)\r\n        \r\n    def forward(self, vision_features, language_features, action_features=None):\r\n        # Cross-attend vision \u0627\u0648\u0631 language\r\n        lang_with_vision = self.vision_to_lang(language_features, vision_features)\r\n        vision_with_lang = self.lang_to_vision(vision_features, language_features)\r\n        \r\n        # \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06cc\u06a9\u0634\u0646 prediction, attend \u06a9\u0648 multimodal context\r\n        \u0627\u06af\u0631 action_features \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None:\r\n            action_with_context = self.action_to_multimodal(action_features, \r\n                torch.cat([lang_with_vision, vision_with_lang], dim=1))\r\n            return vision_with_lang, lang_with_vision, action_with_context\r\n        else:\r\n            return vision_with_lang, lang_with_vision\n"})}),"\n",(0,i.jsx)(e.h2,{id:"vla-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r-\u067e\u06cc\u0679-rn",children:"vla \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r \u067e\u06cc\u0679 rn"}),"\n",(0,i.jsx)(e.h3,{id:"-5"}),"\n",(0,i.jsx)(e.p,{children:"\u0645\u0627\u0627\u0633\u0633\u0631\u0646"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class UnifiedVLATransformer:\r\n    def __init__(self, \r\n                 vocab_size=50257,\r\n                 vision_patch_size=16,\r\n                 embed_dim=768,\r\n                 depth=12,\r\n                 num_heads=12,\r\n                 action_dim=7):\r\n        super().__init__()\r\n        \r\n        # Modal-specific encoders\r\n        self.vision_encoder = VisionTransformer(\r\n            patch_size=vision_patch_size,\r\n            embed_dim=embed_dim,\r\n            depth=depth//2,  # Shallower vision encoder\r\n            num_heads=num_heads\r\n        )\r\n        \r\n        self.language_encoder = LanguageEncoder(\r\n            vocab_size=vocab_size,\r\n            embed_dim=embed_dim,\r\n            num_layers=depth//2,\r\n            num_heads=num_heads\r\n        )\r\n        \r\n        # Cross-modal transformer layers\r\n        self.cross_modal_layers = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(depth)\r\n        ])\r\n        \r\n        # \u0627\u06cc\u06a9\u0634\u0646 prediction head\r\n        self.action_head = nn.Sequential(\r\n            nn.LayerNorm(embed_dim),\r\n            nn.Linear(embed_dim, embed_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(embed_dim // 2, action_dim)\r\n        )\r\n        \r\n        # Task identification head\r\n        self.task_head = nn.Linear(embed_dim, 100)  # 100 possible tasks\r\n    \r\n    def forward(self, images, text_tokens, attention_mask=None):\r\n        # Encode modalities separately\r\n        vision_features = self.vision_encoder(images)  # (B, vision_seq_len, embed_dim)\r\n        lang_features = self.language_encoder(text_tokens, attention_mask)  # (B, text_seq_len, embed_dim)\r\n        \r\n        # Concatenate modalities\r\n        multimodal_input = torch.cat([vision_features, lang_features], dim=1)  # (B, combined_seq_len, embed_dim)\r\n        \r\n        # Process \u06a9\u06d2 \u0633\u0627\u062a\u06be cross-modal layers\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 layer \u0645\u06cc\u06ba self.cross_modal_layers:\r\n            multimodal_input = layer(multimodal_input)\r\n        \r\n        # Extract representations \u06a9\u06d2 \u0644\u06cc\u06d2 different heads\r\n        # Use [CLS] token \u06cc\u0627 mean pooling \u06a9\u06d2 \u0644\u06cc\u06d2 global representation\r\n        pooled_features = multimodal_input.mean(dim=1)  # (B, embed_dim)\r\n        \r\n        # Generate predictions\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(pooled_features)\r\n        task_pred = self.task_head(pooled_features)\r\n        \r\n        return {\r\n            '\u0627\u06cc\u06a9\u0634\u0646\u0632': \u0627\u06cc\u06a9\u0634\u0646\u0632,\r\n            'task': task_pred,\r\n            'multimodal_features': multimodal_input\r\n        }\n"})}),"\n",(0,i.jsx)(e.h3,{id:"-6"}),"\n",(0,i.jsx)(e.p,{children:"\u0631\u06a9\u0648\u0639 \u0648\u0648\u0628\u0644\u06a9\u06c1"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class VLAEcoderDecoder:\r\n    def __init__(self, embed_dim=768, num_layers=12, num_heads=12, action_vocab_size=200):\r\n        super().__init__()\r\n        \r\n        # Encoder \u06a9\u06d2 \u0644\u06cc\u06d2 vision-language input\r\n        self.encoder = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(num_layers // 2)\r\n        ])\r\n        \r\n        # Decoder \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06cc\u06a9\u0634\u0646 generation\r\n        self.decoder = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(num_layers // 2)\r\n        ])\r\n        \r\n        # \u0627\u06cc\u06a9\u0634\u0646 vocabulary projection\r\n        self.action_projection = nn.Linear(embed_dim, action_vocab_size)\r\n        \r\n        # Cross-attention layer \u06a9\u06d2 \u0644\u06cc\u06d2 encoder-decoder \u0645\u0648\u0627\u0635\u0644\u0627\u062a\r\n        self.enc_dec_attention = MultiHeadSelfAttention(embed_dim, num_heads)\r\n    \r\n    def forward(self, vision_lang_features, action_tokens=None):\r\n        # Encode vision-language input\r\n        encoded_features = vision_lang_features\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 layer \u0645\u06cc\u06ba self.encoder:\r\n            encoded_features = layer(encoded_features)\r\n        \r\n        # Decode \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06af\u0631 action_tokens \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None:\r\n            # Teacher forcing \u06a9\u06d2 \u062f\u0648\u0631\u0627\u0646 training\r\n            decoded_features = self.process_decoder_tokens(action_tokens)\r\n        else:\r\n            # Autoregressive generation \u06a9\u06d2 \u062f\u0648\u0631\u0627\u0646 inference\r\n            decoded_features = self.autoregressive_decode(encoded_features)\r\n        \r\n        # Apply cross-attention between encoder \u0627\u0648\u0631 decoder\r\n        attended_features = self.enc_dec_attention(decoded_features, encoded_features)\r\n        \r\n        # Project \u06a9\u0648 \u0627\u06cc\u06a9\u0634\u0646 space\r\n        action_logits = self.action_projection(attended_features)\r\n        \r\n        return action_logits\r\n    \r\n    def process_decoder_tokens(self, action_tokens):\r\n        # Similar \u06a9\u0648 language model processing\r\n        # Embed \u0627\u06cc\u06a9\u0634\u0646 tokens \u0627\u0648\u0631 apply decoder layers\r\n        pass\r\n    \r\n    def autoregressive_decode(self, encoded_features):\r\n        # Autoregressive decoding similar \u06a9\u0648 GPT\r\n        # Generate \u0627\u06cc\u06a9 \u0627\u06cc\u06a9\u0634\u0646 token \u067e\u0631 \u0627\u06cc\u06a9 time\r\n        pass\n"})}),"\n",(0,i.jsx)(e.h2,{id:"\u0645\u0648\u0648\u0644-\u06a9-\u0645\u0648\u0633\u0644-\u0641\u06cc-\u0627\u0648\u0648\u0646\u0627\u0679\u0646-\u06cc\u06a9",children:"\u0645\u0648\u0648\u0644 \u06a9 \u0645\u0648\u0633\u0644 \u0641\u06cc \u0627\u0648\u0648\u0646\u0627\u0679\u0646 \u06cc\u06a9"}),"\n",(0,i.jsx)(e.h3,{id:"\u0630at\u06c1-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc-\u0626\u06cc",children:"\u0630At\u06c1 \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc \u0626\u06cc"}),"\n",(0,i.jsx)(e.p,{children:"\u0637 rauch -\u06a9 v \u06cc\u06a9\u062c a \u06a9 ri \u06cc\u06ba - an \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679 \u067e\u0679."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class EarlyFusionVLA:\r\n    def __init__(self, vision_dim=2048, lang_dim=512, action_dim=7, hidden_dim=1024):\r\n        super().__init__()\r\n        # Project modalities \u06a9\u0648 common space\r\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\r\n        self.lang_proj = nn.Linear(lang_dim, hidden_dim)\r\n        \r\n        # Combined processing\r\n        self.fusion_network = nn.Sequential(\r\n            nn.Linear(hidden_dim * 2, hidden_dim * 2),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_dim * 2, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        self.action_head = nn.Linear(hidden_dim, action_dim)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Project \u06a9\u0648 common space\r\n        vis_proj = self.vision_proj(vision_features)\r\n        lang_proj = self.lang_proj(language_features)\r\n        \r\n        # Concatenate \u0627\u0648\u0631 fuse\r\n        fused = torch.cat([vis_proj, lang_proj], dim=-1)\r\n        fused = self.fusion_network(fused)\r\n        \r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(fused)\r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n"})}),"\n",(0,i.jsx)(e.h3,{id:"-7"}),"\n",(0,i.jsx)(e.p,{children:"\u0639\u0645\u0644 \u06a9\u06d2 \u0637 rauch \u06a9 so \u0622\u0632 Adan \u06c1 \u0637 \u0637 vr \u067e r \u060c \u067e\u06be \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class LateFusionVLA:\r\n    def __init__(self, vision_dim=2048, lang_dim=512, action_dim=7):\r\n        super().__init__()\r\n        # Independent processing branches\r\n        self.vision_branch = nn.Sequential(\r\n            nn.Linear(vision_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256)\r\n        )\r\n        \r\n        self.language_branch = nn.Sequential(\r\n            nn.Linear(lang_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256)\r\n        )\r\n        \r\n        # Fusion layer\r\n        self.fusion = nn.Linear(256 * 2, 512)\r\n        self.action_head = nn.Linear(512, action_dim)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Process independently\r\n        vis_out = self.vision_branch(vision_features)\r\n        lang_out = self.language_branch(language_features)\r\n        \r\n        # Combine late\r\n        combined = torch.cat([vis_out, lang_out], dim=-1)\r\n        fused = self.fusion(combined)\r\n        \r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(fused)\r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n"})}),"\n",(0,i.jsx)(e.h3,{id:"-8"}),"\n",(0,i.jsx)(e.p,{children:"\u06cc\u06a9 ss \u06d2 \u0632\u06cc \u0632\u06cc \u06c1 \u06c1 \u0641\u06cc waua \u0626 n \u0679 s - mautli ssucho \u06ba \u06a9 \u062e \u062e \u062e lai:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class HierarchicalFusionVLA:\r\n    def __init__(self, embed_dim=768):\r\n        super().__init__()\r\n        # Initial modality processing\r\n        self.vision_init = nn.Linear(2048, embed_dim)\r\n        self.lang_init = nn.Linear(512, embed_dim)\r\n        \r\n        # \u06a9\u0645-level fusion\r\n        self.low_fusion = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\r\n        \r\n        # Mid-level processing layers\r\n        self.mid_layers = nn.ModuleList([\r\n            TransformerBlock(embed_dim, 8) \u06a9\u06d2 \u0644\u06cc\u06d2 _ \u0645\u06cc\u06ba range(4)\r\n        ])\r\n        \r\n        # \u0627\u0648\u0646\u0686\u0627-level fusion\r\n        self.high_fusion = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\r\n        \r\n        # Final \u0627\u06cc\u06a9\u0634\u0646 prediction\r\n        self.action_pred = nn.Linear(embed_dim, 7)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Initial processing\r\n        vis_processed = self.vision_init(vision_features).unsqueeze(1)  # Add sequence dimension\r\n        lang_processed = self.lang_init(language_features).unsqueeze(1)\r\n        \r\n        # \u06a9\u0645-level fusion\r\n        fused_low, _ = self.low_fusion(vis_processed, lang_processed, lang_processed)\r\n        \r\n        # Mid-level processing\r\n        mid_features = fused_low\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 layer \u0645\u06cc\u06ba self.mid_layers:\r\n            mid_features = layer(mid_features)\r\n        \r\n        # \u0627\u0648\u0646\u0686\u0627-level fusion \u06a9\u06d2 \u0633\u0627\u062a\u06be context\r\n        final_features, _ = self.high_fusion(mid_features, fused_low, fused_low)\r\n        \r\n        # Predict \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_pred(final_features.squeeze(1))\r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n"})}),"\n",(0,i.jsx)(e.h2,{id:"\u0679-raunn-\u06af-\u067e\u06cc-raa-\u0688\u06cc-mi-\u0632",children:"\u0679 RAUNN \u06af \u067e\u06cc RAA \u0688\u06cc MI \u0632"}),"\n",(0,i.jsx)(e.h3,{id:"-9"}),"\n",(0,i.jsx)(e.p,{children:"s \u06cc\u06a9\u06be\u06cc\u06ba - insan \u06cc mauaa \u06c1 roi \u06ba \u06a9\u06cc taul \u06cc d \u06a9 r \u06cc\u06ba:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def behavioral_cloning_loss(model, batch):\r\n    \"\"\"\r\n    Standard behavioral cloning objective\r\n    \"\"\"\r\n    obs_images = batch['images']  # (B, C, H, W)\r\n    obs_language = batch['language']  # (B, seq_len)\r\n    \u0627\u06cc\u06a9\u0634\u0646\u0632 = batch['expert_actions']  # (B, action_dim)\r\n    \r\n    predicted_actions = model(obs_images, obs_language)['\u0627\u06cc\u06a9\u0634\u0646\u0632']\r\n    \r\n    # Mean squared error \u06a9\u06d2 \u0644\u06cc\u06d2 continuous \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n    bc_loss = F.mse_loss\r\n    \r\n    return bc_loss\n"})}),"\n",(0,i.jsx)(e.h3,{id:"\u06a9-ma-\u06a9-ssiun-\u06d2-sausna-ansan-\u06cc-taauraat-rlhf",children:"\u06a9 MA \u06a9 ssiun \u06d2 Sausna Ansan \u06cc taauraat (rlhf)"}),"\n",(0,i.jsx)(e.p,{children:"\u067e Aal \u06cc Sauchu \u06ba \u06a9 v Buatr Bunan \u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0626\u06d2 \u0626\u06d2 \u0626\u06d2 Sasana \u06cc taaaaaurat \u06a9 a \u06a9 a \u06a9 aasamal asri:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def rlhf_loss(model, batch, reference_model=None):\r\n    \"\"\"\r\n    Reinforcement Learning \u0633\u06d2 Human Feedback\r\n    \"\"\"\r\n    states = batch['states']\r\n    \u0627\u06cc\u06a9\u0634\u0646\u0632 = batch['\u0627\u06cc\u06a9\u0634\u0646\u0632']\r\n    rewards = batch['rewards']  # Human preference scores\r\n    \r\n    # Get log probabilities \u0633\u06d2 current model\r\n    curr_log_probs = model.get_log_prob\r\n    \r\n    # Get log probabilities \u0633\u06d2 reference model (initial policy)\r\n    \u0627\u06af\u0631 reference_model:\r\n        ref_log_probs = reference_model.get_log_prob\r\n        ratio = torch.exp(curr_log_probs - ref_log_probs)\r\n    else:\r\n        ratio = 1.0\r\n    \r\n    # Policy gradient loss\r\n    pg_loss = -(ratio * rewards).mean()\r\n    \r\n    return pg_loss\n"})}),"\n",(0,i.jsx)(e.h3,{id:"\u0645\u0633\u062a\u0627\u0639\u062f-\u0633\u06cc\u0633\u0646\u0627",children:"\u0645\u0633\u062a\u0627\u0639\u062f \u0633\u06cc\u0633\u0646\u0627"}),"\n",(0,i.jsx)(e.p,{children:"nmaaund \u06af\u06cc si \u06cc\u06a9\u06be\u06cc\u06ba\u06d4"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def contrastive_loss(vision_features, language_features, temperature=0.1):\r\n    """\r\n    Contrastive loss \u06a9\u0648 align vision \u0627\u0648\u0631 language representations\r\n    """\r\n    # Compute similarity matrix\r\n    sim_matrix = torch.matmul(vision_features, language_features.T) / temperature\r\n    \r\n    # Targets: diagonal elements \u0686\u0627\u06c1\u06cc\u06d2 \u06c1\u0648\u0646\u0627 \u0627\u0648\u0646\u0686\u0627\r\n    labels = torch.arange(len(vision_features)).\u06a9\u0648(vision_features.device)\r\n    \r\n    # Cross entropy loss \u06a9\u06c1\u0627\u06ba each image \u0686\u0627\u06c1\u06cc\u06d2 match \u0627\u0633 \u06a9\u0627 corresponding text\r\n    loss_img = F.cross_entropy(sim_matrix, labels)\r\n    loss_txt = F.cross_entropy(sim_matrix.T, labels)\r\n    \r\n    return (loss_img + loss_txt) / 2\n'})}),"\n",(0,i.jsx)(e.h2,{id:"\u06a9-mauswaunl-talat",children:"\u06a9 Mauswaunl talat"}),"\n",(0,i.jsx)(e.h3,{id:"maumur-\u06cc-\u06a9\u06cc-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",children:"maumur \u06cc \u200b\u200b\u06a9\u06cc \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,i.jsx)(e.p,{children:"\u0648\u0650\u0644 \u0627\u0650\u0633\u0633\u0644 \u0627\u0650\u0633 \u0645\u0627\u0645\u0627\u0645\u0627\u0644\u0632 \u06a9 \u06a9 \u200b\u200bai \u0641\u06cc \u062d \u062d \u06a9 \u06a9 \u06a9 mauchnil wssaaul \u06a9\u06cc \u0636 crorat \u06c1\u06d2:"}),"\n",(0,i.jsx)(e.p,{children:"."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"** \u0645\u0627\u0624\u0644 \u0645\u0627\u0645\u0648\u0631\u0648\u0631\r\n\u06d4"}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Gradient checkpointing \u06a9\u0648 trade computation \u06a9\u06d2 \u0644\u06cc\u06d2 memory\r\n\u0633\u06d2 torch.utils.checkpoint import checkpoint\r\n\r\nclass MemoryEfficientVLA:\r\n    def __init__(self, base_model):\r\n        super().__init__()\r\n        self.base_model = base_model\r\n    \r\n    def forward(self, images, text):\r\n        def run_part1(imgs, txt):\r\n            return self.base_model.encode_modalities(imgs, txt)\r\n        \r\n        def run_part2(fused):\r\n            return self.base_model.decode_actions(fused)\r\n        \r\n        # Apply gradient checkpointing \u06a9\u0648 reduce memory usage\r\n        fused_repr = checkpoint(run_part1, images, text)\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = checkpoint(run_part2, fused_repr)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n"})}),"\n",(0,i.jsx)(e.h3,{id:"jartb-\u06a9\u06cc-llimbai\u0626-\u06a9\u06cc-\u062d-\u062d-\u062d-\u062d-\u062d-\u062d-\u062d",children:"JARTB \u06a9\u06cc Llimbai\u0626 \u06a9\u06cc \u062d \u062d \u062d \u062d \u062d \u062d \u062d"}),"\n",(0,i.jsx)(e.p,{children:"\u0622\u067e \u06a9\u06d2 \u06a9\u06d2 mauna \u0632 m \u06a9 o \u0686 odair \u0637 vr \u067e Man \u06d2 \u067e \u067e \u067e \u067e \u067e manaun \u06d2 t manaun \u06d2 ta maun \u06d2 lulmba \u0626\u06cc"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Techniques \u06a9\u0648 handle \u0644\u0645\u0628\u0627 sequences:\r\n# 1. Sliding window attention\r\n# 2. Sparse attention patterns\r\n# 3. Linear attention approximations\r\n# 4. Hierarchical processing\r\n\r\nclass SlidingWindowAttention:\r\n    def __init__(self, embed_dim, num_heads, window_size=256):\r\n        super().__init__()\r\n        self.window_size = window_size\r\n        self.attention = nn.MultiheadAttention(\r\n            embed_dim, \r\n            num_heads, \r\n            batch_first=True\r\n        )\r\n    \r\n    def forward(self, x):\r\n        B, T, D = x.shape\r\n        \u0627\u06af\u0631 T <= self.window_size:\r\n            # Standard attention \u0627\u06af\u0631 sequence \u06c1\u06d2 \u0686\u06be\u0648\u0679\u0627 enough\r\n            return self.attention(x, x, x)[0]\r\n        \r\n        # Process \u0645\u06cc\u06ba overlapping windows\r\n        outputs = []\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 \u0645\u06cc\u06ba \u0645\u06cc\u06ba range(0, T, self.window_size):\r\n            end_idx = min\r\n            window = x[:, \u0645\u06cc\u06ba:end_idx, :]\r\n            \r\n            window_out = self.attention(window, window, window)[0]\r\n            outputs.append(window_out)\r\n        \r\n        # Combine outputs\r\n        return torch.cat(outputs, dim=1)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"maa-\u0688-l-\u06ccsaaln-\u06af-\u0627\u0648\u0633\u0646",children:"maa \u0688 l \u06ccsaaln \u06af \u0627\u0648\u0633\u0646"}),"\n",(0,i.jsx)(e.p,{children:"\u06a9\u06d2 \u0637 \u0637 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u067e \u067e \u06cc \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u067e \u067e \u06cc \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u0637 \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u0646\u06cc\u0686\u06d2 \u06cc"}),"\n",(0,i.jsx)(e.p,{children:"\u06d4\r\n."}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"** \u062a\u0686\u0631\u0628 \u06a9\u06d2 \u06a9\u06d2\r\n."}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644 scaling law considerations\r\nSCALING_RELATIONSHIPS = {\r\n    'parameters_vs_performance': 'Generally follows power law \u06a9\u06d2 \u0633\u0627\u062a\u06be diminishing returns',\r\n    'data_requirements': 'Scale roughly linearly \u06a9\u06d2 \u0633\u0627\u062a\u06be model size \u06a9\u06d2 \u0644\u06cc\u06d2 optimal training',\r\n    'compute_requirements': 'Scale superlinearly',\r\n    'inference_latency': 'Increases linearly \u06a9\u06d2 \u0644\u06cc\u06d2 feedforward models'\r\n}\n"})}),"\n",(0,i.jsx)(e.h2,{id:"\u062e-laa-\u0635\u06c1",children:"\u062e LAA \u0635\u06c1"}),"\n",(0,i.jsx)(e.p,{children:"\u06cc\u06c1 \u0627\u06cc\u0644\u0644 \u0644\u0650\u0644 \u060c \u0645\u0627\u0645\u0627 \u0624 \u0650 \u06c1 \u06c1 \u06c1 so/\u06a9\u06cc \u0688\u06cc\u067e lrnn \u06af llrnni \u06af llrnni \u06af \u0622 crauri \u0632 \u0622 rairi \u0632 vanaiuriaun vaun vaun luchnaui \u06cc\u062c- ina maa \u0688- l:"}),"\n",(0,i.jsx)(e.p,{children:"1\r\n2\r\n3\r\n4\r\n5\r\n6"}),"\n",(0,i.jsx)(e.p,{children:"\u0641\u06c1\u06cc \u0641\u06c1\u06cc \u0641\u06c1\u06cc \u06cc\u06c1 \u0622 \u0622 \u0622 \u0622 \u0622 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06c1\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9 \u06a9 \u06a9 \u06a9 j \u06a9 \u06a9 \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u067e \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u060c \u067e \u067e \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06c1 \u06c1 \u06c1 m \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1"})]})}function c(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(_,{...n})}):_(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>t,x:()=>d});var s=r(6540);const i={},a=s.createContext(i);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:t(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);