"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[6426],{7976:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models","title":"\u0686\u0679 2: VLA \u0628\u0646 \u06cc Adatatatat","description":"\u062c a \u062c","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models.md","sourceDirName":"module-4/week-2-vla-fundamentals","slug":"/module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models","permalink":"/ur/docs/module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-2-vla-fundamentals/2-1-introduction-to-vla-models.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"4.4: vla lamli \u06cc n \u0641 aa \u0630 \u0630 \u0630 \u0630 \u0630 \u0630 vr a \u06cc\u067e li \u06a9\u06cc\u0634 n \u0632","permalink":"/ur/docs/module-4/week-1-introduction/4-4-vla-practical-implementation"},"next":{"title":"itt \u06c1 3: vla an \u0636 \u0645\u0645\u0627\u0645","permalink":"/ur/docs/module-4/week-3-vla-integration/3-1-vla-integration-with-robotics"}}');var s=a(4848),t=a(8453);const i={sidebar_position:1,difficulty:"advanced"},l="\u0686\u0679 2: VLA \u0628\u0646 \u06cc Adatatatat",o={},d=[{value:"\u062c a \u062c",id:"\u062c-a-\u062c",level:2},{value:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d",id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",level:2},{value:"\u062a\u0639\u0627\u0631 \u0641 \u06a9 \u06a9 v v wausss \u0627\u0644\u0627 \u06d2 ma ma l \u0632",id:"\u062a\u0639\u0627\u0631-\u0641-\u06a9-\u06a9-v-v-wausss-\u0627\u0644\u0627-\u06d2-ma-ma-l-\u0632",level:2},{value:"\u06a9 a/\u06a9\u06cc vla \u067e\u06cc raaum",id:"\u06a9-a\u06a9\u06cc-vla-\u067e\u06cc-raaum",level:3},{value:"vla maa \u0688 l \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r",id:"vla-maa-\u0688-l-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r",level:2},{value:"bin \u06cc aad \u06cc \u06cc aa \u0621",id:"bin-\u06cc-aad-\u06cc-\u06cc-aa-\u0621",level:3},{value:"\u062a\u06cc\u06af\u0631\u0628 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 ri \u06cc\u0642\u06c1 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9",id:"\u062a\u06cc\u06af\u0631\u0628-\u06a9\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-ri-\u06cc\u0642\u06c1-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",level:2},{value:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a \u06a9 \u0627\u0639\u0645\u0644",id:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a-\u06a9-\u0627\u0639\u0645\u0644",level:3},{value:"vla maa \u0688 l \u06a9\u06cc mautli \u062d alat \u06cc\u06ba",id:"vla-maa-\u0688-l-\u06a9\u06cc-mautli-\u062d-alat-\u06cc\u06ba",level:2},{value:"rt-1 (\u0631\u0648\u0628\u0648\u0633\u0633 \u0679 \u0631\u06cc\u0646\u0633\u0648\u0631 1)",id:"rt-1-\u0631\u0648\u0628\u0648\u0633\u0633-\u0679-\u0631\u06cc\u0646\u0633\u0648\u0631-1",level:3},{value:"\u06a9 l \u06a9 vr \u0679",id:"\u06a9-l-\u06a9-vr-\u0679",level:3},{value:"\u0646\u0639 \u0686\u06cc ln \u062c\u0632",id:"\u0646\u0639-\u0686\u06cc-ln-\u062c\u0632",level:2},{value:"\u06ccsaaln\u06af verse \u06af int \u06cc",id:"\u06ccsaaln\u06af-verse-\u06af-int-\u06cc",level:3},{value:"\u0688\u06cc\u0679 a \u06a9\u06cc \u06a9 arard \u06af\u06cc",id:"\u0688\u06cc\u0679-a-\u06a9\u06cc-\u06a9-arard-\u06af\u06cc",level:3},{value:"\u0639\u0645\u0627\u0644\u06c1 \u0648\u0648\u0631\u06a9 \u0627\u0650\u0633",id:"\u0639\u0645\u0627\u0644\u06c1-\u0648\u0648\u0631\u06a9-\u0627\u0650\u0633",level:2},{value:"\u062e LAA \u0635\u06c1",id:"\u062e-laa-\u0635\u06c1",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"\u0686\u0679-2-vla-\u0628\u0646-\u06cc-adatatatat",children:"\u0686\u0679 2: VLA \u0628\u0646 \u06cc Adatatatat"})}),"\n",(0,s.jsx)(n.h2,{id:"\u062c-a-\u062c",children:"\u062c a \u062c"}),"\n",(0,s.jsx)(n.p,{children:"\u06cc\u06c1 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 vasnown- \u0632 \u0632 bain \u06a9\u06cc \u0632 bain ss \u06d2 mataaar \u0641 jrwata \u06c1\u06d2\u06d4"}),"\n",(0,s.jsx)(n.h2,{id:"ss-\u06cc\u06a9\u06be-n-\u06d2-\u06a9\u06d2-maua-\u0635-d",children:"ss \u06cc\u06a9\u06be n \u06d2 \u06a9\u06d2 maua \u0635 d"}),"\n",(0,s.jsx)(n.p,{children:"\u06a9\u06d2 \u0630 \u06a9 \u06a9 \u06a9 \u06a9 a/\u06a9\u06cc \u06a9\u06cc \u062e \u062e \u062e \u062e atatam \u06a9 a \u06cc\u06c1 \u06cc\u06c1 \u06c1\u0641 \u06c1\u0641 \u0622\u067e \u0622\u067e \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06d2 \u06d2 \u06d2 \u06d2 \u06af \u06af \u06af \u06af \u06af"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"smau/\u06a9\u06cc \u06a9\u06cc \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u0622 \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06a9"}),"\n",(0,s.jsx)(n.li,{children:"s \u06cc\u06a9\u06be\u06cc\u06ba \u06a9\u06cc s \u06d2 \u06a9\u06cc s \u06d2 \u06a9\u06cc \u06cc \u06cc \u06cc \u06cc \u06cc \u06d2 \u06d2 \u06d2 \u06d2 Maaul \u0632 anaumamaam wauchn \u060c \u0632 \u0632 \u060c \u0632 \u0632 \u060c \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632"}),"\n",(0,s.jsx)(n.li,{children:"\u06a9 a/\u06a9\u06cc atrb \u06cc at \u06a9\u06d2 \u0637 r \u06cc\u0642 \u06a9 r \u06cc\u0642 \u06a9 \u06a9 ar \u06a9 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2 \u06cc\u06d2"}),"\n",(0,s.jsx)(n.li,{children:"\u0628\u0646 \u06cc aad \u06cc vla maa \u0688 l a \u062c\u0632 aa \u0621 \u06a9 jna \u0641\u0630 \u06a9 r \u06cc\u06ba"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\u062a\u0639\u0627\u0631-\u0641-\u06a9-\u06a9-v-v-wausss-\u0627\u0644\u0627-\u06d2-ma-ma-l-\u0632",children:"\u062a\u0639\u0627\u0631 \u0641 \u06a9 \u06a9 v v wausss \u0627\u0644\u0627 \u06d2 ma ma l \u0632"}),"\n",(0,s.jsx)(n.p,{children:". \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0650 \u0650 \u0650 \u0650 \u0650 \u0650 \u0650 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u060c \u060c \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0650\u06a9 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632 \u0632"}),"\n",(0,s.jsx)(n.h3,{id:"\u06a9-a\u06a9\u06cc-vla-\u067e\u06cc-raaum",children:"\u06a9 a/\u06a9\u06cc vla \u067e\u06cc raaum"}),"\n",(0,s.jsx)(n.p,{children:"rwaut \u06cc rewaus jrbroce aaam \u0637 vr \u067e ri i \u06cc\u06a9 \u067e \u067e \u067e a \u0626\u067e laiauchn ni \u0642\u0637\u06c1 j \u06a9\u06cc \u067e\u06cc \u067e\u06cc \u067e\u06cc rroiu \u06a9 rata \u06c1\u06d2:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Perception -> Reasoning -> \u0627\u06cc\u06a9\u0634\u0646 Planning -> Execution\n"})}),"\n",(0,s.jsx)(n.p,{children:"\u0648\u0650\u0644 \u0627\u0650\u0633\u0633\u0644 \u0627\u0650\u0633 \u0645\u0627\u0645\u0627 \u0632 \u0646\u0650\u0646\u0650\u0646\u062d\u0631\u06cc\u0679\u0688 \u0627\u0639\u0627\u0631\u0648 \u0686 \u0627\u06be\u0627\u0646\u0627 \u06a9\u06cc \u0627:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Vision + Language -> Joint Embedding -> \u0627\u06cc\u06a9\u0634\u0646 Prediction\n"})}),"\n",(0,s.jsx)(n.h3,{id:""}),"\n",(0,s.jsx)(n.p,{children:"1\r\n2.\r\n3.\r\n4."}),"\n",(0,s.jsx)(n.h2,{id:"vla-maa-\u0688-l-\u0622-r-\u06a9\u06cc\u0679\u06cc\u06a9\u0686-r",children:"vla maa \u0688 l \u0622 r \u06a9\u06cc\u0679\u06cc\u06a9\u0686 r"}),"\n",(0,s.jsx)(n.h3,{id:"bin-\u06cc-aad-\u06cc-\u06cc-aa-\u0621",children:"bin \u06cc aad \u06cc \u06cc aa \u0621"}),"\n",(0,s.jsx)(n.p,{children:"i \u06cc\u06a9 aaam vla maa \u0688 l maus \u06cc\u06ba abatadai \u0634 aaml \u06c1\u06cc\u06ba:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{}),"\n",(0,s.jsx)(n.li,{}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"3"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 nn\r\n\r\nclass VLAModel:\r\n    def __init__(self, vision_encoder, language_encoder, action_head, fusion_layer):\r\n        super(VLAModel, self).__init__()\r\n        self.vision_encoder = vision_encoder\r\n        self.language_encoder = language_encoder\r\n        self.action_head = action_head\r\n        self.fusion_layer = fusion_layer\r\n    \r\n    def forward(self, images, text_commands):\r\n        # Encode visual input\r\n        vision_features = self.vision_encoder(images)\r\n        \r\n        # Encode language input\r\n        lang_features = self.language_encoder(text_commands)\r\n        \r\n        # Fuse multimodal features\r\n        fused_features = self.fusion_layer(vision_features, lang_features)\r\n        \r\n        # Generate \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(fused_features)\r\n        \r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n\r\nclass VisionEncoder:\r\n    def __init__(self):\r\n        super(VisionEncoder, self).__init__()\r\n        # Use \u0627\u06cc\u06a9 pre-trained vision model like ResNet, ViT, etc.\r\n        self.backbone = torch.hub.load('pytorch/vision:v0.10.0', \r\n                                       'resnet50', pretrained=True)\r\n        self.projection = nn.Linear(2048, 512)  # Project \u06a9\u0648 common space\r\n    \r\n    def forward(self, x):\r\n        features = self.backbone(x)\r\n        projected = self.projection(features)\r\n        return projected\r\n\r\nclass LanguageEncoder:\r\n    def __init__(self):\r\n        super(LanguageEncoder, self).__init__()\r\n        \u0633\u06d2 transformers import AutoTokenizer, AutoModel\r\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\r\n        self.backbone = AutoModel.from_pretrained('bert-base-uncased')\r\n        self.projection = nn.Linear(768, 512)  # Project \u06a9\u0648 common space\r\n    \r\n    def forward(self, text):\r\n        tokens = self.tokenizer(text, return_tensors='pt', padding=True)\r\n        features = self.backbone(**tokens).last_hidden_state[:, 0, :]  # CLS token\r\n        projected = self.projection(features)\r\n        return projected\r\n\r\nclass ActionHead:\r\n    def __init__(self):\r\n        super(ActionHead, self).__init__()\r\n        self.action_predictor = nn.Sequential(\r\n            nn.Linear(512, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 7)  # 7-DOF robotic arm joint velocities\r\n        )\r\n    \r\n    def forward(self, x):\r\n        return self.action_predictor(x)\r\n\r\nclass FusionLayer:\r\n    def __init__(self):\r\n        super(FusionLayer, self).__init__()\r\n        self.multi_modal_transformer = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(d_model=512, nhead=8),\r\n            num_layers=6\r\n        )\r\n    \r\n    def forward(self, vision_features, lang_features):\r\n        # Concatenate features along sequence dimension\r\n        combined_features = torch.cat([vision_features, lang_features], dim=1)\r\n        \r\n        # Apply multimodal transformer\r\n        fused_features = self.multi_modal_transformer(combined_features)\r\n        \r\n        # Return fused representation\r\n        return fused_features.mean(dim=1)  # Average pooling\n"})}),"\n",(0,s.jsx)(n.h2,{id:"\u062a\u06cc\u06af\u0631\u0628-\u06a9\u06d2-\u06a9\u06d2-\u06a9\u06d2-\u0637-\u0637-ri-\u06cc\u0642\u06c1-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9-\u06a9",children:"\u062a\u06cc\u06af\u0631\u0628 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0637 \u0637 ri \u06cc\u0642\u06c1 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9"}),"\n",(0,s.jsx)(n.h3,{id:"-1"}),"\n",(0,s.jsx)(n.p,{children:"\u0648\u0627\u0633 asl a \u0650 s maa \u0624 la \u0632 \u06a9 s"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"BAPR \u06cc MAUA \u06c1 DAT (ttasawaur \u060c WAUCH)"}),"\n",(0,s.jsx)(n.li,{children:"lsan \u06cc wauaa \u062d at (a \u062d\u06a9 amat \u060c \u060c \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u06c1 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u062d\u06a9 \u060c \u06c1 \u06c1 \u06c1\r\n\u06d4"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u0645\u062b\u0627\u0644 VLA dataset structure\r\nclass VLADataset(torch.utils.data.Dataset):\r\n    def __init__(self, data_path):\r\n        self.data_path = data_path\r\n        self.episodes = self.load_episodes()\r\n    \r\n    def __len__(self):\r\n        return len(self.episodes)\r\n    \r\n    def __getitem__(self, idx):\r\n        episode = self.episodes[idx]\r\n        \r\n        # Load visual observation\r\n        image = self.load_image(episode['image_path'])\r\n        \r\n        # Load language instruction\r\n        instruction = episode['instruction']\r\n        \r\n        # Load \u0627\u06cc\u06a9\u0634\u0646\r\n        \u0627\u06cc\u06a9\u0634\u0646 = torch.tensor\r\n        \r\n        return {\r\n            'image': image,\r\n            'instruction': instruction,\r\n            '\u0627\u06cc\u06a9\u0634\u0646': \u0627\u06cc\u06a9\u0634\u0646\r\n        }\r\n    \r\n    def load_episodes(self):\r\n        # Load episode metadata \u0633\u06d2 JSON \u06cc\u0627 similar\r\n        pass\r\n    \r\n    def load_image(self, path):\r\n        # Load \u0627\u0648\u0631 preprocess image\r\n        pass\n"})}),"\n",(0,s.jsx)(n.h3,{id:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a-\u06a9-\u0627\u0639\u0645\u0644",children:"\u062a\u0642\u0628\u0631\u0628\u06cc\u062a \u06a9 \u0627\u0639\u0645\u0644"}),"\n",(0,s.jsx)(n.p,{children:"\u0648\u0650\u0644 \u0627\u0650\u0633\u0633\u0644 \u0627\u0650\u0633 \u0645\u0627\u0645\u0627 \u0632 \u06c1\u06cc\u06ba \u06c1\u06cc\u06ba \u0627\u0639\u0645 \u0627\u0639\u0648\u0631 \u067e \u067e \u067e \u067e \u067e \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u06a9 \u062b \u062b \u062b \u062b\r\n1.\r\n2\r\n3"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def train_vla_model(model, dataset, num_epochs=10):\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    loss_fn = nn.MSELoss()\r\n    \r\n    \u06a9\u06d2 \u0644\u06cc\u06d2 epoch \u0645\u06cc\u06ba range(num_epochs):\r\n        epoch_loss = 0.0\r\n        \u06a9\u06d2 \u0644\u06cc\u06d2 batch \u0645\u06cc\u06ba torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True):\r\n            optimizer.zero_grad()\r\n            \r\n            # Forward pass\r\n            actions_pred = model(batch['image'], batch['instruction'])\r\n            \r\n            # Compute loss\r\n            loss = loss_fn\r\n            \r\n            # Backward pass\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            epoch_loss += loss.item()\r\n        \r\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataset):.4f}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vla-maa-\u0688-l-\u06a9\u06cc-mautli-\u062d-alat-\u06cc\u06ba",children:"vla maa \u0688 l \u06a9\u06cc mautli \u062d alat \u06cc\u06ba"}),"\n",(0,s.jsx)(n.h3,{id:"rt-1-\u0631\u0648\u0628\u0648\u0633\u0633-\u0679-\u0631\u06cc\u0646\u0633\u0648\u0631-1",children:"rt-1 (\u0631\u0648\u0628\u0648\u0633\u0633 \u0679 \u0631\u06cc\u0646\u0633\u0648\u0631 1)"}),"\n",(0,s.jsx)(n.p,{children:"\u06af \u0622\u06a9\u0633\u0644 \u06a9\u06d2 \u0622 \u0622 \u0679\u06cc -1 maa \u0688 l maul mausisaurmr \u06a9 a \u06a9 a jastaamal \u06a9\u06cc a \u06af\u06cc a \u06c1\u06d2\u06d4 \u06c1\u06d2\u06d4"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class RT1Model:\r\n    def __init__(self, num_tasks=100):\r\n        super(RT1Model, self).__init__()\r\n        self.vision_encoder = VisionEncoder()\r\n        self.task_encoder = nn.Embedding(num_tasks, 512)\r\n        self.transformer = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(d_model=512, nhead=8),\r\n            num_layers=12\r\n        )\r\n        self.action_head = nn.Linear(512, 7)  # 7-DOF \u0631\u0648\u0628\u0648\u0679 \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n    \r\n    def forward(self, images, task_id):\r\n        vision_features = self.vision_encoder(images)\r\n        task_features = self.task_encoder(task_id)\r\n        \r\n        # Concatenate \u0627\u0648\u0631 process\r\n        combined = torch.cat([vision_features, task_features], dim=1)\r\n        transformed = self.transformer(combined)\r\n        \r\n        # Predict \u0627\u06cc\u06a9\u0634\u0646\u0632\r\n        \u0627\u06cc\u06a9\u0634\u0646\u0632 = self.action_head(transformed[:, 0, :])  # Use \u067e\u06c1\u0644\u0627 token\r\n        return \u0627\u06cc\u06a9\u0634\u0646\u0632\n"})}),"\n",(0,s.jsx)(n.h3,{id:"\u06a9-l-\u06a9-vr-\u0679",children:"\u06a9 l \u06a9 vr \u0679"}),"\n",(0,s.jsx)(n.p,{children:"\u06a9\u0644\u067e\u0648\u0631\u0679 \u0646\u06d2 \u06a9\u0644\u067e (\u0648\u0698\u0646 \u0644\u06cc\u0646\u06af\u0648\u06cc\u062c \u0645\u0627\u0688\u0644) \u06a9\u0648 \u06cc\u06a9\u062c\u0627 \u06a9\u06cc\u0627 \u06c1\u06d2 \u06a9\u06d2 saat\u06be \u0645\u0642\u0627\u0645\u06cc \u062a\u0648\u062c\u06c1 \u06a9\u06d2 \u0644\u0650\u0644 \u0631\u0648\u0628\u0648\u0679\u06a9 \u06c1\u06cc\u0631\u0627 \u067e\u06be\u06cc\u0631\u06cc:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import clip\r\n\r\nclass CLIPortModel:\r\n    def __init__(self):\r\n        super(CLIPortModel, self).__init__()\r\n        # Load pre-trained CLIP model\r\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device='cuda')\r\n        \r\n        # Attention mechanisms \u06a9\u06d2 \u0644\u06cc\u06d2 spatial reasoning\r\n        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\r\n        \r\n        # Transport \u0627\u0648\u0631 place networks\r\n        self.transport_network = self.build_conv_network()\r\n        self.place_network = self.build_conv_network()\r\n    \r\n    def build_conv_network(self):\r\n        return nn.Sequential(\r\n            nn.Conv2d(512, 256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(128, 1, kernel_size=1)\r\n        )\r\n    \r\n    def forward(self, image, text):\r\n        # Encode image-text pair \u06a9\u06d2 \u0633\u0627\u062a\u06be CLIP\r\n        image_features = self.clip_model.encode_image(image)\r\n        text_features = self.clip_model.encode_text(clip.tokenize(text))\r\n        \r\n        # Apply spatial attention\r\n        attended_features = self.attention(\r\n            image_features, text_features, text_features\r\n        )\r\n        \r\n        # Generate transport \u0627\u0648\u0631 place heatmaps\r\n        transport_heatmap = self.transport_network(attended_features)\r\n        place_heatmap = self.place_network(attended_features)\r\n        \r\n        return transport_heatmap, place_heatmap\n"})}),"\n",(0,s.jsx)(n.h2,{id:"\u0646\u0639-\u0686\u06cc-ln-\u062c\u0632",children:"\u0646\u0639 \u0686\u06cc ln \u062c\u0632"}),"\n",(0,s.jsx)(n.h3,{id:"\u06ccsaaln\u06af-verse-\u06af-int-\u06cc",children:"\u06ccsaaln\u06af verse \u06af int \u06cc"}),"\n",(0,s.jsx)(n.p,{children:"\u0648\u0650\u0644 \u0627\u0650\u0633\u0644 \u0627\u0650\u0633\u0645\u0645\u0627\u0624 \u0632 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u06a9\u06d2 \u0650 \u0644\u0650\u0644 \u06c1 \u06c1 \u06c1M \u06a9 Maussmosnsnl wsasal \u06a9\u06cc \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636 \u0636:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Distributed training \u062a\u0631\u062a\u06cc\u0628 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0628\u0691\u0627 \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632\r\nimport torch.distributed \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 dist\r\n\u0633\u06d2 torch.nn.parallel import DistributedDataParallel \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 DDP\r\n\r\ndef setup_distributed_training():\r\n    # Initialize distributed training\r\n    dist.init_process_group(backend='nccl')\r\n    \r\n    # Create model \u0627\u0648\u0631 wrap \u06a9\u06d2 \u0633\u0627\u062a\u06be DDP\r\n    model = VLAModel(\r\n        vision_encoder=VisionEncoder(),\r\n        language_encoder=LanguageEncoder(),\r\n        action_head=ActionHead(),\r\n        fusion_layer=FusionLayer()\r\n    )\r\n    \r\n    model = DDP(model)\r\n    \r\n    return model\n"})}),"\n",(0,s.jsx)(n.h3,{id:"\u0688\u06cc\u0679-a-\u06a9\u06cc-\u06a9-arard-\u06af\u06cc",children:"\u0688\u06cc\u0679 a \u06a9\u06cc \u06a9 arard \u06af\u06cc"}),"\n",(0,s.jsx)(n.p,{children:"\u0679 r \u06cc nnn \u06af vaul ala \u06d2 slaus maa \u0624 lai mwaur \u0637 ri \u06cc\u0642\u06d2 saudaudududud \u0688\u06cc\u0679 a:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Few-shot learning approaches \u06a9\u06d2 \u0644\u06cc\u06d2 \u0648\u06cc \u0627\u06cc\u0644 \u0627\u06d2 \u0645\u0627\u0688\u0644\u0632\r\nclass FewShotVLA:\r\n    def __init__(self, base_model):\r\n        super(FewShotVLA, self).__init__()\r\n        self.base_model = base_model\r\n        self.adaptation_head = nn.Linear(512, 7)  # Adjust \u06a9\u06d2 \u0644\u06cc\u06d2 \u0646\u06cc\u0627 tasks\r\n    \r\n    def forward(self, images, text, support_set=None):\r\n        \u0627\u06af\u0631 support_set \u06c1\u06d2 \u0646\u06c1\u06cc\u06ba None:\r\n            # Adapt \u06a9\u0648 \u0646\u06cc\u0627 task using support set\r\n            adapted_features = self.adapt_to_task(support_set)\r\n        else:\r\n            # Use base model directly\r\n            adapted_features = self.base_model(images, text)\r\n        \r\n        return self.adaptation_head(adapted_features)\r\n    \r\n    def adapt_to_task(self, support_set):\r\n        # \u0646\u0641\u0627\u0630 \u06a9\u06d2 \u0644\u06cc\u06d2 task adaptation\r\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"\u0639\u0645\u0627\u0644\u06c1-\u0648\u0648\u0631\u06a9-\u0627\u0650\u0633",children:"\u0639\u0645\u0627\u0644\u06c1 \u0648\u0648\u0631\u06a9 \u0627\u0650\u0633"}),"\n",(0,s.jsx)(n.p,{children:"\u06c1\u0641 \u06c1\u0641 t \u06c1 's vr \u06a9 iass maus Buna \u06cc aad \u06cc vla maa \u0688 l \u06a9 o jna \u0641\u0630 inaa \u0641\u0630 inaa \u0634 aaml \u06c1\u06d2:"}),"\n",(0,s.jsx)(n.p,{children:"1\r\n2\r\n3.\r\n4. \u0679\u06cc s \u0679 \u06a9 a/\u06a9\u06cc maa \u0688 l - \u0633\u0639\u062f \u06c1 rewboc \u06a9 am"}),"\n",(0,s.jsx)(n.h2,{id:"\u062e-laa-\u0635\u06c1",children:"\u062e LAA \u0635\u06c1"}),"\n",(0,s.jsx)(n.p,{children:"\u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 \u06c1\u0641 vision \u0648\u0698\u0646 \u06a9\u06cc \u0632\u06cc\u0646\u062a \u0633\u06d2 \u0686\u0644\u0646\u06d2 \u0648\u0627\u0644\u06cc \u0632\u0628\u0627\u0646- iagn (vla) maa\u0688l \u060c \u062c \u06a9\u06cc s \u06a9\u06cc \u06a9\u06cc \u0646 jhaund\u06af\u06cc \u06a9srati\u06d2 \u062cs \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u06a9\u06cc \u0622\u067e '' '' '' '' '' '' '' '' \u0622\u06cc\u0627\u062a \u0686\u06cc ln \u062c\u0632\u06d4 \u0627\u06af\u0644\u0627 \u060c \u06c1 \u06c1 \u06c1 m vla maa \u0688 l an \u0636 mamam \u06a9\u06d2 SAAT \u06be rewboc sssaumi \u06a9 s"})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>l});var r=a(6540);const s={},t=r.createContext(s);function i(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);