"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[5571],{953:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3/week-4-isaac-sim-applications/4-1-isaac-sim-applications","title":"Week 4: Isaac Sim Applications","description":"Overview","source":"@site/docs/module-3/week-4-isaac-sim-applications/4-1-isaac-sim-applications.md","sourceDirName":"module-3/week-4-isaac-sim-applications","slug":"/module-3/week-4-isaac-sim-applications/4-1-isaac-sim-applications","permalink":"/docs/module-3/week-4-isaac-sim-applications/4-1-isaac-sim-applications","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-3/week-4-isaac-sim-applications/4-1-isaac-sim-applications.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"difficulty":"intermediate"},"sidebar":"tutorialSidebar","previous":{"title":"Week 3: Advanced Isaac Sim","permalink":"/docs/module-3/week-3-advanced-isaac-sim/3-1-advanced-isaac-sim-techniques"},"next":{"title":"4.1: Overview of Vision-Language-Action (VLA) Models","permalink":"/docs/module-4/week-1-introduction/4-1-overview-of-vla-models"}}');var a=r(4848),t=r(8453);const o={sidebar_position:3,difficulty:"intermediate"},s="Week 4: Isaac Sim Applications",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Autonomous Robot Validation",id:"autonomous-robot-validation",level:2},{value:"Navigation Pipeline Validation",id:"navigation-pipeline-validation",level:3},{value:"Obstacle Avoidance Testing",id:"obstacle-avoidance-testing",level:3},{value:"Perception Pipeline Testing",id:"perception-pipeline-testing",level:2},{value:"Sensor Data Validation",id:"sensor-data-validation",level:3},{value:"Hardware-in-the-Loop Testing",id:"hardware-in-the-loop-testing",level:2},{value:"Integration with Real Hardware",id:"integration-with-real-hardware",level:3},{value:"AI Training with Isaac Sim",id:"ai-training-with-isaac-sim",level:2},{value:"Reinforcement Learning Environment",id:"reinforcement-learning-environment",level:3},{value:"Deployment Strategy",id:"deployment-strategy",level:2},{value:"From Simulation to Real Robot",id:"from-simulation-to-real-robot",level:3},{value:"Practical Exercise",id:"practical-exercise",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-4-isaac-sim-applications",children:"Week 4: Isaac Sim Applications"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This week explores real-world applications of Isaac Sim in robotics development, including testing autonomous systems, validating perception algorithms, and accelerating robot development cycles through simulation."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, you will:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Apply Isaac Sim for autonomous robot validation"}),"\n",(0,a.jsx)(n.li,{children:"Implement perception pipeline testing in simulation"}),"\n",(0,a.jsx)(n.li,{children:"Use Isaac Sim for hardware-in-the-loop testing"}),"\n",(0,a.jsx)(n.li,{children:"Deploy simulation-tested solutions to real robots"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"autonomous-robot-validation",children:"Autonomous Robot Validation"}),"\n",(0,a.jsx)(n.h3,{id:"navigation-pipeline-validation",children:"Navigation Pipeline Validation"}),"\n",(0,a.jsx)(n.p,{children:"Using Isaac Sim to validate navigation pipelines before real-world deployment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Navigation validation in Isaac Sim\r\nimport omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.navigation import PathPlanner\r\nfrom omni.isaac.range_sensor import _RangeSensor\r\nimport numpy as np\r\n\r\nclass NavigationValidator:\r\n    def __init__(self):\r\n        self.world = World(stage_units_in_meters=1.0)\r\n        self.path_planner = None\r\n        self.metrics_collector = MetricsCollector()\r\n        \r\n    def setup_navigation_validation(self):\r\n        # Create a complex test environment\r\n        self.create_validation_environment()\r\n        \r\n        # Initialize navigation stack\r\n        self.init_navigation_stack()\r\n        \r\n    def create_validation_environment(self):\r\n        # Add multiple test environments\r\n        environments = [\r\n            "warehouse",\r\n            "hospital", \r\n            "outdoor_urban",\r\n            "indoor_office"\r\n        ]\r\n        \r\n        for i, env_name in enumerate(environments):\r\n            env_path = f"omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Environments/{env_name}.usd"\r\n            # Add environment to simulation\r\n            pass  # Implementation would add the environment\r\n    \r\n    def init_navigation_stack(self):\r\n        # Initialize ROS 2 navigation stack in simulation\r\n        from geometry_msgs.msg import PoseStamped\r\n        from nav_msgs.msg import Path\r\n        from sensor_msgs.msg import LaserScan\r\n        \r\n        # Setup navigation topics\r\n        self.goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\r\n        self.path_sub = self.create_subscription(Path, \'/plan\', self.path_callback, 10)\r\n        self.lidar_sub = self.create_subscription(LaserScan, \'/scan\', self.lidar_callback, 10)\r\n    \r\n    def run_validation_test(self, test_scenario):\r\n        # Run navigation test in simulated environment\r\n        start_pose = test_scenario[\'start_pose\']\r\n        goal_pose = test_scenario[\'goal_pose\']\r\n        \r\n        # Set robot initial position\r\n        self.set_robot_pose(start_pose)\r\n        \r\n        # Send navigation goal\r\n        goal_msg = PoseStamped()\r\n        goal_msg.header.frame_id = "map"\r\n        goal_msg.pose = goal_pose\r\n        self.goal_pub.publish(goal_msg)\r\n        \r\n        # Monitor navigation progress\r\n        success = self.monitor_navigation(goal_pose)\r\n        \r\n        # Collect metrics\r\n        metrics = self.metrics_collector.get_metrics()\r\n        \r\n        return {\r\n            "success": success,\r\n            "metrics": metrics,\r\n            "scenario": test_scenario\r\n        }\r\n    \r\n    def monitor_navigation(self, goal_pose, timeout=60.0):\r\n        # Monitor navigation to goal\r\n        start_time = self.get_clock().now()\r\n        \r\n        while (self.get_clock().now() - start_time).nanoseconds < timeout * 1e9:\r\n            if self.at_goal_position(goal_pose):\r\n                return True\r\n            self.world.step(render=True)\r\n        \r\n        return False  # Timeout\r\n    \r\n    def at_goal_position(self, goal_pose, threshold=0.5):\r\n        # Check if robot is within threshold of goal\r\n        robot_pos = self.get_robot_position()\r\n        distance = np.linalg.norm(np.array([robot_pos.x, robot_pos.y]) - \r\n                                  np.array([goal_pose.position.x, goal_pose.position.y]))\r\n        return distance < threshold\n'})}),"\n",(0,a.jsx)(n.h3,{id:"obstacle-avoidance-testing",children:"Obstacle Avoidance Testing"}),"\n",(0,a.jsx)(n.p,{children:"Test obstacle avoidance algorithms in complex simulated environments:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ObstacleAvoidanceValidator:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.test_results = []\r\n        \r\n    def test_obstacle_scenarios(self):\r\n        scenarios = [\r\n            {\r\n                "name": "static_obstacles",\r\n                "obstacles": self.create_static_obstacles(),\r\n                "robot_path": [0, 0, 10, 10]  # start_x, start_y, goal_x, goal_y\r\n            },\r\n            {\r\n                "name": "dynamic_obstacles",\r\n                "obstacles": self.create_dynamic_obstacles(),\r\n                "robot_path": [0, 0, 10, 10]\r\n            },\r\n            {\r\n                "name": "narrow_corridor",\r\n                "obstacles": self.create_narrow_corridor(),\r\n                "robot_path": [0, 0, 10, 0]\r\n            }\r\n        ]\r\n        \r\n        for scenario in scenarios:\r\n            result = self.run_scenario_test(scenario)\r\n            self.test_results.append(result)\r\n    \r\n    def create_dynamic_obstacles(self):\r\n        # Create moving obstacles that test collision avoidance\r\n        from omni.isaac.core.objects import DynamicCuboid\r\n        \r\n        obstacles = []\r\n        for i in range(5):\r\n            obstacle = DynamicCuboid(\r\n                prim_path=f"/World/dynamic_obstacle_{i}",\r\n                name=f"obstacle_{i}",\r\n                position=[5.0, i, 0.5],\r\n                size=0.5,\r\n                mass=1.0\r\n            )\r\n            obstacles.append({\r\n                "object": obstacle,\r\n                "path": [(5, i), (5, i+5)],  # movement path\r\n                "speed": 0.5  # m/s\r\n            })\r\n        \r\n        return obstacles\r\n    \r\n    def run_scenario_test(self, scenario):\r\n        # Run the obstacle avoidance test for a scenario\r\n        # Implementation would move obstacles along paths and test robot response\r\n        pass\n'})}),"\n",(0,a.jsx)(n.h2,{id:"perception-pipeline-testing",children:"Perception Pipeline Testing"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-data-validation",children:"Sensor Data Validation"}),"\n",(0,a.jsx)(n.p,{children:"Validate perception pipelines using Isaac Sim's sensor simulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Perception pipeline validation\r\nfrom omni.isaac.sensor import Camera\r\nfrom omni.isaac.range_sensor import LidarRtx\r\nimport cv2\r\n\r\nclass PerceptionValidator:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        # Setup multiple sensor types\r\n        self.setup_sensors()\r\n        \r\n    def setup_sensors(self):\r\n        # RGB camera\r\n        self.rgb_camera = Camera(\r\n            prim_path="/World/rgb_camera",\r\n            position=[0.2, 0, 0.1],\r\n            frequency=30\r\n        )\r\n        \r\n        # Depth camera\r\n        self.depth_camera = Camera(\r\n            prim_path="/World/depth_camera",\r\n            position=[0.2, 0, 0.1],\r\n            frequency=30\r\n        )\r\n        \r\n        # 3D LIDAR\r\n        self.lidar = LidarRtx(\r\n            prim_path="/World/lidar",\r\n            name="sensor",\r\n            translation=(0.2, 0, 0.1),\r\n            configuration=self.lidar.create_lidar_sensor(\r\n                fps=20,\r\n                horizontal_resolution=1080,\r\n                vertical_resolution=32,\r\n                horizontal_laser_angle=3.14,\r\n                vertical_laser_angle=0.5,\r\n                max_range=20,\r\n                min_range=0.1\r\n            )\r\n        )\r\n    \r\n    def validate_detection_pipeline(self, detection_algorithm):\r\n        # Validate a detection pipeline against ground truth\r\n        test_objects = self.setup_test_objects()\r\n        \r\n        for obj in test_objects:\r\n            # Get sensor data\r\n            rgb_image = self.rgb_camera.get_rgb()\r\n            depth_image = self.depth_camera.get_depth()\r\n            lidar_data = self.lidar.get_linear_depth_data()\r\n            \r\n            # Run detection algorithm\r\n            detections = detection_algorithm.process(\r\n                rgb_image, depth_image, lidar_data\r\n            )\r\n            \r\n            # Compare with ground truth\r\n            ground_truth = self.get_ground_truth(obj)\r\n            accuracy = self.calculate_accuracy(detections, ground_truth)\r\n            \r\n            # Log results\r\n            self.log_detection_result(obj, detections, ground_truth, accuracy)\r\n    \r\n    def get_ground_truth(self, obj):\r\n        # Get ground truth for test object\r\n        return {\r\n            "position": obj.get_world_pose(),\r\n            "dimensions": obj.get_bounding_box(),\r\n            "class": obj.get_class_label()\r\n        }\r\n    \r\n    def calculate_accuracy(self, detections, ground_truth):\r\n        # Calculate detection accuracy metrics\r\n        # Implementation would compare detections to ground truth\r\n        pass\n'})}),"\n",(0,a.jsx)(n.h2,{id:"hardware-in-the-loop-testing",children:"Hardware-in-the-Loop Testing"}),"\n",(0,a.jsx)(n.h3,{id:"integration-with-real-hardware",children:"Integration with Real Hardware"}),"\n",(0,a.jsx)(n.p,{children:"Using Isaac Sim in hardware-in-the-loop (HIL) testing scenarios:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import JointState\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass HardwareInLoopTest(Node):\r\n    def __init__(self):\r\n        super().__init__('hil_test_node')\r\n        \r\n        # Publishers for real robot\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/real_robot/cmd_vel', 10)\r\n        self.sim_cmd_vel_pub = self.create_publisher(Twist, '/sim_robot/cmd_vel', 10)\r\n        \r\n        # Subscribers for real robot feedback\r\n        self.joint_states_sub = self.create_subscription(\r\n            JointState, '/real_robot/joint_states', self.joint_state_callback, 10)\r\n        \r\n        # Simulation interfaces\r\n        self.setup_simulation_interfaces()\r\n        \r\n        # Synchronization timer\r\n        self.timer = self.create_timer(0.1, self.sync_callback)\r\n        \r\n    def setup_simulation_interfaces(self):\r\n        # Setup interfaces to Isaac Sim\r\n        pass\r\n    \r\n    def sync_callback(self):\r\n        # Synchronize real robot and simulation\r\n        # Send commands to both real robot and simulation\r\n        # Compare responses\r\n        pass\r\n    \r\n    def joint_state_callback(self, msg):\r\n        # Receive joint states from real robot\r\n        # Apply to simulation robot for synchronization\r\n        pass\n"})}),"\n",(0,a.jsx)(n.h2,{id:"ai-training-with-isaac-sim",children:"AI Training with Isaac Sim"}),"\n",(0,a.jsx)(n.h3,{id:"reinforcement-learning-environment",children:"Reinforcement Learning Environment"}),"\n",(0,a.jsx)(n.p,{children:"Create Isaac Sim as a reinforcement learning environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import gym\r\nfrom gym import spaces\r\nimport numpy as np\r\n\r\nclass IsaacSimRLEnv(gym.Env):\r\n    def __init__(self, sim_world):\r\n        super(IsaacSimRLEnv, self).__init__()\r\n        \r\n        self.world = sim_world\r\n        self.robot = None\r\n        \r\n        # Define action and observation spaces\r\n        self.action_space = spaces.Box(\r\n            low=-1.0, high=1.0, shape=(2,), dtype=np.float32  # linear vel, angular vel\r\n        )\r\n        \r\n        self.observation_space = spaces.Box(\r\n            low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32  # 20-dim state vector\r\n        )\r\n    \r\n    def reset(self):\r\n        # Reset the simulation to initial state\r\n        self.world.reset()\r\n        \r\n        # Place robot at random starting position\r\n        start_pos = self.sample_random_position()\r\n        self.robot.set_position(start_pos)\r\n        \r\n        return self.get_observation()\r\n    \r\n    def step(self, action):\r\n        # Execute action in simulation\r\n        self.apply_action(action)\r\n        \r\n        # Step simulation\r\n        self.world.step(render=True)\r\n        \r\n        # Get observation\r\n        obs = self.get_observation()\r\n        \r\n        # Calculate reward\r\n        reward = self.calculate_reward()\r\n        \r\n        # Check if episode is done\r\n        done = self.is_episode_done()\r\n        \r\n        info = {}\r\n        \r\n        return obs, reward, done, info\r\n    \r\n    def get_observation(self):\r\n        # Get current state observation\r\n        # This could include robot pose, sensor readings, etc.\r\n        pass\r\n    \r\n    def apply_action(self, action):\r\n        # Apply action to robot in simulation\r\n        linear_vel, angular_vel = action\r\n        # Send command to robot simulator\r\n        pass\r\n    \r\n    def calculate_reward(self):\r\n        # Calculate reward based on robot behavior\r\n        pass\r\n    \r\n    def is_episode_done(self):\r\n        # Check if episode should end\r\n        pass\n"})}),"\n",(0,a.jsx)(n.h2,{id:"deployment-strategy",children:"Deployment Strategy"}),"\n",(0,a.jsx)(n.h3,{id:"from-simulation-to-real-robot",children:"From Simulation to Real Robot"}),"\n",(0,a.jsx)(n.p,{children:"Best practices for deploying simulation-tested solutions to real robots:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class DeploymentPreparer:\r\n    def __init__(self):\r\n        self.sim_to_real_mapping = {}\r\n        \r\n    def create_sim_to_real_mapping(self):\r\n        # Map simulation parameters to real robot parameters\r\n        self.sim_to_real_mapping = {\r\n            "sim_camera_info": "real_camera_info",\r\n            "sim_lidar_frame": "real_lidar_frame",\r\n            "sim_odom_topic": "real_odom_topic",\r\n            "sim_cmd_vel_topic": "real_cmd_vel_topic"\r\n        }\r\n    \r\n    def simulate_real_world_conditions(self):\r\n        # Add noise and uncertainty to simulation\r\n        # to better match real-world behavior\r\n        \r\n        # Add sensor noise\r\n        self.add_sensor_noise()\r\n        \r\n        # Add actuator delays\r\n        self.add_actuator_delays()\r\n        \r\n        # Add environmental uncertainties\r\n        self.add_env_uncertainties()\r\n    \r\n    def domain_randomization(self):\r\n        # Apply domain randomization to improve transfer\r\n        self.randomize_physics_params()\r\n        self.randomize_sensor_params()\r\n        self.randomize_environment_params()\r\n    \r\n    def validation_pipeline(self):\r\n        # Validate solution across multiple sim conditions\r\n        # before real-world deployment\r\n        test_conditions = [\r\n            "different_lighting",\r\n            "varied_terrain",\r\n            "sensor_noise",\r\n            "dynamic_obstacles"\r\n        ]\r\n        \r\n        for condition in test_conditions:\r\n            success_rate = self.validate_in_condition(condition)\r\n            if success_rate < 0.95:  # 95% threshold\r\n                print(f"Failed validation in {condition}: {success_rate}")\r\n                return False\r\n        \r\n        return True\n'})}),"\n",(0,a.jsx)(n.h2,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,a.jsx)(n.p,{children:"This week's exercise involves creating a complete validation pipeline:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Set up an Isaac Sim environment for navigation testing"}),"\n",(0,a.jsx)(n.li,{children:"Implement perception pipeline validation"}),"\n",(0,a.jsx)(n.li,{children:"Test obstacle avoidance in various scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Prepare a solution for deployment to a real robot"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This week explored real-world applications of Isaac Sim including autonomous robot validation, perception pipeline testing, hardware-in-the-loop testing, and AI training. You've learned how to effectively use Isaac Sim to accelerate robot development and validate solutions before real-world deployment. This concludes Module 3 on NVIDIA Isaac Sim."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var i=r(6540);const a={},t=i.createContext(a);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);