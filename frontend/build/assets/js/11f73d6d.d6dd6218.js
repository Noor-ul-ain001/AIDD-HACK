"use strict";(globalThis.webpackChunkphysical_ai_platform_frontend=globalThis.webpackChunkphysical_ai_platform_frontend||[]).push([[7067],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}},8816:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/week-1-introduction/4-2-vla-architecture-deep-learning","title":"4.2: VLA Model Architecture and Deep Learning Fundamentals","description":"Overview","source":"@site/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning.md","sourceDirName":"module-4/week-1-introduction","slug":"/module-4/week-1-introduction/4-2-vla-architecture-deep-learning","permalink":"/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/noor-ana/physical-ai-platform/tree/main/docs/module-4/week-1-introduction/4-2-vla-architecture-deep-learning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"difficulty":"advanced"},"sidebar":"tutorialSidebar","previous":{"title":"4.1: Overview of Vision-Language-Action (VLA) Models","permalink":"/docs/module-4/week-1-introduction/4-1-overview-of-vla-models"},"next":{"title":"4.3: VLA Training Data Collection and Preparation","permalink":"/docs/module-4/week-1-introduction/4-3-vla-training-data-collection"}}');var s=r(4848),t=r(8453);const o={sidebar_position:2,difficulty:"advanced"},a="4.2: VLA Model Architecture and Deep Learning Fundamentals",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Foundation: Neural Network Components",id:"foundation-neural-network-components",level:2},{value:"Vision Encoders",id:"vision-encoders",level:3},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:4},{value:"Vision Transformers (ViTs)",id:"vision-transformers-vits",level:4},{value:"Hybrid Approaches",id:"hybrid-approaches",level:4},{value:"Language Encoders",id:"language-encoders",level:3},{value:"Transformer-based Language Models",id:"transformer-based-language-models",level:4},{value:"Action Decoders",id:"action-decoders",level:3},{value:"Continuous Action Spaces",id:"continuous-action-spaces",level:4},{value:"Discrete Action Spaces",id:"discrete-action-spaces",level:4},{value:"Attention Mechanisms in VLA Models",id:"attention-mechanisms-in-vla-models",level:2},{value:"Self-Attention",id:"self-attention",level:3},{value:"Cross-Attention",id:"cross-attention",level:3},{value:"Multimodal Attention",id:"multimodal-attention",level:3},{value:"VLA Architecture Patterns",id:"vla-architecture-patterns",level:2},{value:"Unified Transformer Architecture",id:"unified-transformer-architecture",level:3},{value:"Encoder-Decoder Architecture",id:"encoder-decoder-architecture",level:3},{value:"Multimodal Fusion Techniques",id:"multimodal-fusion-techniques",level:2},{value:"Early Fusion",id:"early-fusion",level:3},{value:"Late Fusion",id:"late-fusion",level:3},{value:"Hierarchical Fusion",id:"hierarchical-fusion",level:3},{value:"Training Paradigms",id:"training-paradigms",level:2},{value:"Behavioral Cloning (BC)",id:"behavioral-cloning-bc",level:3},{value:"Reinforcement Learning from Human Feedback (RLHF)",id:"reinforcement-learning-from-human-feedback-rlhf",level:3},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Computational Considerations",id:"computational-considerations",level:2},{value:"Memory Efficiency",id:"memory-efficiency",level:3},{value:"Sequence Length Limitations",id:"sequence-length-limitations",level:3},{value:"Model Scaling Laws",id:"model-scaling-laws",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"42-vla-model-architecture-and-deep-learning-fundamentals",children:"4.2: VLA Model Architecture and Deep Learning Fundamentals"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This submodule examines the deep learning architectures that power Vision-Language-Action (VLA) models. We'll explore the foundational neural network components, attention mechanisms, multimodal fusion techniques, and how these elements work together to enable intelligent robotic behavior."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this submodule, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the foundational deep learning architectures used in VLA models"}),"\n",(0,s.jsx)(n.li,{children:"Learn about transformer-based architectures for multimodal processing"}),"\n",(0,s.jsx)(n.li,{children:"Explore attention mechanisms and their role in VLA models"}),"\n",(0,s.jsx)(n.li,{children:"Understand the technical components of multimodal fusion"}),"\n",(0,s.jsx)(n.li,{children:"Learn about sequence modeling for action generation"}),"\n",(0,s.jsx)(n.li,{children:"Appreciate the computational requirements of VLA models"}),"\n",(0,s.jsx)(n.li,{children:"Understand the difference between encoder, decoder, and encoder-decoder architectures"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"foundation-neural-network-components",children:"Foundation: Neural Network Components"}),"\n",(0,s.jsx)(n.h3,{id:"vision-encoders",children:"Vision Encoders"}),"\n",(0,s.jsx)(n.p,{children:"Vision encoders in VLA models typically use one of these architectures:"}),"\n",(0,s.jsx)(n.h4,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Series of convolutional layers followed by pooling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Translation equivariance, hierarchical feature extraction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disadvantages"}),": Limited long-range relationships, fixed receptive field"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Early VLA models, embedded systems"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass VisionEncoder(nn.Module):\r\n    def __init__(self, input_channels=3, feature_dim=512):\r\n        super().__init__()\r\n        self.conv_layers = nn.Sequential(\r\n            nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\r\n            nn.ReLU(),\r\n            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\r\n        )\r\n        self.projection = nn.Linear(256, feature_dim)\r\n    \r\n    def forward(self, x):\r\n        features = self.conv_layers(x)  # Shape: (batch, 256, 1, 1)\r\n        features = features.view(features.size(0), -1)  # Flatten\r\n        projected_features = self.projection(features)  # Shape: (batch, feature_dim)\r\n        return projected_features\n"})}),"\n",(0,s.jsx)(n.h4,{id:"vision-transformers-vits",children:"Vision Transformers (ViTs)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Patch embeddings + transformer blocks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Global attention, scalable to large models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disadvantages"}),": Large data requirements, quadratic complexity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Modern VLA models, state-of-the-art performance"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass VisionTransformer(nn.Module):\r\n    def __init__(self, patch_size=16, num_channels=3, embed_dim=768, depth=12, num_heads=12):\r\n        super().__init__()\r\n        self.patch_size = patch_size\r\n        self.embed_dim = embed_dim\r\n        self.num_patches = (224 // patch_size) ** 2\r\n        \r\n        # Patch embedding layer\r\n        self.patch_embed = nn.Conv2d(num_channels, embed_dim, \r\n                                     kernel_size=patch_size, stride=patch_size)\r\n        \r\n        # Positional embeddings\r\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\r\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\r\n        \r\n        # Transformer blocks\r\n        self.blocks = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\r\n        ])\r\n        \r\n        self.norm = nn.LayerNorm(embed_dim)\r\n    \r\n    def forward(self, x):\r\n        B, C, H, W = x.shape\r\n        \r\n        # Convert image to patches\r\n        x = self.patch_embed(x)  # (B, embed_dim, num_patches_h, num_patches_w)\r\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\r\n        \r\n        # Add class token\r\n        cls_tokens = self.cls_token.expand(B, -1, -1)\r\n        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)\r\n        \r\n        # Add positional embeddings\r\n        x = x + self.pos_embed[:, :x.size(1)]\r\n        \r\n        # Apply transformer blocks\r\n        for block in self.blocks:\r\n            x = block(x)\r\n        \r\n        x = self.norm(x)\r\n        return x[:, 0]  # Return class token embedding\r\n\r\nclass TransformerBlock(nn.Module):\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\r\n        self.norm1 = nn.LayerNorm(embed_dim)\r\n        self.norm2 = nn.LayerNorm(embed_dim)\r\n        self.mlp = nn.Sequential(\r\n            nn.Linear(embed_dim, embed_dim * 4),\r\n            nn.GELU(),\r\n            nn.Linear(embed_dim * 4, embed_dim)\r\n        )\r\n    \r\n    def forward(self, x):\r\n        # Self-attention\r\n        attn_out, _ = self.attention(x, x, x)\r\n        x = x + attn_out\r\n        x = self.norm1(x)\r\n        \r\n        # Feed-forward\r\n        mlp_out = self.mlp(x)\r\n        x = x + mlp_out\r\n        x = self.norm2(x)\r\n        \r\n        return x\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hybrid-approaches",children:"Hybrid Approaches"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ConvNeXt"}),": Convolutional layers with Transformer-style normalization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Swin Transformer"}),": Shifted windows for local-global attention"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Attention"}),": Linear attention mechanisms for efficiency"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"language-encoders",children:"Language Encoders"}),"\n",(0,s.jsx)(n.p,{children:"Language encoders in VLA models are typically based on transformer architectures:"}),"\n",(0,s.jsx)(n.h4,{id:"transformer-based-language-models",children:"Transformer-based Language Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BERT style"}),": Bidirectional context understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPT style"}),": Causal generation of text and actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"T5 style"}),": Encoder-decoder for text-to-text tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass LanguageEncoder(nn.Module):\r\n    def __init__(self, vocab_size=50257, embed_dim=768, max_seq_len=512, num_layers=12, num_heads=12):\r\n        super().__init__()\r\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\r\n        self.pos_embedding = nn.Embedding(max_seq_len, embed_dim)\r\n        \r\n        self.blocks = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\r\n        ])\r\n        \r\n        self.ln_f = nn.LayerNorm(embed_dim)  # Final layer norm\r\n    \r\n    def forward(self, input_ids, attention_mask=None):\r\n        # Embed tokens\r\n        token_emb = self.token_embedding(input_ids)  # (B, seq_len, embed_dim)\r\n        \r\n        # Add positional embeddings\r\n        seq_len = input_ids.size(1)\r\n        pos_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)\r\n        pos_emb = self.pos_embedding(pos_ids)  # (seq_len, embed_dim)\r\n        pos_emb = pos_emb.unsqueeze(0)  # (1, seq_len, embed_dim)\r\n        \r\n        x = token_emb + pos_emb\r\n        \r\n        # Apply transformer blocks\r\n        for block in self.blocks:\r\n            x = block(x)\r\n        \r\n        x = self.ln_f(x)\r\n        return x  # (B, seq_len, embed_dim)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"action-decoders",children:"Action Decoders"}),"\n",(0,s.jsx)(n.p,{children:"Action decoders transform high-level representations into low-level robotic commands:"}),"\n",(0,s.jsx)(n.h4,{id:"continuous-action-spaces",children:"Continuous Action Spaces"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MuJoCo-style"}),": Joint angles, velocities, forces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Operational Space"}),": End-effector positions, rotations"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"discrete-action-spaces",children:"Discrete Action Spaces"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Primitive Actions"}),": Pick, place, open, close"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Symbolic Actions"}),": High-level commands"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass ActionDecoder(nn.Module):\r\n    def __init__(self, latent_dim=512, action_dim=7, hidden_dim=256):\r\n        super().__init__()\r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(latent_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim),\r\n            nn.Tanh()  # Normalize to [-1, 1] for continuous control\r\n        )\r\n        \r\n        # For different action types, we might also have:\r\n        # - Separate heads for position and rotation\r\n        # - Variational layers for uncertainty estimation\r\n        # - Temporal prediction for trajectory planning\r\n    \r\n    def forward(self, latent_state):\r\n        # Latent state from multimodal fusion\r\n        action = self.decoder(latent_state)\r\n        return action\n"})}),"\n",(0,s.jsx)(n.h2,{id:"attention-mechanisms-in-vla-models",children:"Attention Mechanisms in VLA Models"}),"\n",(0,s.jsx)(n.h3,{id:"self-attention",children:"Self-Attention"}),"\n",(0,s.jsx)(n.p,{children:"Self-attention enables each element in a sequence to attend to all other elements:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass MultiHeadSelfAttention(nn.Module):\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        assert embed_dim % num_heads == 0\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = embed_dim // num_heads\r\n        \r\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\r\n        self.proj = nn.Linear(embed_dim, embed_dim)\r\n        \r\n    def forward(self, x):\r\n        B, N, C = x.shape\r\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\r\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\r\n        q, k, v = qkv[0], qkv[1], qkv[2]\r\n        \r\n        # Compute attention weights\r\n        attn_weights = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\r\n        attn_weights = F.softmax(attn_weights, dim=-1)\r\n        \r\n        # Apply attention to values\r\n        output = attn_weights @ v\r\n        output = output.transpose(1, 2).reshape(B, N, C)\r\n        \r\n        return self.proj(output)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"cross-attention",children:"Cross-Attention"}),"\n",(0,s.jsx)(n.p,{children:"Cross-attention allows modalities to attend to each other:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class CrossAttention(nn.Module):\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = embed_dim // num_heads\r\n        \r\n        # Separate projections for query (e.g., language) and key/value (e.g., vision)\r\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\r\n        self.kv_proj = nn.Linear(embed_dim, embed_dim * 2)\r\n        self.output_proj = nn.Linear(embed_dim, embed_dim)\r\n    \r\n    def forward(self, query, key_value):\r\n        B, N, C = query.shape\r\n        _, M, _ = key_value.shape\r\n        \r\n        # Project query, key, and value\r\n        q = self.query_proj(query).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\r\n        kv = self.kv_proj(key_value).reshape(B, M, 2, self.num_heads, self.head_dim).transpose(1, 2)\r\n        k, v = kv[:, :, 0], kv[:, :, 1]\r\n        \r\n        # Compute cross-attention\r\n        attn_weights = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\r\n        attn_weights = F.softmax(attn_weights, dim=-1)\r\n        \r\n        output = attn_weights @ v\r\n        output = output.transpose(1, 2).reshape(B, N, C)\r\n        \r\n        return self.output_proj(output)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-attention",children:"Multimodal Attention"}),"\n",(0,s.jsx)(n.p,{children:"In VLA models, attention mechanisms help integrate information across modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultimodalAttention(nn.Module):\r\n    def __init__(self, embed_dim, num_heads):\r\n        super().__init__()\r\n        self.vision_to_lang = CrossAttention(embed_dim, num_heads)\r\n        self.lang_to_vision = CrossAttention(embed_dim, num_heads)\r\n        self.action_to_multimodal = CrossAttention(embed_dim, num_heads)\r\n        \r\n    def forward(self, vision_features, language_features, action_features=None):\r\n        # Cross-attend vision and language\r\n        lang_with_vision = self.vision_to_lang(language_features, vision_features)\r\n        vision_with_lang = self.lang_to_vision(vision_features, language_features)\r\n        \r\n        # For action prediction, attend to multimodal context\r\n        if action_features is not None:\r\n            action_with_context = self.action_to_multimodal(action_features, \r\n                torch.cat([lang_with_vision, vision_with_lang], dim=1))\r\n            return vision_with_lang, lang_with_vision, action_with_context\r\n        else:\r\n            return vision_with_lang, lang_with_vision\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vla-architecture-patterns",children:"VLA Architecture Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"unified-transformer-architecture",children:"Unified Transformer Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Modern VLA models often use a single transformer architecture that processes all modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class UnifiedVLATransformer(nn.Module):\r\n    def __init__(self, \r\n                 vocab_size=50257,\r\n                 vision_patch_size=16,\r\n                 embed_dim=768,\r\n                 depth=12,\r\n                 num_heads=12,\r\n                 action_dim=7):\r\n        super().__init__()\r\n        \r\n        # Modal-specific encoders\r\n        self.vision_encoder = VisionTransformer(\r\n            patch_size=vision_patch_size,\r\n            embed_dim=embed_dim,\r\n            depth=depth//2,  # Shallower vision encoder\r\n            num_heads=num_heads\r\n        )\r\n        \r\n        self.language_encoder = LanguageEncoder(\r\n            vocab_size=vocab_size,\r\n            embed_dim=embed_dim,\r\n            num_layers=depth//2,\r\n            num_heads=num_heads\r\n        )\r\n        \r\n        # Cross-modal transformer layers\r\n        self.cross_modal_layers = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\r\n        ])\r\n        \r\n        # Action prediction head\r\n        self.action_head = nn.Sequential(\r\n            nn.LayerNorm(embed_dim),\r\n            nn.Linear(embed_dim, embed_dim // 2),\r\n            nn.ReLU(),\r\n            nn.Linear(embed_dim // 2, action_dim)\r\n        )\r\n        \r\n        # Task identification head\r\n        self.task_head = nn.Linear(embed_dim, 100)  # 100 possible tasks\r\n    \r\n    def forward(self, images, text_tokens, attention_mask=None):\r\n        # Encode modalities separately\r\n        vision_features = self.vision_encoder(images)  # (B, vision_seq_len, embed_dim)\r\n        lang_features = self.language_encoder(text_tokens, attention_mask)  # (B, text_seq_len, embed_dim)\r\n        \r\n        # Concatenate modalities\r\n        multimodal_input = torch.cat([vision_features, lang_features], dim=1)  # (B, combined_seq_len, embed_dim)\r\n        \r\n        # Process with cross-modal layers\r\n        for layer in self.cross_modal_layers:\r\n            multimodal_input = layer(multimodal_input)\r\n        \r\n        # Extract representations for different heads\r\n        # Use [CLS] token or mean pooling for global representation\r\n        pooled_features = multimodal_input.mean(dim=1)  # (B, embed_dim)\r\n        \r\n        # Generate predictions\r\n        actions = self.action_head(pooled_features)\r\n        task_pred = self.task_head(pooled_features)\r\n        \r\n        return {\r\n            'actions': actions,\r\n            'task': task_pred,\r\n            'multimodal_features': multimodal_input\r\n        }\n"})}),"\n",(0,s.jsx)(n.h3,{id:"encoder-decoder-architecture",children:"Encoder-Decoder Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Some VLA models use encoder-decoder architectures similar to T5:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class VLAEcoderDecoder(nn.Module):\r\n    def __init__(self, embed_dim=768, num_layers=12, num_heads=12, action_vocab_size=200):\r\n        super().__init__()\r\n        \r\n        # Encoder for vision-language input\r\n        self.encoder = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers // 2)\r\n        ])\r\n        \r\n        # Decoder for action generation\r\n        self.decoder = nn.ModuleList([\r\n            TransformerBlock(embed_dim, num_heads) for _ in range(num_layers // 2)\r\n        ])\r\n        \r\n        # Action vocabulary projection\r\n        self.action_projection = nn.Linear(embed_dim, action_vocab_size)\r\n        \r\n        # Cross-attention layer for encoder-decoder communication\r\n        self.enc_dec_attention = MultiHeadSelfAttention(embed_dim, num_heads)\r\n    \r\n    def forward(self, vision_lang_features, action_tokens=None):\r\n        # Encode vision-language input\r\n        encoded_features = vision_lang_features\r\n        for layer in self.encoder:\r\n            encoded_features = layer(encoded_features)\r\n        \r\n        # Decode actions\r\n        if action_tokens is not None:\r\n            # Teacher forcing during training\r\n            decoded_features = self.process_decoder_tokens(action_tokens)\r\n        else:\r\n            # Autoregressive generation during inference\r\n            decoded_features = self.autoregressive_decode(encoded_features)\r\n        \r\n        # Apply cross-attention between encoder and decoder\r\n        attended_features = self.enc_dec_attention(decoded_features, encoded_features)\r\n        \r\n        # Project to action space\r\n        action_logits = self.action_projection(attended_features)\r\n        \r\n        return action_logits\r\n    \r\n    def process_decoder_tokens(self, action_tokens):\r\n        # Similar to language model processing\r\n        # Embed action tokens and apply decoder layers\r\n        pass\r\n    \r\n    def autoregressive_decode(self, encoded_features):\r\n        # Autoregressive decoding similar to GPT\r\n        # Generate one action token at a time\r\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-fusion-techniques",children:"Multimodal Fusion Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Combine modalities at the input level:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class EarlyFusionVLA(nn.Module):\r\n    def __init__(self, vision_dim=2048, lang_dim=512, action_dim=7, hidden_dim=1024):\r\n        super().__init__()\r\n        # Project modalities to common space\r\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\r\n        self.lang_proj = nn.Linear(lang_dim, hidden_dim)\r\n        \r\n        # Combined processing\r\n        self.fusion_network = nn.Sequential(\r\n            nn.Linear(hidden_dim * 2, hidden_dim * 2),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_dim * 2, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        self.action_head = nn.Linear(hidden_dim, action_dim)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Project to common space\r\n        vis_proj = self.vision_proj(vision_features)\r\n        lang_proj = self.lang_proj(language_features)\r\n        \r\n        # Concatenate and fuse\r\n        fused = torch.cat([vis_proj, lang_proj], dim=-1)\r\n        fused = self.fusion_network(fused)\r\n        \r\n        actions = self.action_head(fused)\r\n        return actions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Process modalities independently, then combine late in the network:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class LateFusionVLA(nn.Module):\r\n    def __init__(self, vision_dim=2048, lang_dim=512, action_dim=7):\r\n        super().__init__()\r\n        # Independent processing branches\r\n        self.vision_branch = nn.Sequential(\r\n            nn.Linear(vision_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256)\r\n        )\r\n        \r\n        self.language_branch = nn.Sequential(\r\n            nn.Linear(lang_dim, 512),\r\n            nn.ReLU(),\r\n            nn.Linear(512, 256)\r\n        )\r\n        \r\n        # Fusion layer\r\n        self.fusion = nn.Linear(256 * 2, 512)\r\n        self.action_head = nn.Linear(512, action_dim)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Process independently\r\n        vis_out = self.vision_branch(vision_features)\r\n        lang_out = self.language_branch(language_features)\r\n        \r\n        # Combine late\r\n        combined = torch.cat([vis_out, lang_out], dim=-1)\r\n        fused = self.fusion(combined)\r\n        \r\n        actions = self.action_head(fused)\r\n        return actions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"hierarchical-fusion",children:"Hierarchical Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Multiple fusion points at different levels of abstraction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class HierarchicalFusionVLA(nn.Module):\r\n    def __init__(self, embed_dim=768):\r\n        super().__init__()\r\n        # Initial modality processing\r\n        self.vision_init = nn.Linear(2048, embed_dim)\r\n        self.lang_init = nn.Linear(512, embed_dim)\r\n        \r\n        # Low-level fusion\r\n        self.low_fusion = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\r\n        \r\n        # Mid-level processing layers\r\n        self.mid_layers = nn.ModuleList([\r\n            TransformerBlock(embed_dim, 8) for _ in range(4)\r\n        ])\r\n        \r\n        # High-level fusion\r\n        self.high_fusion = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\r\n        \r\n        # Final action prediction\r\n        self.action_pred = nn.Linear(embed_dim, 7)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        # Initial processing\r\n        vis_processed = self.vision_init(vision_features).unsqueeze(1)  # Add sequence dimension\r\n        lang_processed = self.lang_init(language_features).unsqueeze(1)\r\n        \r\n        # Low-level fusion\r\n        fused_low, _ = self.low_fusion(vis_processed, lang_processed, lang_processed)\r\n        \r\n        # Mid-level processing\r\n        mid_features = fused_low\r\n        for layer in self.mid_layers:\r\n            mid_features = layer(mid_features)\r\n        \r\n        # High-level fusion with context\r\n        final_features, _ = self.high_fusion(mid_features, fused_low, fused_low)\r\n        \r\n        # Predict actions\r\n        actions = self.action_pred(final_features.squeeze(1))\r\n        return actions\n"})}),"\n",(0,s.jsx)(n.h2,{id:"training-paradigms",children:"Training Paradigms"}),"\n",(0,s.jsx)(n.h3,{id:"behavioral-cloning-bc",children:"Behavioral Cloning (BC)"}),"\n",(0,s.jsx)(n.p,{children:"Learn to imitate human demonstrations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def behavioral_cloning_loss(model, batch):\r\n    \"\"\"\r\n    Standard behavioral cloning objective\r\n    \"\"\"\r\n    obs_images = batch['images']  # (B, C, H, W)\r\n    obs_language = batch['language']  # (B, seq_len)\r\n    actions = batch['expert_actions']  # (B, action_dim)\r\n    \r\n    predicted_actions = model(obs_images, obs_language)['actions']\r\n    \r\n    # Mean squared error for continuous actions\r\n    bc_loss = F.mse_loss(predicted_actions, actions)\r\n    \r\n    return bc_loss\n"})}),"\n",(0,s.jsx)(n.h3,{id:"reinforcement-learning-from-human-feedback-rlhf",children:"Reinforcement Learning from Human Feedback (RLHF)"}),"\n",(0,s.jsx)(n.p,{children:"Use human feedback to improve policies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def rlhf_loss(model, batch, reference_model=None):\r\n    \"\"\"\r\n    Reinforcement Learning from Human Feedback\r\n    \"\"\"\r\n    states = batch['states']\r\n    actions = batch['actions']\r\n    rewards = batch['rewards']  # Human preference scores\r\n    \r\n    # Get log probabilities from current model\r\n    curr_log_probs = model.get_log_prob(states, actions)\r\n    \r\n    # Get log probabilities from reference model (initial policy)\r\n    if reference_model:\r\n        ref_log_probs = reference_model.get_log_prob(states, actions)\r\n        ratio = torch.exp(curr_log_probs - ref_log_probs)\r\n    else:\r\n        ratio = 1.0\r\n    \r\n    # Policy gradient loss\r\n    pg_loss = -(ratio * rewards).mean()\r\n    \r\n    return pg_loss\n"})}),"\n",(0,s.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,s.jsx)(n.p,{children:"Learn representations that align modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def contrastive_loss(vision_features, language_features, temperature=0.1):\r\n    """\r\n    Contrastive loss to align vision and language representations\r\n    """\r\n    # Compute similarity matrix\r\n    sim_matrix = torch.matmul(vision_features, language_features.T) / temperature\r\n    \r\n    # Targets: diagonal elements should be high\r\n    labels = torch.arange(len(vision_features)).to(vision_features.device)\r\n    \r\n    # Cross entropy loss where each image should match its corresponding text\r\n    loss_img = F.cross_entropy(sim_matrix, labels)\r\n    loss_txt = F.cross_entropy(sim_matrix.T, labels)\r\n    \r\n    return (loss_img + loss_txt) / 2\n'})}),"\n",(0,s.jsx)(n.h2,{id:"computational-considerations",children:"Computational Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"memory-efficiency",children:"Memory Efficiency"}),"\n",(0,s.jsx)(n.p,{children:"VLA models require substantial computational resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Activation Memory"}),": Storing intermediate computations during forward pass"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Memory"}),": Storing model parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimizer Memory"}),": For training (especially Adam, which stores momentum and variance)"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Gradient checkpointing to trade computation for memory\r\nfrom torch.utils.checkpoint import checkpoint\r\n\r\nclass MemoryEfficientVLA(nn.Module):\r\n    def __init__(self, base_model):\r\n        super().__init__()\r\n        self.base_model = base_model\r\n    \r\n    def forward(self, images, text):\r\n        def run_part1(imgs, txt):\r\n            return self.base_model.encode_modalities(imgs, txt)\r\n        \r\n        def run_part2(fused):\r\n            return self.base_model.decode_actions(fused)\r\n        \r\n        # Apply gradient checkpointing to reduce memory usage\r\n        fused_repr = checkpoint(run_part1, images, text)\r\n        actions = checkpoint(run_part2, fused_repr)\r\n        \r\n        return actions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"sequence-length-limitations",children:"Sequence Length Limitations"}),"\n",(0,s.jsx)(n.p,{children:"Attention mechanisms scale quadratically with sequence length:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Techniques to handle long sequences:\r\n# 1. Sliding window attention\r\n# 2. Sparse attention patterns\r\n# 3. Linear attention approximations\r\n# 4. Hierarchical processing\r\n\r\nclass SlidingWindowAttention(nn.Module):\r\n    def __init__(self, embed_dim, num_heads, window_size=256):\r\n        super().__init__()\r\n        self.window_size = window_size\r\n        self.attention = nn.MultiheadAttention(\r\n            embed_dim, \r\n            num_heads, \r\n            batch_first=True\r\n        )\r\n    \r\n    def forward(self, x):\r\n        B, T, D = x.shape\r\n        if T <= self.window_size:\r\n            # Standard attention if sequence is short enough\r\n            return self.attention(x, x, x)[0]\r\n        \r\n        # Process in overlapping windows\r\n        outputs = []\r\n        for i in range(0, T, self.window_size):\r\n            end_idx = min(i + self.window_size, T)\r\n            window = x[:, i:end_idx, :]\r\n            \r\n            window_out = self.attention(window, window, window)[0]\r\n            outputs.append(window_out)\r\n        \r\n        # Combine outputs (note: this ignores cross-window attention)\r\n        return torch.cat(outputs, dim=1)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-scaling-laws",children:"Model Scaling Laws"}),"\n",(0,s.jsx)(n.p,{children:"As VLA models grow larger:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance"})," typically improves with scale but has diminishing returns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational requirements"})," grow significantly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training data needs"})," increase"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fine-tuning"})," may become more important than pre-training"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example scaling law considerations\r\nSCALING_RELATIONSHIPS = {\r\n    'parameters_vs_performance': 'Generally follows power law with diminishing returns',\r\n    'data_requirements': 'Scale roughly linearly with model size for optimal training',\r\n    'compute_requirements': 'Scale superlinearly (roughly cubic for optimal training)',\r\n    'inference_latency': 'Increases linearly for feedforward models'\r\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This submodule covered the deep learning architectures that underpin Vision-Language-Action models:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Foundation components"}),": Vision, language, and action encoders/decoders"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Attention mechanisms"}),": Self-attention, cross-attention, and multimodal attention"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture patterns"}),": Unified transformers, encoder-decoder, and hybrid approaches"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion techniques"}),": Early, late, and hierarchical multimodal fusion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training paradigms"}),": Behavioral cloning, RLHF, and contrastive learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational considerations"}),": Memory efficiency and scaling laws"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Understanding these architectural components is crucial for implementing, customizing, and deploying effective VLA models in robotic applications. In the next submodule, we'll explore the practical aspects of training VLA models with real-world robotics data."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);